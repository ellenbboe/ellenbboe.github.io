<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[HTTP 代理原理与实现]]></title>
    <url>%2F2018%2F09%2F16%2Fday1%2F</url>
    <content type="text"><![CDATA[HTTP 代理原理与实现HTTP 客户端向代理发送请求报文，代理服务器需要正确地处理请求和连接（例如正确处理 Connection: keep-alive），同时向服务器发送请求，并将收到的响应转发给客户端。 假如我通过代理访问 A 网站，对于 A 来说，它会把代理当做客户端，完全察觉不到真正客户端的存在，这实现了隐藏客户端 IP 的目的。 数据经过代理服务器后可能会经过修改,要小心数据有效性 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657581 │ var net = require('net')2 │ var url = require('url')3 │ var http = require('http')4 │5 │ var hostname = '127.0.0.1'6 │ var port = '8888'7 │===================================这个服务从请求报文中解析出请求 URL 和其他必要参数，新建到服务端的请求，并把代理收到的请求转发给新建的请求，最后再把服务端响应返回给浏览器。===================================8 │ var request = (creq,cres)=&gt;&#123;9 │ console.log(creq.header);10 │ var u = url.parse(creq.url);===============重新构建请求===============11 │ var options = &#123;12 │ hostname : u.hostname,13 │ port : u.port || 80,14 │ path : u.path,15 │ method : creq.method,16 │ headers : creq.headers,17 │ &#125;;18 │==================得到响应==================19 │ var preq = http.request(options, (pres)=&gt;&#123;20 │ cres.writeHead(pres.statusCode,pres.headers);//估计是添加头21 │ pres.pipe(cres);//装入22 │ &#125;).on('error',function(e)&#123;23 │ cres.end();24 │ &#125;);25 │ creq.pipe(preq);26 │ &#125;27 │==================================这个服务从 CONNECT 请求报文中解析出域名和端口，创建到服务端的 TCP 连接，并和 CONNECT 请求中的 TCP 连接串起来，最后再响应一个 Connection Established 响应。===============================28 │ var connect = (creq, csock) =&gt;&#123;29 │ console.log(creq.headers);30 │31 │ var u = url.parse("http://"+creq.url);32 │33 │ var psock = net.connect(u.port, u.hostname, ()=&gt;&#123;34 │ csock.write('HTTP/1.1 200 Connection Established\r\n\r\n');35 │ psock.pipe(csock);36 │ &#125;).on('error',(e)=&gt;&#123;37 │ csock.end();38 │ &#125;);39 │ csock.pipe(psock);40 │ &#125;41 │42 │43 │ var proxy = http.createServer().on('request',request).on('connect',connect);44 │ proxy.listen(port, hostname,() =&gt;&#123;45 │ console.log("Proxy run in 127.0.0.1:8888");46 │ &#125;) 分析代码主要思路:使用nodejs,通过构建数据包进行http代理,实现转发,起到一个连接作用 part 1 发送的请求通过代理,代理解析出url的信息,与请求一起创建出新的包,发送给服务器,获得响应,在将响应转发给客户端 part 2 得到请求时,解析url的信息,得到域名和端口,服务器发出构建tcp链接的请求,之后将代理与服务器的tcp连接(a) 和 代理本身构建的tcp连接(b)结合,形成通路]]></content>
      <categories>
        <category>nodejs</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在android安装linux 失败]]></title>
    <url>%2F2018%2F09%2F15%2Fandroid-linux%2F</url>
    <content type="text"><![CDATA[失败原因:三星note2 N7100 的 kernel版本太低实验推迟 ……..]]></content>
      <categories>
        <category>不定时记录</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Pass 4]]></title>
    <url>%2F2018%2F09%2F11%2Fspring4%2F</url>
    <content type="text"><![CDATA[Spring Beans 自动装配你已经学会如何使用&lt;bean&gt;元素来声明 bean 和通过使用 XML 配置文件中的和元素来注入 。 Spring 容器可以在不使用和 元素的情况下自动装配相互协作的 bean 之间的关系，这有助于减少编写一个大的基于 Spring 的应用程序的 XML 配置的数量。 自动装配模式下列自动装配模式，它们可用于指示 Spring 容器为来使用自动装配进行依赖注入。你可以使用元素的 autowire 属性为一个 bean 定义指定自动装配模式。 模式 描述 no 这是默认的设置，它意味着没有自动装配，你应该使用显式的bean引用来连线。你不用为了连线做特殊的事。在依赖注入章节你已经看到这个了。 byName 由属性名自动装配。Spring 容器看到在 XML 配置文件中 bean 的自动装配的属性设置为 byName。然后尝试匹配，并且将它的属性与在配置文件中被定义为相同名称的 beans 的属性进行连接。 byType 由属性数据类型自动装配。Spring 容器看到在 XML 配置文件中 bean 的自动装配的属性设置为 byType。然后如果它的类型匹配配置文件中的一个确切的 bean 名称，它将尝试匹配和连接属性的类型。如果存在不止一个这样的 bean，则一个致命的异常将会被抛出。 constructor 类似于 byType，但该类型适用于构造函数参数类型。如果在容器中没有一个构造函数参数类型的 bean，则一个致命错误将会发生。 autodetect Spring首先尝试通过 constructor 使用自动装配来连接，如果它不执行，Spring 尝试通过 byType 来自动装配。 自动装配的局限性当自动装配始终在同一个项目中使用时，它的效果最好。如果通常不使用自动装配，它可能会使开发人员混淆的使用它来连接只有一个或两个 bean 定义。不过，自动装配可以显著减少需要指定的属性或构造器参数，但你应该在使用它们之前考虑到自动装配的局限性和缺点。 限制 描述 重写的可能性 你可以使用总是重写自动装配的 和 设置来指定依赖关系。 原始数据类型 你不能自动装配所谓的简单类型包括基本类型，字符串和类。 混乱的本质 自动装配不如显式装配精确，所以如果可能的话尽可能使用显式装配。 Spring 自动装配 ‘byName’(关心id)这种模式由属性名称指定自动装配。Spring 容器看作 beans，在 XML 配置文件中 beans 的 auto-wire 属性设置为 byName。然后，它尝试将它的属性与配置文件中定义为相同名称的 beans 进行匹配和连接。如果找到匹配项，它将注入这些 beans，否则，它将抛出异常。 例如，在配置文件中，如果一个 bean 定义设置为自动装配 byName，并且它包含 spellChecker 属性（即，它有一个 setSpellChecker(…) 方法），那么 Spring 就会查找定义名为 spellChecker 的 bean，并且用它来设置这个属性。你仍然可以使用 标签连接其余的属性。下面的例子将说明这个概念。 这里是 TextEditor.java 文件的内容：1234567891011121314151617181920package com.tutorialspoint;public class TextEditor &#123; private SpellChecker spellChecker; private String name; public void setSpellChecker( SpellChecker spellChecker )&#123; this.spellChecker = spellChecker; &#125; public SpellChecker getSpellChecker() &#123; return spellChecker; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public void spellCheck() &#123; spellChecker.checkSpelling(); &#125;&#125; 下面是另一个依赖类文件 SpellChecker.java 的内容：12345678910111213141516171819202122package com.tutorialspoint;public class SpellChecker &#123; public SpellChecker() &#123; System.out.println(&quot;Inside SpellChecker constructor.&quot; ); &#125; public void checkSpelling() &#123; System.out.println(&quot;Inside checkSpelling.&quot; ); &#125; &#125;下面是 MainApp.java 文件的内容：package com.tutorialspoint;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class MainApp &#123; public static void main(String[] args) &#123; ApplicationContext context = new ClassPathXmlApplicationContext(&quot;Beans.xml&quot;); TextEditor te = (TextEditor) context.getBean(&quot;textEditor&quot;); te.spellCheck(); &#125;&#125; 下面是在正常情况下的配置文件 Beans.xml 文件：123456789101112131415161718&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;!-- Definition for textEditor bean --&gt; &lt;bean id=&quot;textEditor&quot; class=&quot;com.tutorialspoint.TextEditor&quot;&gt; &lt;property name=&quot;spellChecker&quot; ref=&quot;spellChecker&quot; /&gt; &lt;property name=&quot;name&quot; value=&quot;Generic Text Editor&quot; /&gt; &lt;/bean&gt; &lt;!-- Definition for spellChecker bean --&gt; &lt;bean id=&quot;spellChecker&quot; class=&quot;com.tutorialspoint.SpellChecker&quot;&gt; &lt;/bean&gt;&lt;/beans&gt; 但是，如果你要使用自动装配 “byName”，那么你的 XML 配置文件将成为如下：123456789101112131415161718&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;!-- Definition for textEditor bean --&gt; &lt;bean id=&quot;textEditor&quot; class=&quot;com.tutorialspoint.TextEditor&quot; autowire=&quot;byName&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;Generic Text Editor&quot; /&gt; &lt;/bean&gt; &lt;!-- Definition for spellChecker bean --&gt; &lt;bean id=&quot;spellChecker&quot; class=&quot;com.tutorialspoint.SpellChecker&quot;&gt; &lt;/bean&gt;&lt;/beans&gt; 一旦你完成了创建源代码和 bean 的配置文件，我们就可以运行该应用程序。如果你的应用程序一切都正常，它将打印下面的消息：12Inside SpellChecker constructor.Inside checkSpelling. Spring 自动装配 byType(不关心id)这种模式由属性类型指定自动装配。Spring 容器看作 beans，在 XML 配置文件中 beans 的 autowire 属性设置为 byType。然后，如果它的 type 恰好与配置文件中 beans 名称中的一个相匹配，它将尝试匹配和连接它的属性。如果找到匹配项，它将注入这些 beans，否则，它将抛出异常。 例如，在配置文件中，如果一个 bean 定义设置为自动装配 byType，并且它包含 SpellChecker 类型的 spellChecker 属性，那么 Spring 就会查找定义名为 SpellChecker 的 bean，并且用它来设置这个属性。你仍然可以使用&lt;property&gt;标签连接其余属性。下面的例子将说明这个概念，你会发现和上面的例子没有什么区别，除了 XML 配置文件已经被改变。123456789101112131415161718&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;!-- Definition for textEditor bean --&gt; &lt;bean id=&quot;textEditor&quot; class=&quot;com.tutorialspoint.TextEditor&quot; autowire=&quot;byType&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;Generic Text Editor&quot; /&gt; &lt;/bean&gt; &lt;!-- Definition for spellChecker bean --&gt; &lt;bean id=&quot;SpellChecker&quot; class=&quot;com.tutorialspoint.SpellChecker&quot;&gt; &lt;/bean&gt;&lt;/beans&gt; Spring 由构造函数自动装配这种模式与 byType 非常相似，但它应用于构造器参数。Spring 容器看作 beans，在 XML 配置文件中 beans 的 autowire 属性设置为 constructor。然后，它尝试把它的构造函数的参数与配置文件中 beans 名称中的一个进行匹配和连线。如果找到匹配项，它会注入这些 bean，否则，它会抛出异常。 例如，在配置文件中，如果一个 bean 定义设置为通过构造函数自动装配，而且它有一个带有 SpellChecker 类型的参数之一的构造函数，那么 Spring 就会查找定义名为 SpellChecker 的 bean，并用它来设置构造函数的参数。你仍然可以使用 标签连接其余属性。下面的例子将说明这个概念。 这里是 TextEditor.java 文件的内容：123456789101112131415161718package com.tutorialspoint;public class TextEditor &#123; private SpellChecker spellChecker; private String name; public TextEditor( SpellChecker spellChecker, String name ) &#123; this.spellChecker = spellChecker; this.name = name; &#125; public SpellChecker getSpellChecker() &#123; return spellChecker; &#125; public String getName() &#123; return name; &#125; public void spellCheck() &#123; spellChecker.checkSpelling(); &#125;&#125; 下面是另一个依赖类文件 SpellChecker.java 的内容：12345678910package com.tutorialspoint;public class SpellChecker &#123; public SpellChecker()&#123; System.out.println(&quot;Inside SpellChecker constructor.&quot; ); &#125; public void checkSpelling() &#123; System.out.println(&quot;Inside checkSpelling.&quot; ); &#125; &#125; 下面是 MainApp.java 文件的内容：1234567891011package com.tutorialspoint;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class MainApp &#123; public static void main(String[] args) &#123; ApplicationContext context = new ClassPathXmlApplicationContext(&quot;Beans.xml&quot;); TextEditor te = (TextEditor) context.getBean(&quot;textEditor&quot;); te.spellCheck(); &#125;&#125; 下面是在正常情况下的配置文件 Beans.xml 文件：123456789101112131415161718&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;!-- Definition for textEditor bean --&gt; &lt;bean id=&quot;textEditor&quot; class=&quot;com.tutorialspoint.TextEditor&quot;&gt; &lt;constructor-arg ref=&quot;spellChecker&quot; /&gt; &lt;constructor-arg value=&quot;Generic Text Editor&quot;/&gt; &lt;/bean&gt; &lt;!-- Definition for spellChecker bean --&gt; &lt;bean id=&quot;spellChecker&quot; class=&quot;com.tutorialspoint.SpellChecker&quot;&gt; &lt;/bean&gt;&lt;/beans&gt; 但是，如果你要使用自动装配 “by constructor”，那么你的 XML 配置文件将成为如下：123456789101112131415161718&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;!-- Definition for textEditor bean --&gt; &lt;bean id=&quot;textEditor&quot; class=&quot;com.tutorialspoint.TextEditor&quot; autowire=&quot;constructor&quot;&gt; &lt;constructor-arg value=&quot;Generic Text Editor&quot;/&gt; &lt;/bean&gt; &lt;!-- Definition for spellChecker bean --&gt; &lt;bean id=&quot;SpellChecker&quot; class=&quot;com.tutorialspoint.SpellChecker&quot;&gt; &lt;/bean&gt;&lt;/beans&gt; 一旦你完成了创建源代码和 bean 的配置文件，我们就可以运行该应用程序。如果你的应用程序一切都正常，它将打印下面的消息：12Inside SpellChecker constructor.Inside checkSpelling. 主观总结:自动装配是在xml中的bean属性中修改autowrite属性其中byName 依靠的是idbyType 依靠的是类别(似乎是系统自己找的)使用constructor-arg时,要明确构造函数存在参数可以注入constructor 和constructor-arg一样,只是不用写已经在xml中的变量了]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Pass 3]]></title>
    <url>%2F2018%2F09%2F10%2Fspring3%2F</url>
    <content type="text"><![CDATA[Spring 依赖注入每个基于应用程序的 java 都有几个对象，这些对象一起工作来呈现出终端用户所看到的工作的应用程序。当编写一个复杂的 Java 应用程序时，应用程序类应该尽可能独立于其他 Java 类来 增加这些类重用的可能性，并且在做单元测试时，测试独立于其他类的独立性。依赖注入（或有时称为布线）有助于把这些类粘合在一起，同时保持他们独立。 假设你有一个包含文本编辑器组件的应用程序，并且你想要提供拼写检查。标准代码看起来是这样的：123456public class TextEditor &#123; private SpellChecker spellChecker; public TextEditor() &#123; spellChecker = new SpellChecker(); &#125;&#125; 在这里我们所做的就是创建一个 TextEditor 和 SpellChecker 之间的依赖关系。在控制反转的场景中，我们反而会做这样的事情：123456public class TextEditor &#123; private SpellChecker spellChecker; public TextEditor(SpellChecker spellChecker) &#123; this.spellChecker = spellChecker; &#125;&#125; 在这里，TextEditor 不应该担心 SpellChecker 的实现。SpellChecker 将会独立实现，并且在 TextEditor 实例化的时候将提供给 TextEditor，整个过程是由 Spring 框架的控制。(个人理解就是:原先在public方法中要创建的spellchecker已经在外面实现了,所以独立了..) 在这里，我们已经从 TextEditor 中删除了全面控制，并且把它保存到其他地方（即 XML 配置文件），且依赖关系（即 SpellChecker 类）通过类构造函数被注入到 TextEditor 类中。因此，控制流通过依赖注入（DI）已经“反转”，因为你已经有效地委托依赖关系到一些外部系统。(个人理解:委托在依赖关系的外部系统意思是可以通过外部的函数实现注入) 依赖注入的第二种方法是通过 TextEditor 类的 Setter 方法，我们将创建 SpellChecker 实例，该实例将被用于调用 setter 方法来初始化 TextEditor 的属性。因此，DI 主要有两种变体和下面的两个子章将结合实例涵盖它们： 序号 依赖注入类型 &amp; 描述 1 Constructor-based dependency injection 当容器调用带有多个参数的构造函数类时，实现基于构造函数的 DI，每个代表在其他类中的一个依赖关系。 2 Setter-based dependency injection 基于 setter 方法的 DI 是通过在调用无参数的构造函数或无参数的静态工厂方法实例化 bean 之后容器调用 beans 的 setter 方法来实现的。 你可以混合这两种方法，基于构造函数和基于 setter 方法的 DI，然而使用有强制性依存关系的构造函数和有可选依赖关系的 setter是一个好的做法。 代码是 DI 原理的清洗机，当对象与它们的依赖关系被提供时，解耦效果更明显。对象不查找它的依赖关系，也不知道依赖关系的位置或类，而这一切都由 Spring 框架控制的。 Spring 基于构造函数的依赖注入当容器调用带有一组参数的类构造函数时，基于构造函数的 DI 就完成了，其中每个参数代表一个对其他类的依赖。 示例：下面的例子显示了一个类 TextEditor，只能用构造函数注入来实现依赖注入。 让我们用 Eclipse IDE 适当地工作，并按照以下步骤创建一个 Spring 应用程序。 步骤 描述 1 创建一个名为 SpringExample 的项目，并在创建的项目中的 src 文件夹下创建包 com.tutorialspoint 。 2 使用 Add External JARs 选项添加必需的 Spring 库，解释见 Spring Hello World Example chapter. 3 在 com.tutorialspoint 包下创建 Java类 TextEditor，SpellChecker 和 MainApp。 4 在 src 文件夹下创建 Beans 的配置文件 Beans.xml 。 5 最后一步是创建所有 Java 文件和 Bean 配置文件的内容并按照如下所示的方法运行应用程序。 这是 TextEditor.java 文件的内容：1234567891011package com.tutorialspoint;public class TextEditor &#123; private SpellChecker spellChecker; public TextEditor(SpellChecker spellChecker) &#123; System.out.println(&quot;Inside TextEditor constructor.&quot; ); this.spellChecker = spellChecker; &#125; public void spellCheck() &#123; spellChecker.checkSpelling(); &#125;&#125; 下面是另一个依赖类文件 SpellChecker.java 的内容：123456789package com.tutorialspoint;public class SpellChecker &#123; public SpellChecker()&#123; System.out.println(&quot;Inside SpellChecker constructor.&quot; ); &#125; public void checkSpelling() &#123; System.out.println(&quot;Inside checkSpelling.&quot; ); &#125;&#125; 以下是 MainApp.java 文件的内容：1234567891011package com.tutorialspoint;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class MainApp &#123; public static void main(String[] args) &#123; ApplicationContext context = new ClassPathXmlApplicationContext(&quot;Beans.xml&quot;); TextEditor te = (TextEditor) context.getBean(&quot;textEditor&quot;); te.spellCheck(); &#125;&#125; 下面是配置文件 Beans.xml 的内容，它有基于构造函数注入的配置：1234567891011121314151617&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;!-- Definition for textEditor bean --&gt; &lt;bean id=&quot;textEditor&quot; class=&quot;com.tutorialspoint.TextEditor&quot;&gt; &lt;constructor-arg ref=&quot;spellChecker&quot;/&gt; &lt;/bean&gt; &lt;!-- Definition for spellChecker bean --&gt; &lt;bean id=&quot;spellChecker&quot; class=&quot;com.tutorialspoint.SpellChecker&quot;&gt; &lt;/bean&gt;&lt;/beans&gt; 当你完成了创建源和 bean 配置文件后，让我们开始运行应用程序。如果你的应用程序运行顺利的话，那么将会输出下述所示消息：123Inside SpellChecker constructor.Inside TextEditor constructor.Inside checkSpelling. 构造函数参数解析:如果存在不止一个参数时，当把参数传递给构造函数时，可能会存在歧义。要解决这个问题，那么构造函数的参数在 bean 定义中的顺序就是把这些参数提供给适当的构造函数的顺序就可以了。考虑下面的类:123456package x.y;public class Foo &#123; public Foo(Bar bar, Baz baz) &#123; // ... &#125;&#125; 下述配置文件工作顺利：123456789&lt;beans&gt; &lt;bean id=&quot;foo&quot; class=&quot;x.y.Foo&quot;&gt; &lt;constructor-arg ref=&quot;bar&quot;/&gt; &lt;constructor-arg ref=&quot;baz&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;bar&quot; class=&quot;x.y.Bar&quot;/&gt; &lt;bean id=&quot;baz&quot; class=&quot;x.y.Baz&quot;/&gt;&lt;/beans&gt; 让我们再检查一下我们传递给构造函数不同类型的位置。考虑下面的类：123456package x.y;public class Foo &#123; public Foo(int year, String name) &#123; // ... &#125;&#125; 如果你使用 type 属性显式的指定了构造函数参数的类型，容器也可以使用与简单类型匹配的类型。例如：12345678&lt;beans&gt; &lt;bean id=&quot;exampleBean&quot; class=&quot;examples.ExampleBean&quot;&gt; &lt;constructor-arg type=&quot;int&quot; value=&quot;2001&quot;/&gt; &lt;constructor-arg type=&quot;java.lang.String&quot; value=&quot;Zara&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; 最后并且也是最好的传递构造函数参数的方式，使用 index 属性来显式的指定构造函数参数的索引。下面是基于索引为 0 的例子，如下所示：12345678&lt;beans&gt; &lt;bean id=&quot;exampleBean&quot; class=&quot;examples.ExampleBean&quot;&gt; &lt;constructor-arg index=&quot;0&quot; value=&quot;2001&quot;/&gt; &lt;constructor-arg index=&quot;1&quot; value=&quot;Zara&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; 最后，如果你想要向一个对象传递一个引用，你需要使用 标签的 ref 属性，如果你想要直接传递值，那么你应该使用如上所示的 value 属性。 (对构造函数传入参数!!!!!!!可怕) Spring 基于设值函数的依赖注入当容器调用一个无参的构造函数或一个无参的静态 factory 方法来初始化你的 bean 后，通过容器在你的 bean 上调用设值函数，基于设值函数的 DI 就完成了。 示例：下述例子显示了一个类 TextEditor，它只能使用纯粹的基于设值函数的注入来实现依赖注入。 让我们用 Eclipse IDE 适当地工作，并按照以下步骤创建一个 Spring 应用程序。 步骤 描述 1 创建一个名为 SpringExample 的项目，并在创建的项目中的 src 文件夹下创建包 com.tutorialspoint 。 2 使用 Add External JARs 选项添加必需的 Spring 库，解释见 Spring Hello World Example chapter. 3 在 com.tutorialspoint 包下创建 Java类 TextEditor，SpellChecker 和 MainApp。 4 在 src 文件夹下创建 Beans 的配置文件 Beans.xml 。 5 最后一步是创建所有 Java 文件和 Bean 配置文件的内容并按照如下所示的方法运行应用程序。 下面是 TextEditor.java 文件的内容：12345678910111213141516package com.tutorialspoint;public class TextEditor &#123; private SpellChecker spellChecker; // a setter method to inject the dependency. public void setSpellChecker(SpellChecker spellChecker) &#123; System.out.println(&quot;Inside setSpellChecker.&quot; ); this.spellChecker = spellChecker; &#125; // a getter method to return spellChecker public SpellChecker getSpellChecker() &#123; return spellChecker; &#125; public void spellCheck() &#123; spellChecker.checkSpelling(); &#125;&#125; 在这里，你需要检查设值函数方法的名称转换。要设置一个变量 spellChecker，我们使用 setSpellChecker() 方法，该方法与 Java POJO 类非常相似。让我们创建另一个依赖类文件 SpellChecker.java 的内容：123456789package com.tutorialspoint;public class SpellChecker &#123; public SpellChecker()&#123; System.out.println(&quot;Inside SpellChecker constructor.&quot; ); &#125; public void checkSpelling() &#123; System.out.println(&quot;Inside checkSpelling.&quot; ); &#125; &#125; 以下是 MainApp.java 文件的内容：1234567891011package com.tutorialspoint;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class MainApp &#123; public static void main(String[] args) &#123; ApplicationContext context = new ClassPathXmlApplicationContext(&quot;Beans.xml&quot;); TextEditor te = (TextEditor) context.getBean(&quot;textEditor&quot;); te.spellCheck(); &#125;&#125; 下面是配置文件 Beans.xml 的内容，该文件有基于设值函数注入的配置：123456789101112131415161718&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;!-- Definition for textEditor bean --&gt; &lt;bean id=&quot;textEditor&quot; class=&quot;com.tutorialspoint.TextEditor&quot;&gt; &lt;property name=&quot;spellChecker&quot; ref=&quot;spellChecker&quot;/&gt; &lt;!-- 这里的 property是在java bean中的变量名,ref这里写id--&gt; &lt;/bean&gt; &lt;!-- Definition for spellChecker bean --&gt; &lt;bean id=&quot;spellChecker&quot; class=&quot;com.tutorialspoint.SpellChecker&quot;&gt; &lt;/bean&gt;&lt;/beans&gt; 你应该注意定义在基于构造函数注入和基于设值函数注入中的 Beans.xml 文件的区别。唯一的区别就是在基于构造函数注入中，我们使用的是〈bean〉标签中的〈constructor-arg〉元素，而在基于设值函数的注入中，我们使用的是〈bean〉标签中的〈property〉元素。 第二个你需要注意的点是，如果你要把一个引用传递给一个对象，那么你需要使用 标签的 ref 属性，而如果你要直接传递一个值，那么你应该使用 value 属性(value与index 和 type搭配)。 当你完成了创建源和 bean 配置文件后，让我们开始运行应用程序。如果你的应用程序运行顺利的话，那么将会输出下述所示消息：123Inside SpellChecker constructor.Inside setSpellChecker.Inside checkSpelling. 使用 p-namespace 实现 XML 配置：如果你有许多的设值函数方法，那么在 XML 配置文件中使用 p-namespace 是非常方便的。让我们查看一下区别： 以带有 标签的标准 XML 配置文件为例：1234567891011121314151617&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;bean id=&quot;john-classic&quot; class=&quot;com.example.Person&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;John Doe&quot;/&gt; &lt;property name=&quot;spouse&quot; ref=&quot;jane&quot;/&gt; &lt;/bean&gt; &lt;bean name=&quot;jane&quot; class=&quot;com.example.Person&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;John Doe&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; 上述 XML 配置文件可以使用 p-namespace 以一种更简洁的方式重写，如下所示：123456789101112131415161718&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;bean id=&quot;john-classic&quot; class=&quot;com.example.Person&quot; p:name=&quot;John Doe&quot; p:spouse-ref=&quot;jane&quot;/&gt; &lt;/bean&gt; &lt;bean name=&quot;jane&quot; class=&quot;com.example.Person&quot; p:name=&quot;John Doe&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; 在这里，你不应该区别指定原始值和带有 p-namespace(p后面更上bean的变量名称)的对象引用。-ref 部分表明这不是一个直接的值，而是对另一个 bean 的引用。 Spring 注入内部 Beans正如你所知道的 Java 内部类是在其他类的范围内被定义的，同理，inner beans 是在其他 bean 的范围内定义的 bean。因此在 或 元素内 元素被称为内部bean，如下所示。1234567891011121314&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;bean id=&quot;outerBean&quot; class=&quot;...&quot;&gt; &lt;property name=&quot;target&quot;&gt; &lt;bean id=&quot;innerBean&quot; class=&quot;...&quot;/&gt; &lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 这里是 TextEditor.java 文件的内容：12345678910111213141516package com.tutorialspoint;public class TextEditor &#123; private SpellChecker spellChecker; // a setter method to inject the dependency. public void setSpellChecker(SpellChecker spellChecker) &#123; System.out.println(&quot;Inside setSpellChecker.&quot; ); this.spellChecker = spellChecker; &#125; // a getter method to return spellChecker public SpellChecker getSpellChecker() &#123; return spellChecker; &#125; public void spellCheck() &#123; spellChecker.checkSpelling(); &#125;&#125; 下面是另一个依赖的类文件 SpellChecker.java 内容：123456789package com.tutorialspoint;public class SpellChecker &#123; public SpellChecker()&#123; System.out.println(&quot;Inside SpellChecker constructor.&quot; ); &#125; public void checkSpelling()&#123; System.out.println(&quot;Inside checkSpelling.&quot; ); &#125; &#125; 下面是 MainApp.java 文件的内容：12345678910package com.tutorialspoint;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class MainApp &#123; public static void main(String[] args) &#123; ApplicationContext context = new ClassPathXmlApplicationContext(&quot;Beans.xml&quot;); TextEditor te = (TextEditor) context.getBean(&quot;textEditor&quot;); te.spellCheck(); &#125;&#125; 下面是使用内部 bean 为基于 setter 注入进行配置的配置文件 Beans.xml 文件：123456789101112131415&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;!-- Definition for textEditor bean using inner bean --&gt; &lt;bean id=&quot;textEditor&quot; class=&quot;com.tutorialspoint.TextEditor&quot;&gt; &lt;property name=&quot;spellChecker&quot;&gt; &lt;bean id=&quot;spellChecker&quot; class=&quot;com.tutorialspoint.SpellChecker&quot;/&gt; &lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; (就像是直接new了一个对象进行赋值)一旦你创建源代码和 bean 配置文件完成后，我们就可以运行该应用程序。如果你的应用程序一切都正常，将输出以下信息：123Inside SpellChecker constructor.Inside setSpellChecker.Inside checkSpelling. Spring 注入集合你已经看到了如何使用 value 属性来配置基本数据类型和在你的 bean 配置文件中使用&lt;property&gt;标签的 ref属性来配置对象引用。这两种情况下处理将值传递给一个 bean。 现在如果你想传递多个值，如 Java Collection 类型 List、Set、Map 和 Properties，应该怎么做呢。为了处理这种情况，Spring 提供了四种类型的集合的配置元素，如下所示： 元素 描述 &lt;list> 它有助于连线，如注入一列值，允许重复。 &lt;set> 它有助于连线一组值，但不能重复。 &lt;map> 它可以用来注入名称-值对的集合，其中名称和值可以是任何类型。 &lt;props> 它可以用来注入名称-值对的集合，其中名称和值都是字符串类型。 你可以使用&lt;list>或&lt;set>来连接任何 java.util.Collection 的实现或数组。 你会遇到两种情况（a）传递集合中直接的值（b）传递一个 bean 的引用作为集合的元素。这里是 JavaCollection.java 文件的内容：1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.tutorialspoint;import java.util.* ;public class JavaCollection &#123; List addressList; Set addressSet; Map addressMap; Properties addressProp; // a setter method to set List public void setAddressList(List addressList) &#123; this.addressList = addressList; &#125; // prints and returns all the elements of the list. public List getAddressList() &#123; System.out.println(&quot;List Elements :&quot; + addressList); return addressList; &#125; // a setter method to set Set public void setAddressSet(Set addressSet) &#123; this.addressSet = addressSet; &#125; // prints and returns all the elements of the Set. public Set getAddressSet() &#123; System.out.println(&quot;Set Elements :&quot; + addressSet); return addressSet; &#125; // a setter method to set Map public void setAddressMap(Map addressMap) &#123; this.addressMap = addressMap; &#125; // prints and returns all the elements of the Map. public Map getAddressMap() &#123; System.out.println(&quot;Map Elements :&quot; + addressMap); return addressMap; &#125; // a setter method to set Property public void setAddressProp(Properties addressProp) &#123; this.addressProp = addressProp; &#125; // prints and returns all the elements of the Property. public Properties getAddressProp() &#123; System.out.println(&quot;Property Elements :&quot; + addressProp); return addressProp; &#125;&#125; 下面是 MainApp.java 文件的内容：1234567891011121314package com.tutorialspoint;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class MainApp &#123; public static void main(String[] args) &#123; ApplicationContext context = new ClassPathXmlApplicationContext(&quot;Beans.xml&quot;); JavaCollection jc=(JavaCollection)context.getBean(&quot;javaCollection&quot;); jc.getAddressList(); jc.getAddressSet(); jc.getAddressMap(); jc.getAddressProp(); &#125;&#125; 下面是配置所有类型的集合的配置文件 Beans.xml 文件：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;!-- Definition for javaCollection --&gt; &lt;bean id=&quot;javaCollection&quot; class=&quot;com.tutorialspoint.JavaCollection&quot;&gt; &lt;!-- results in a setAddressList(java.util.List) call --&gt; &lt;property name=&quot;addressList&quot;&gt; &lt;list&gt; &lt;value&gt;INDIA&lt;/value&gt; &lt;value&gt;Pakistan&lt;/value&gt; &lt;value&gt;USA&lt;/value&gt; &lt;value&gt;USA&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;!-- results in a setAddressSet(java.util.Set) call --&gt; &lt;property name=&quot;addressSet&quot;&gt; &lt;set&gt; &lt;value&gt;INDIA&lt;/value&gt; &lt;value&gt;Pakistan&lt;/value&gt; &lt;value&gt;USA&lt;/value&gt; &lt;value&gt;USA&lt;/value&gt; &lt;/set&gt; &lt;/property&gt; &lt;!-- results in a setAddressMap(java.util.Map) call --&gt; &lt;property name=&quot;addressMap&quot;&gt; &lt;map&gt; &lt;entry key=&quot;1&quot; value=&quot;INDIA&quot;/&gt; &lt;entry key=&quot;2&quot; value=&quot;Pakistan&quot;/&gt; &lt;entry key=&quot;3&quot; value=&quot;USA&quot;/&gt; &lt;entry key=&quot;4&quot; value=&quot;USA&quot;/&gt; &lt;/map&gt; &lt;/property&gt; &lt;!-- results in a setAddressProp(java.util.Properties) call --&gt; &lt;property name=&quot;addressProp&quot;&gt; &lt;props&gt; &lt;prop key=&quot;one&quot;&gt;INDIA&lt;/prop&gt; &lt;prop key=&quot;two&quot;&gt;Pakistan&lt;/prop&gt; &lt;prop key=&quot;three&quot;&gt;USA&lt;/prop&gt; &lt;prop key=&quot;four&quot;&gt;USA&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 一旦你创建源代码和 bean 配置文件完成后，我们就可以运行该应用程序。你应该注意这里不需要配置文件。如果你的应用程序一切都正常，将输出以下信息：1234List Elements :[INDIA, Pakistan, USA, USA]Set Elements :[INDIA, Pakistan, USA]Map Elements :&#123;1=INDIA, 2=Pakistan, 3=USA, 4=USA&#125;Property Elements :&#123;two=Pakistan, one=INDIA, three=USA, four=USA&#125; 注入 Bean 引用下面的 Bean 定义将帮助你理解如何注入 bean 的引用作为集合的元素。甚至你可以将引用和值混合在一起，如下所示：12345678910111213141516171819202122232425262728293031323334353637383940&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;!-- Bean Definition to handle references and values --&gt; &lt;bean id=&quot;...&quot; class=&quot;...&quot;&gt; &lt;!-- Passing bean reference for java.util.List --&gt; &lt;property name=&quot;addressList&quot;&gt; &lt;list&gt; &lt;ref bean=&quot;address1&quot;/&gt; &lt;ref bean=&quot;address2&quot;/&gt; &lt;value&gt;Pakistan&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;!-- Passing bean reference for java.util.Set --&gt; &lt;property name=&quot;addressSet&quot;&gt; &lt;set&gt; &lt;ref bean=&quot;address1&quot;/&gt; &lt;ref bean=&quot;address2&quot;/&gt; &lt;value&gt;Pakistan&lt;/value&gt; &lt;/set&gt; &lt;/property&gt; &lt;!-- Passing bean reference for java.util.Map --&gt; &lt;property name=&quot;addressMap&quot;&gt; &lt;map&gt; &lt;entry key=&quot;one&quot; value=&quot;INDIA&quot;/&gt; &lt;entry key =&quot;two&quot; value-ref=&quot;address1&quot;/&gt; &lt;entry key =&quot;three&quot; value-ref=&quot;address2&quot;/&gt; &lt;/map&gt; &lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 为了使用上面的 bean 定义，你需要定义 setter 方法，它们应该也能够是用这种方式来处理引用。 注入 null 和空字符串的值如果你需要传递一个空字符串作为值，那么你可以传递它，如下所示：123&lt;bean id=&quot;...&quot; class=&quot;exampleBean&quot;&gt; &lt;property name=&quot;email&quot; value=&quot;&quot;/&gt;&lt;/bean&gt; 前面的例子相当于 Java 代码：exampleBean.setEmail(“”)。 如果你需要传递一个 NULL 值，那么你可以传递它，如下所示：123&lt;bean id=&quot;...&quot; class=&quot;exampleBean&quot;&gt; &lt;property name=&quot;email&quot;&gt;&lt;null/&gt;&lt;/property&gt;&lt;/bean&gt; 前面的例子相当于 Java 代码：exampleBean.setEmail(null)。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Pass 2]]></title>
    <url>%2F2018%2F09%2F10%2Fspring2%2F</url>
    <content type="text"><![CDATA[Spring Bean 作用域(设置scope)当在 Spring 中定义一个 bean 时，你必须声明该 bean 的作用域的选项。例如，为了强制 Spring 在每次需要时都产生一个新的 bean 实例，你应该声明 bean 的作用域的属性为 prototype。同理，如果你想让 Spring 在每次需要时都返回同一个bean实例，你应该声明 bean 的作用域的属性为 singleton。 Spring 框架支持以下五个作用域，如果你使用 web-aware ApplicationContext 时，其中三个是可用的。 作用域 描述 singleton 在spring IoC容器仅存在一个Bean实例，Bean以单例方式存在，默认值(每次get到的都是上次的旧的) prototype 每次从容器中调用Bean时，都返回一个新的实例，即每次调用getBean()时，相当于执行newXxxBean()(获得的是一个全新的对象) request 每次HTTP请求都会创建一个新的Bean，该作用域仅适用于WebApplicationContext环境 session 同一个HTTP Session共享一个Bean，不同Session使用不同的Bean，仅适用于WebApplicationContext环境 global-session 一般用于Portlet应用环境，该运用域仅适用于WebApplicationContext环境 本章将讨论前两个范围，当我们将讨论有关 web-aware Spring ApplicationContext 时，其余三个将被讨论。 singleton 作用域：当一个bean的作用域为Singleton，那么Spring IoC容器中只会存在一个共享的bean实例，并且所有对bean的请求，只要id与该bean定义相匹配，则只会返回bean的同一实例。 Singleton是单例类型，就是在创建起容器时就同时自动创建了一个bean的对象，不管你是否使用，他都存在了，每次获取到的对象都是同一个对象。注意，Singleton作用域是Spring中的缺省作用域(默认值)。你可以在 bean 的配置文件中设置作用域的属性为 singleton，如下所示： 1234&lt;!-- A bean definition with singleton scope --&gt;&lt;bean id=&quot;...&quot; class=&quot;...&quot; scope=&quot;singleton&quot;&gt; &lt;!-- collaborators and configuration for this bean go here --&gt;&lt;/bean&gt; 例子我们在适当的位置使用 Eclipse IDE，然后按照下面的步骤来创建一个 Spring 应用程序： 步骤 描述 1 创建一个名称为 SpringExample 的项目，并且在创建项目的 src 文件夹中创建一个包 com.tutorialspoint。 2 使用 Add External JARs 选项，添加所需的 Spring 库，在 Spring Hello World Example 章节解释。 3 在 com.tutorialspoint 包中创建 Java 类 HelloWorld 和 MainApp。 4 在 src 文件夹中创建 Beans 配置文件 Beans.xml。 5 最后一步是创建的所有 Java 文件和 Bean 配置文件的内容，并运行应用程序，解释如下。 这里是 HelloWorld.java 文件的内容：12345678910package com.tutorialspoint;public class HelloWorld &#123; private String message; public void setMessage(String message)&#123; this.message = message; &#125; public void getMessage()&#123; System.out.println(&quot;Your Message : &quot; + message); &#125;&#125; 下面是 MainApp.java 文件的内容：12345678910111213package com.tutorialspoint;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class MainApp &#123; public static void main(String[] args) &#123; ApplicationContext context = new ClassPathXmlApplicationContext(&quot;Beans.xml&quot;); HelloWorld objA = (HelloWorld) context.getBean(&quot;helloWorld&quot;); objA.setMessage(&quot;I&apos;m object A&quot;); objA.getMessage(); HelloWorld objB = (HelloWorld) context.getBean(&quot;helloWorld&quot;); objB.getMessage(); &#125;&#125; 下面是 singleton 作用域必需的配置文件 Beans.xml：123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;bean id=&quot;helloWorld&quot; class=&quot;com.tutorialspoint.HelloWorld&quot; scope=&quot;singleton&quot;&gt; &lt;/bean&gt;&lt;/beans&gt; 一旦你创建源代码和 bean 配置文件完成后，我们就可以运行该应用程序。如果你的应用程序一切都正常，将输出以下信息：12Your Message : I&apos;m object AYour Message : I&apos;m object A prototype 作用域当一个bean的作用域为Prototype，表示一个bean定义对应多个对象实例。Prototype作用域的bean会导致在每次对该bean请求（将其注入到另一个bean中，或者以程序的方式调用容器的getBean()方法）时都会创建一个新的bean实例。Prototype是原型类型，它在我们创建容器的时候并没有实例化，而是当我们获取bean的时候才会去创建一个对象，而且我们每次获取到的对象都不是同一个对象。根据经验，对有状态的bean应该使用prototype作用域，而对无状态的bean则应该使用singleton作用域。 为了定义 prototype 作用域，你可以在 bean 的配置文件中设置作用域的属性为 prototype，如下所示：1234&lt;!-- A bean definition with singleton scope --&gt;&lt;bean id=&quot;...&quot; class=&quot;...&quot; scope=&quot;prototype&quot;&gt; &lt;!-- collaborators and configuration for this bean go here --&gt;&lt;/bean&gt; 例子我们在适当的位置使用 Eclipse IDE，然后按照下面的步骤来创建一个 Spring 应用程序： 步骤 描述 1 创建一个名称为 SpringExample 的项目，并且在创建项目的 src 文件夹中创建一个包com.tutorialspoint。 2 使用 Add External JARs 选项，添加所需的 Spring 库，解释见 Spring Hello World Example 章节。 3 在 com.tutorialspoint 包中创建 Java 类 HelloWorld 和 MainApp。 4 在 src 文件夹中创建 Beans 配置文件Beans.xml。 5 最后一步是创建的所有 Java 文件和 Bean 配置文件的内容，并运行应用程序，解释如下所示。 这里是 HelloWorld.java 文件的内容：12345678910111213package com.tutorialspoint;public class HelloWorld &#123; private String message; public void setMessage(String message)&#123; this.message = message; &#125; public void getMessage()&#123; System.out.println(&quot;Your Message : &quot; + message); &#125;&#125; 下面是 MainApp.java 文件的内容：12345678910111213package com.tutorialspoint;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class MainApp &#123; public static void main(String[] args) &#123; ApplicationContext context = new ClassPathXmlApplicationContext(&quot;Beans.xml&quot;); HelloWorld objA = (HelloWorld) context.getBean(&quot;helloWorld&quot;); objA.setMessage(&quot;I&apos;m object A&quot;); objA.getMessage(); HelloWorld objB = (HelloWorld) context.getBean(&quot;helloWorld&quot;); objB.getMessage(); &#125;&#125; 下面是 prototype 作用域必需的配置文件 Beans.xml：123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;bean id=&quot;helloWorld&quot; class=&quot;com.tutorialspoint.HelloWorld&quot; scope=&quot;prototype&quot;&gt; &lt;/bean&gt;&lt;/beans&gt; 一旦你创建源代码和 Bean 配置文件完成后，我们就可以运行该应用程序。如果你的应用程序一切都正常，将输出以下信息：12Your Message : I&apos;m object AYour Message : null Spring Bean 生命周期理解 Spring bean 的生命周期很容易。当一个 bean 被实例化时，它可能需要执行一些初始化使它转换成可用状态。同样，当 bean 不再需要，并且从容器中移除时，可能需要做一些清除工作。 尽管还有一些在 Bean 实例化和销毁之间发生的活动，但是本章将只讨论两个重要的生命周期回调方法，它们在 bean 的初始化和销毁的时候是必需的。 为了定义安装和拆卸一个 bean，我们只要声明带有 init-method 和/或 destroy-method 参数的 。init-method 属性指定一个方法，在实例化 bean 时，立即调用该方法。同样，destroy-method 指定一个方法，只有从容器中移除 bean 之后，才能调用该方法。 初始化回调org.springframework.beans.factory.InitializingBean 接口指定一个单一的方法： void afterPropertiesSet() throws Exception;因此，你可以简单地实现上述接口和初始化工作可以在 afterPropertiesSet() 方法中执行，如下所示： 12345public class ExampleBean implements InitializingBean &#123; public void afterPropertiesSet() &#123; // do some initialization work &#125;&#125; 在基于 XML 的配置元数据的情况下，你可以使用 init-method 属性来指定带有 void 无参数方法的名称。例如：12&lt;bean id=&quot;exampleBean&quot; class=&quot;examples.ExampleBean&quot; init-method=&quot;init&quot;/&gt; 下面是类的定义：12345public class ExampleBean &#123; public void init() &#123; // do some initialization work &#125;&#125; 销毁回调org.springframework.beans.factory.DisposableBean 接口指定一个单一的方法： void destroy() throws Exception;因此，你可以简单地实现上述接口并且结束工作可以在 destroy() 方法中执行，如下所示：12345public class ExampleBean implements DisposableBean &#123; public void destroy() &#123; // do some destruction work &#125;&#125; 在基于 XML 的配置元数据的情况下，你可以使用 destroy-method 属性来指定带有 void 无参数方法的名称。例如：12&lt;bean id=&quot;exampleBean&quot; class=&quot;examples.ExampleBean&quot; destroy-method=&quot;destroy&quot;/&gt; 下面是类的定义：12345public class ExampleBean &#123; public void destroy() &#123; // do some destruction work &#125;&#125; 如果你在非 web 应用程序环境中使用 Spring 的 IoC 容器；例如在丰富的客户端桌面环境中；那么在 JVM 中你要注册关闭 hook。这样做可以确保正常关闭，为了让所有的资源都被释放，可以在单个 beans 上调用 destroy 方法。 建议你不要使用 InitializingBean 或者 DisposableBean 的回调方法，因为 XML 配置在命名方法上提供了极大的灵活性。 例子我们在适当的位置使用 Eclipse IDE，然后按照下面的步骤来创建一个 Spring 应用程序： 步骤 描述 1 创建一个名称为 SpringExample 的项目，并且在创建项目的 src 文件夹中创建一个包 com.tutorialspoint。 2 使用 Add External JARs 选项，添加所需的 Spring 库，解释见 Spring Hello World Example 章节。 3 在 com.tutorialspoint 包中创建 Java 类 HelloWorld 和 MainApp。 4 在 src 文件夹中创建 Beans 配置文件 Beans.xml。 5 最后一步是创建的所有 Java 文件和 Bean 配置文件的内容，并运行应用程序，解释如下所示。 这里是 HelloWorld.java 的文件的内容：123456789101112131415161718package com.tutorialspoint;public class HelloWorld &#123; private String message; public void setMessage(String message)&#123; this.message = message; &#125; public void getMessage()&#123; System.out.println(&quot;Your Message : &quot; + message); &#125; public void init()&#123; System.out.println(&quot;Bean is going through init.&quot;); &#125; public void destroy()&#123; System.out.println(&quot;Bean will destroy now.&quot;); &#125;&#125; 下面是 MainApp.java 文件的内容。在这里，你需要注册一个在 AbstractApplicationContext 类中声明的关闭 hook 的 registerShutdownHook() 方法。它将确保正常关闭，并且调用相关的 destroy 方法。1234567891011package com.tutorialspoint;import org.springframework.context.support.AbstractApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class MainApp &#123; public static void main(String[] args) &#123; AbstractApplicationContext context = new ClassPathXmlApplicationContext(&quot;Beans.xml&quot;); HelloWorld obj = (HelloWorld) context.getBean(&quot;helloWorld&quot;); obj.getMessage(); context.registerShutdownHook(); &#125;&#125; 下面是 init 和 destroy 方法必需的配置文件 Beans.xml 文件：1234567891011121314&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;bean id=&quot;helloWorld&quot; class=&quot;com.tutorialspoint.HelloWorld&quot; init-method=&quot;init&quot; destroy-method=&quot;destroy&quot;&gt; &lt;property name=&quot;message&quot; value=&quot;Hello World!&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; 一旦你创建源代码和 bean 配置文件完成后，我们就可以运行该应用程序。如果你的应用程序一切都正常，将输出以下信息：123Bean is going through init.Your Message : Hello World!Bean will destroy now. 默认的初始化和销毁方法如果你有太多具有相同名称的初始化或者销毁方法的 Bean，那么你不需要在每一个 bean 上声明初始化方法和销毁方法。框架使用 元素中的 default-init-method 和 default-destroy-method 属性提供了灵活地配置这种情况，如下所示：123456789101112&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot; default-init-method=&quot;init&quot; default-destroy-method=&quot;destroy&quot;&gt; &lt;bean id=&quot;...&quot; class=&quot;...&quot;&gt; &lt;!-- collaborators and configuration for this bean go here --&gt; &lt;/bean&gt;&lt;/beans&gt; Spring Bean 后置处理器BeanPostProcessor 接口定义回调方法，你可以实现该方法来提供自己的实例化逻辑，依赖解析逻辑等。你也可以在 Spring 容器通过插入一个或多个 BeanPostProcessor 的实现来完成实例化，配置和初始化一个bean之后实现一些自定义逻辑回调方法。 你可以配置多个 BeanPostProcesso r接口，通过设置 BeanPostProcessor 实现的 Ordered 接口提供的 order 属性来控制这些 BeanPostProcessor 接口的执行顺序。 BeanPostProcessor 可以对 bean（或对象）实例进行操作，这意味着 Spring IoC 容器实例化一个 bean 实例，然后 BeanPostProcessor 接口进行它们的工作。 ApplicationContext 会自动检测由 BeanPostProcessor 接口的实现定义的 bean，注册这些 bean 为后置处理器，然后通过在容器中创建 bean，在适当的时候调用它。 例子：下面的例子显示如何在 ApplicationContext 的上下文中编写，注册和使用 BeanPostProcessor。 这里是 HelloWorld.java 文件的内容：12345678910111213141516package com.tutorialspoint;public class HelloWorld &#123; private String message; public void setMessage(String message)&#123; this.message = message; &#125; public void getMessage()&#123; System.out.println(&quot;Your Message : &quot; + message); &#125; public void init()&#123; System.out.println(&quot;Bean is going through init.&quot;); &#125; public void destroy()&#123; System.out.println(&quot;Bean will destroy now.&quot;); &#125;&#125; 这是实现 BeanPostProcessor 的非常简单的例子，它在任何 bean 的初始化的之前和之后输入该 bean 的名称。你可以在初始化 bean 的之前和之后实现更复杂的逻辑，因为你有两个访问内置 bean 对象的后置处理程序的方法。 这里是 InitHelloWorld.java 文件的内容：12345678910111213package com.tutorialspoint;import org.springframework.beans.factory.config.BeanPostProcessor;import org.springframework.beans.BeansException;public class InitHelloWorld implements BeanPostProcessor &#123; public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println(&quot;BeforeInitialization : &quot; + beanName); return bean; // you can return any other object as well &#125; public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println(&quot;AfterInitialization : &quot; + beanName); return bean; // you can return any other object as well &#125;&#125; 下面是 MainApp.java 文件的内容。在这里，你需要注册一个在 AbstractApplicationContext 类中声明的关闭 hook 的 registerShutdownHook() 方法。它将确保正常关闭，并且调用相关的 destroy 方法。1234567891011package com.tutorialspoint;import org.springframework.context.support.AbstractApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class MainApp &#123; public static void main(String[] args) &#123; AbstractApplicationContext context = new ClassPathXmlApplicationContext(&quot;Beans.xml&quot;); HelloWorld obj = (HelloWorld) context.getBean(&quot;helloWorld&quot;); obj.getMessage(); context.registerShutdownHook(); &#125;&#125; 下面是 init 和 destroy 方法需要的配置文件 Beans.xml 文件：123456789101112131415&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;bean id=&quot;helloWorld&quot; class=&quot;com.tutorialspoint.HelloWorld&quot; init-method=&quot;init&quot; destroy-method=&quot;destroy&quot;&gt; &lt;property name=&quot;message&quot; value=&quot;Hello World!&quot;/&gt; &lt;/bean&gt; &lt;bean class=&quot;com.tutorialspoint.InitHelloWorld&quot; /&gt;&lt;/beans&gt; 一旦你创建源代码和 bean 配置文件完成后，我们就可以运行该应用程序。如果你的应用程序一切都正常，将输出以下信息：12345BeforeInitialization : helloWorldBean is going through init.AfterInitialization : helloWorldYour Message : Hello World!Bean will destroy now. Spring Bean 定义继承bean 定义可以包含很多的配置信息，包括构造函数的参数，属性值，容器的具体信息例如初始化方法，静态工厂方法名，等等。 子 bean 的定义继承父定义的配置数据。子定义可以根据需要重写一些值，或者添加其他值。 Spring Bean 定义的继承与 Java 类的继承无关，但是继承的概念是一样的。你可以定义一个父 bean 的定义作为模板和其他子 bean 就可以从父 bean 中继承所需的配置。 当你使用基于 XML 的配置元数据时，通过使用父属性，指定父 bean 作为该属性的值来表明子 bean 的定义。 例子我们在适当的位置使用 Eclipse IDE，然后按照下面的步骤来创建一个 Spring 应用程序： 步骤 描述 1 创建一个名称为 SpringExample 的项目，并且在创建项目的 src 文件夹中创建一个包 com.tutorialspoint。 2 使用 Add External JARs 选项，添加所需的 Spring 库，解释见 Spring Hello World Example 章节。 3 在 com.tutorialspoint 包中创建 Java 类 HelloWorld、HelloIndia 和 MainApp。 4 在 src 文件夹中创建 Beans 配置文件 Beans.xml。 5 最后一步是创建的所有 Java 文件和 Bean 配置文件的内容，并运行应用程序，解释如下所示。 下面是配置文件 Beans.xml，在该配置文件中我们定义有两个属性 message1 和 message2 的 “helloWorld” bean。然后，使用 parent 属性把 “helloIndia” bean 定义为 “helloWorld” bean 的孩子。这个子 bean 继承 message2 的属性，重写 message1 的属性，并且引入一个属性 message3。123456789101112131415161718&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;bean id=&quot;helloWorld&quot; class=&quot;com.tutorialspoint.HelloWorld&quot;&gt; &lt;property name=&quot;message1&quot; value=&quot;Hello World!&quot;/&gt; &lt;property name=&quot;message2&quot; value=&quot;Hello Second World!&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;helloIndia&quot; class=&quot;com.tutorialspoint.HelloIndia&quot; parent=&quot;helloWorld&quot;&gt; &lt;property name=&quot;message1&quot; value=&quot;Hello India!&quot;/&gt; &lt;property name=&quot;message3&quot; value=&quot;Namaste India!&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; 这里是 HelloWorld.java 文件的内容：1234567891011121314151617package com.tutorialspoint;public class HelloWorld &#123; private String message1; private String message2; public void setMessage1(String message)&#123; this.message1 = message; &#125; public void setMessage2(String message)&#123; this.message2 = message; &#125; public void getMessage1()&#123; System.out.println(&quot;World Message1 : &quot; + message1); &#125; public void getMessage2()&#123; System.out.println(&quot;World Message2 : &quot; + message2); &#125;&#125; 这里是 HelloIndia.java 文件的内容：12345678910111213141516171819202122232425262728293031package com.tutorialspoint;public class HelloIndia &#123; private String message1; private String message2; private String message3; public void setMessage1(String message)&#123; this.message1 = message; &#125; public void setMessage2(String message)&#123; this.message2 = message; &#125; public void setMessage3(String message)&#123; this.message3 = message; &#125; public void getMessage1()&#123; System.out.println(&quot;India Message1 : &quot; + message1); &#125; public void getMessage2()&#123; System.out.println(&quot;India Message2 : &quot; + message2); &#125; public void getMessage3()&#123; System.out.println(&quot;India Message3 : &quot; + message3); &#125;&#125; 下面是 MainApp.java 文件的内容：1234567891011121314151617181920package com.tutorialspoint;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class MainApp &#123; public static void main(String[] args) &#123; ApplicationContext context = new ClassPathXmlApplicationContext(&quot;Beans.xml&quot;); HelloWorld objA = (HelloWorld) context.getBean(&quot;helloWorld&quot;); objA.getMessage1(); objA.getMessage2(); HelloIndia objB = (HelloIndia) context.getBean(&quot;helloIndia&quot;); objB.getMessage1(); objB.getMessage2(); objB.getMessage3(); &#125;&#125; 一旦你创建源代码和 bean 配置文件完成后，我们就可以运行该应用程序。如果你的应用程序一切都正常，将输出以下信息：12345World Message1 : Hello World!World Message2 : Hello Second World!India Message1 : Hello India!India Message2 : Hello Second World!India Message3 : Namaste India! 在这里你可以观察到，我们创建 “helloIndia” bean 的同时并没有传递 message2，但是由于 Bean 定义的继承，所以它传递了 message2。 Bean 定义模板你可以创建一个 Bean 定义模板，不需要花太多功夫它就可以被其他子 bean 定义使用。在定义一个 Bean 定义模板时，你不应该指定类的属性，而应该指定带 true 值的抽象属性，如下所示：12345678910111213141516171819&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;bean id=&quot;beanTeamplate&quot; abstract=&quot;true&quot;&gt; &lt;property name=&quot;message1&quot; value=&quot;Hello World!&quot;/&gt; &lt;property name=&quot;message2&quot; value=&quot;Hello Second World!&quot;/&gt; &lt;property name=&quot;message3&quot; value=&quot;Namaste India!&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;helloIndia&quot; class=&quot;com.tutorialspoint.HelloIndia&quot; parent=&quot;beanTeamplate&quot;&gt; &lt;property name=&quot;message1&quot; value=&quot;Hello India!&quot;/&gt; &lt;property name=&quot;message3&quot; value=&quot;Namaste India!&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; 父 bean 自身不能被实例化，因为它是不完整的，而且它也被明确地标记为抽象的。当一个定义是抽象的，它仅仅作为一个纯粹的模板 bean 定义来使用的，充当子定义的父定义使用。 主观总结:在使用初始化和摧毁的函数的时候可以使用初始化和摧毁的接口不调用context.registerShutdownHook(); 似乎摧毁的函数出不来BeanPostProcessor 似乎是自动调用的,在当要初始化一个bean和初始化完成后的时候,id 是用来在mainapp里面get bean的]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Pass 1]]></title>
    <url>%2F2018%2F09%2F10%2Fspring1%2F</url>
    <content type="text"><![CDATA[教材:https://www.w3cschool.cn/wkspring/ Spring 概述spring 是最受欢迎的企业级 Java 应用程序开发框架，数以百万的来自世界各地的开发人员使用 Spring 框架来创建性能好、易于测试、可重用的代码。 Spring 框架是一个开源的 Java 平台，它最初是由 Rod Johnson 编写的，并且于 2003 年 6 月首次在 Apache 2.0 许可下发布。 Spring 是轻量级的框架，其基础版本只有 2 MB 左右的大小。 Spring 框架的核心特性是可以用于开发任何 Java 应用程序，但是在 Java EE 平台上构建 web 应用程序是需要扩展的。 Spring 框架的目标是使 J2EE 开发变得更容易使用，通过启用基于 POJO 编程模型 来促进良好的编程实践。 pojo 实际意义就是普通的JavaBeans（简单的实体类），特点就是支持业务逻辑的协助类。POJO类的作用是方便程序员使用数据库中的数据表，对于程序员来说，可以很方便的将POJO类当作对象来进行使用，也可以方便的调用其get，set方法。但不允许有业务方法,也不能携带有connection之类的方法，即不包含业务逻辑或持久逻辑等。 使用 Spring 框架的好处下面列出的是使用 Spring 框架主要的好处： Spring 可以使开发人员使用 POJOs 开发企业级的应用程序。只使用 POJOs 的好处是你不需要一个 EJB 容器产品，比如一个应用程序服务器，但是你可以选择使用一个健壮的 servlet 容器，比如 Tomcat 或者一些商业产品。 Spring 在一个单元模式中是有组织的。即使包和类的数量非常大，你只要担心你需要的，而其它的就可以忽略了。 Spring 不会让你白费力气做重复工作，它真正的利用了一些现有的技术，像ORM 框架、日志框架、JEE、Quartz 和 JDK 计时器，其他视图技术。 测试一个用 Spring 编写的应用程序很容易，因为环境相关的代码被移动到这个框架中。此外，通过使用 JavaBean-style POJOs，它在使用依赖注入注入测试数据时变得更容易。 Spring 的 web 框架是一个设计良好的 web MVC 框架，它为比如 Structs 或者其他工程上的或者不怎么受欢迎的 web 框架提供了一个很好的供替代的选择。 Spring 对JavaEE开发中非常难用的一些API（JDBC、JavaMail、远程调用等），都提供了封装，使这些API应用难度大大降低。 轻量级的 IOC 容器往往是轻量级的，例如，特别是当与 EJB 容器相比的时候。这有利于在内存和 CPU 资源有限的计算机上开发和部署应用程序。 Spring提供了一致的事务管理接口，可向下扩展到（使用一个单一的数据库，例如）本地事务并扩展到全局事务（例如，使用 JTA）。 依赖注入（DI）Spring 最认同的技术是 控制反转的依赖注入（DI）模式。控制反转（IoC） 是一个通用的概念，它可以用许多不同的方式去表达，依赖注入仅仅是控制反转的一个具体的例子。 当编写一个复杂的 Java 应用程序时，应用程序类应该尽可能的 独立于 其他的 Java 类来增加这些类可重用可能性，当进行单元测试时，可以使它们独立于其他类进行测试。依赖注入（或者有时被称为配线）有助于将这些类粘合在一起，并且在同一时间让它们保持独立。 到底什么是依赖注入？让我们将这两个词分开来看一看。这里将依赖关系部分转化为两个类之间的关联。例如，类 A 依赖于类 B。现在，让我们看一看第二部分，注入。所有这一切都意味着类 B 将通过 IoC 被注入到类 A 中。 依赖注入可以以向构造函数传递参数的方式发生，或者通过使用 setter 方法 post-construction。由于依赖注入是 Spring 框架的核心部分，所以我将在一个单独的章节中利用很好的例子去解释这一概念。(依赖注入可以使用多种方式实现) 面向方面的程序设计（AOP）Spring 框架的一个关键组件是面向方面的程序设计（AOP）框架。一个程序中跨越多个点的功能被称为横切关注点，这些横切关注点在概念上独立于应用程序的业务逻辑。有各种各样常见的很好的关于方面的例子，比如日志记录、声明性事务、安全性，和缓存等等。 在 OOP (面向对象)中模块化的关键单元是类，而在 AOP(面向方面) 中模块化的关键单元是方面。AOP 帮助你将横切关注点从它们所影响的对象中分离出来，然而依赖注入帮助你将你的应用程序对象从彼此中分离出来。 Spring 框架的 AOP 模块提供了面向方面的程序设计实现，可以定义诸如方法拦截器和切入点等，从而使实现功能的代码彻底的解耦出来。使用源码级的元数据，可以用类似于.Net属性的方式合并行为信息到代码中。我将在一个独立的章节中讨论更多关于 Spring AOP 的概念。 体系结构Spring 有可能成为所有企业应用程序的一站式服务点，然而，Spring 是模块化的，允许你挑选和选择适用于你的模块，不必要把剩余部分也引入。下面的部分对在 Spring 框架中所有可用的模块给出了详细的介绍。 Spring 框架提供约 20 个模块，可以根据应用程序的要求来使用。 核心容器核心容器由spring-core，spring-beans，spring-context，spring-context-support和spring-expression（SpEL，Spring表达式语言，Spring Expression Language）等模块组成，它们的细节如下： spring-core模块提供了框架的基本组成部分，包括 IoC 和依赖注入功能。 spring-beans 模块提供 BeanFactory，工厂模式的微妙实现，它移除了编码式单例的需要，并且可以把配置和依赖从实际编码逻辑中解耦。 context模块建立在由core和 beans 模块的基础上建立起来的，它以一种类似于JNDI注册的方式访问对象。Context模块继承自Bean模块，并且添加了国际化（比如，使用资源束）、事件传播、资源加载和透明地创建上下文（比如，通过Servelet容器）等功能。Context模块也支持Java EE的功能，比如EJB、JMX和远程调用等。ApplicationContext接口是Context模块的焦点。spring-context-support提供了对第三方库集成到Spring上下文的支持，比如缓存（EhCache, Guava, JCache）、邮件（JavaMail）、调度（CommonJ, Quartz）、模板引擎（FreeMarker, JasperReports, Velocity）等。 spring-expression模块提供了强大的表达式语言，用于在运行时查询和操作对象图。它是JSP2.1规范中定义的统一表达式语言的扩展，支持set和get属性值、属性赋值、方法调用、访问数组集合及索引的内容、逻辑算术运算、命名变量、通过名字从Spring IoC容器检索对象，还支持列表的投影、选择以及聚合等。。 数据访问/集成数据访问/集成层包括 JDBC，ORM，OXM，JMS 和事务处理模块，它们的细节如下： （注：JDBC=Java Data Base Connectivity，ORM=Object Relational Mapping，OXM=Object XML Mapping，JMS=Java Message Service） JDBC 模块提供了JDBC抽象层，它消除了冗长的JDBC编码和对数据库供应商特定错误代码的解析。 ORM 模块提供了对流行的对象关系映射API的集成，包括JPA、JDO和Hibernate等。通过此模块可以让这些ORM框架和spring的其它功能整合，比如前面提及的事务管理。 OXM 模块提供了对OXM实现的支持，比如JAXB、Castor、XML Beans、JiBX、XStream等。 JMS 模块包含生产（produce）和消费（consume）消息的功能。从Spring 4.1开始，集成了spring-messaging模块。。 事务模块为实现特殊接口类及所有的 POJO 支持编程式和声明式事务管理。（注：编程式事务需要自己写beginTransaction()、commit()、rollback()等事务管理方法，声明式事务是通过注解或配置由spring自动处理，编程式事务粒度更细） WebWeb 层由 Web，Web-MVC，Web-Socket 和 Web-Portlet 组成，它们的细节如下： Web 模块提供面向web的基本功能和面向web的应用上下文，比如多部分（multipart）文件上传功能、使用Servlet监听器初始化IoC容器等。它还包括HTTP客户端以及Spring远程调用中与web相关的部分。。 Web-MVC 模块为web应用提供了模型视图控制（MVC）和REST Web服务的实现。Spring的MVC框架可以使领域模型代码和web表单完全地分离，且可以与Spring框架的其它所有功能进行集成。 Web-Socket 模块为 WebSocket-based 提供了支持，而且在 web 应用程序中提供了客户端和服务器端之间通信的两种方式。 Web-Portlet 模块提供了用于Portlet环境的MVC实现，并反映了spring-webmvc模块的功能。 其他还有其他一些重要的模块，像 AOP，Aspects，Instrumentation，Web 和测试模块，它们的细节如下： AOP 模块提供了面向方面的编程实现，允许你定义方法拦截器和切入点对代码进行干净地解耦，从而使实现功能的代码彻底的解耦出来。使用源码级的元数据，可以用类似于.Net属性的方式合并行为信息到代码中。 Aspects 模块提供了与 AspectJ 的集成，这是一个功能强大且成熟的面向切面编程（AOP）框架。 Instrumentation 模块在一定的应用服务器中提供了类 instrumentation 的支持和类加载器的实现。 Messaging 模块为 STOMP 提供了支持作为在应用程序中 WebSocket 子协议的使用。它也支持一个注解编程模型，它是为了选路和处理来自 WebSocket 客户端的 STOMP 信息。 测试模块支持对具有 JUnit 或 TestNG 框架的 Spring 组件的测试。 Spring IoC 容器IoC 容器Spring 容器 是 Spring 框架的核心。容器将创建对象，把它们连接在一起，配置它们，并管理他们的整个生命周期从创建到销毁。 Spring 容器使用 依赖注入（DI） 来管理组成一个应用程序的组件。这些对象被称为 Spring Beans，我们将在下一章中进行讨论。 通过阅读配置元数据(通过xml的配置)提供的指令，容器知道对哪些对象进行实例化，配置和组装。配置元数据可以通过 XML，Java 注释或 Java 代码来表示。下图是 Spring 如何工作的高级视图。 Spring IoC 容器利用 Java 的 POJO 类和配置元数据来生成完全配置和可执行的系统或应用程序。 Spring 提供了以下两种不同类型的容器。 序号 容器 &amp; 描述 1 Spring BeanFactory 容器 它是最简单的容器，给 DI 提供了基本的支持，它用 org.springframework.beans.factory.BeanFactory 接口来定义。BeanFactory 或者相关的接口，如 BeanFactoryAware，InitializingBean，DisposableBean，在 Spring 中仍然存在具有大量的与 Spring 整合的第三方框架的反向兼容性的目的。 2 Spring ApplicationContext 容器 该容器添加了更多的企业特定的功能，例如从一个属性文件中解析文本信息的能力，发布应用程序事件给感兴趣的事件监听器的能力。该容器是由 org.springframework.context.ApplicationContext 接口定义。 ApplicationContext 容器包括 BeanFactory 容器的所有功能，所以通常建议超过 BeanFactory。BeanFactory 仍然可以用于轻量级的应用程序，如移动设备或基于 applet 的应用程序，其中它的数据量和速度是显著 Spring BeanFactory 容器这是一个最简单的容器，它主要的功能是为 依赖注入 （DI） 提供支持，这个容器接口在 org.springframework.beans.factory.BeanFactor 中被定义。 BeanFactory 和相关的接口，比如BeanFactoryAware、 DisposableBean、InitializingBean，仍旧保留在 Spring 中，主要目的是向后兼容已经存在的和那些 Spring 整合在一起的第三方框架。 在 Spring 中，有大量对 BeanFactory 接口的实现。其中，最常被使用的是 XmlBeanFactory 类。这个容器从一个 XML 文件中读取配置元数据，由这些元数据来生成一个被配置化的系统或者应用。 在资源宝贵的移动设备或者基于 applet 的应用当中， BeanFactory 会被优先选择。否则，一般使用的是ApplicationContext，除非你有更好的理由选择 BeanFactory。 例子：假设我们已经安装 Eclipse IDE，按照下面的步骤，我们可以创建一个 Spring 应用程序。 步骤 描述 1 创建一个名为 SpringExample 的工程并在 src 文件夹下新建一个名为 com.tutorialspoint 文件夹。 2 点击右键，选择 Add External JARs 选项，导入 Spring 的库文件，正如我们在 Spring Hello World Example 章节中提到的导入方式。 3 在 com.tutorialspoint 文件夹下创建 HelloWorld.java 和 MainApp.java 两个类文件。 4 在 src 文件夹下创建 Bean 的配置文件 Beans.xml 5 最后的步骤是创建所有 Java 文件和 Bean 的配置文件的内容，按照如下所示步骤运行应用程序。 Spring ApplicationContext 容器Application Context 是 spring 中较高级的容器。和 BeanFactory 类似，它可以加载配置文件中定义的 bean，将所有的 bean 集中在一起，当有请求的时候分配 bean。 另外，它增加了企业所需要的功能，比如，从属性文件中解析文本信息和将事件传递给所指定的监听器。这个容器在 org.springframework.context.ApplicationContext interface 接口中定义。 ApplicationContext 包含 BeanFactory 所有的功能，一般情况下，相对于 BeanFactory，ApplicationContext 会更加优秀。当然，BeanFactory 仍可以在轻量级应用中使用，比如移动设备或者基于 applet 的应用程序。(一般都用appplictoncontext) 最常被使用的 ApplicationContext 接口实现： FileSystemXmlApplicationContext：该容器从 XML 文件中加载已被定义的 bean。在这里，你需要提供给构造器 XML 文件的完整路径。 ClassPathXmlApplicationContext：该容器从 XML 文件中加载已被定义的 bean。在这里，你不需要提供 XML 文件的完整路径，只需正确配置 CLASSPATH 环境变量即可，因为，容器会从 CLASSPATH 中搜索 bean 配置文件。 WebXmlApplicationContext：该容器会在一个 web 应用程序的范围内加载在 XML 文件中已被定义的 bean。 我们已经在 Spring Hello World Example章节中看到过 ClassPathXmlApplicationContext 容器，并且，在基于 spring 的 web 应用程序这个独立的章节中，我们讨论了很多关于 XmlWebApplicationContext。所以，接下来，让我们看一个关于 FileSystemXmlApplicationContext 的例子。 例子:假设我们已经安装 Eclipse IDE，按照下面的步骤，我们可以创建一个 Spring 应用程序。 步骤 描述 1 创建一个名为 SpringExample 的工程， 在 src 下新建一个名为 com.tutorialspoint 的文件夹src 2 点击右键，选择 Add External JARs 选项，导入 Spring 的库文件，正如我们在 Spring Hello World Example 章节中提到的导入方式。 3 在 com.tutorialspoint 文件夹下创建 HelloWorld.java 和 MainApp.java 两个类文件。 4 文件夹下创建 Bean 的配置文件 Beans.xml。 5 最后的步骤是编辑所有 JAVA 文件的内容和 Bean 的配置文件,按照以前我们讲的那样去运行应用程序。 Spring Bean 定义被称作 bean 的对象是构成应用程序的支柱也是由 Spring IoC 容器 管理的。bean 是一个被实例化，组装，并通过 Spring IoC 容器所管理的对象。这些 bean 是由用容器提供的配置元数据创建的，例如，已经在先前章节看到的，在 XML 的表单中的 定义。 bean 定义包含称为配置元数据的信息，下述容器也需要知道配置元数据： 如何创建一个 bean bean 的生命周期的详细信息 bean 的依赖关系 上述所有的配置元数据转换成一组构成每个 bean 定义的下列属性。 属性 描述 class 这个属性是强制性的，并且指定用来创建 bean 的 bean 类。 name 这个属性指定唯一的 bean 标识符。在基于 XML 的配置元数据中，你可以使用 ID 和/或 name 属性来指定 bean 标识符。 scope 这个属性指定由特定的 bean 定义创建的对象的作用域，它将会在 bean 作用域的章节中进行讨论。 constructor-arg 它是用来注入依赖关系的，并会在接下来的章节中进行讨论。 properties 它是用来注入依赖关系的，并会在接下来的章节中进行讨论。 autowiring mode 它是用来注入依赖关系的，并会在接下来的章节中进行讨论。 lazy-initialization mode 延迟初始化的 bean 告诉 IoC 容器在它第一次被请求时，而不是在启动时去创建一个 bean 实例。 initialization 方法 在 bean 的所有必需的属性被容器设置之后，调用回调方法。它将会在 bean 的生命周期章节中进行讨论。 destruction 方法 当包含该 bean 的容器被销毁时，使用回调方法。它将会在 bean 的生命周期章节中进行讨论。 Spring 配置元数据Spring IoC 容器完全由实际编写的配置元数据的格式解耦。有下面三个重要的方法把配置元数据提供给 Spring 容器： 基于 XML 的配置文件。 基于注解的配置 基于 Java 的配置 你已经看到了如何把基于 XML 的配置元数据提供给容器，但是让我们看看另一个基于 XML 配置文件的例子，这个配置文件中有不同的 bean 定义，包括延迟初始化，初始化方法和销毁方法的：123456789101112131415161718192021222324252627282930&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd&quot;&gt; &lt;!-- A simple bean definition --&gt; &lt;bean id=&quot;...&quot; class=&quot;...&quot;&gt; &lt;!-- collaborators and configuration for this bean go here --&gt; &lt;/bean&gt; &lt;!-- A bean definition with lazy init set on --&gt; &lt;bean id=&quot;...&quot; class=&quot;...&quot; lazy-init=&quot;true&quot;&gt; &lt;!-- collaborators and configuration for this bean go here --&gt; &lt;/bean&gt; &lt;!-- A bean definition with initialization method --&gt; &lt;bean id=&quot;...&quot; class=&quot;...&quot; init-method=&quot;...&quot;&gt; &lt;!-- collaborators and configuration for this bean go here --&gt; &lt;/bean&gt; &lt;!-- A bean definition with destruction method --&gt; &lt;bean id=&quot;...&quot; class=&quot;...&quot; destroy-method=&quot;...&quot;&gt; &lt;!-- collaborators and configuration for this bean go here --&gt; &lt;/bean&gt; &lt;!-- more bean definitions go here --&gt;&lt;/beans&gt; 你可以查看 Spring Hello World 实例 来理解如何定义，配置和创建 Spring Beans。 关于基于注解的配置将在一个单独的章节中进行讨论。刻意把它保留在一个单独的章节，是因为我想让你在开始使用注解和 Spring 依赖注入编程之前，能掌握一些其他重要的 Spring 概念。 本次学习心得:容器通过获取xml配置文件中的id得到class文件,通过class文件得到对象,容器可以获得多个对象,成为一个对象集合,和我分析json文件有相同的感觉]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vigenere_cipher(维西尼亚)密码]]></title>
    <url>%2F2018%2F09%2F07%2Fvigenere-cipher%2F</url>
    <content type="text"><![CDATA[Vigenere Cipher凯撒密码凯撒密码是一种简单的加密方法，即将文本中的每一个字符都位移相同的位置。 如选定位移3位：12原文：a b c密文：d e f 由于出现了字母频度分析，凯撒密码变得很容易破解。“如果我们知道一条加密信息所使用的语言，那么破译这条加密信息的方法就是找出同样的语言写的一篇其他文章，大约一页纸长，然后我们计算其中每个字母的出现频率。我们将 频率最高的字母标为1号，频率排第2的标为2号，第三标为3号，依次类推，直到数完样品文章中所有字母。然后我们观察需要破译的密文，同样分类出所有的字母，找出频率最高的字母，并全部用样本文章中最高频率的字母替换。第二高频的字母用样本中2号代替，第三则用3号替换，直到密文中所有字母均已被样本中的字母替换。” (0.0 厉害啊) 维吉尼亚密码分解后实则就是多个凯撒密码，只要知道密钥的长度，我们就可以将其分解。如密文为：ABCDEFGHIJKLMN如果我们知道密钥长度为3，就可将其分解为三组：123组1：A D G J N组2：B E H K组3：C F I M 分解后每组就是一个凯撒密码，即组内的位移量是一致的，对每一组即可用频度分析法来解密。所以破解维吉尼亚密码的关键就是确定密钥的长度。两种方法确定密钥的长度一种是Kasiski测试法:确定两个相同字符串之间长度(至少为3)的公因数(目前用这个)一种是重合指数法定义公式： pi(字母的出现频率–已知) 常用公式：xi为字母的频数，L为密文的长度 英语文本的重合指数为P(A)^2 + P(B)^2+……+P(Z)^2 = 0.065利用重合指数推测密钥长度的原理在于，对于一个由凯撒密码加密的序列，由于所有字母的位移程度相同，所以密文的重合指数应等于原文语言的重合指数假设使用维吉尼亚密码加密的密文串为y=y1y2…yn。将串y分割成m个长度相等的子串(m组)，分别为y1，y2，…，ym，这样就可以以列的形式写出密文，组成一个m×(n/m)矩阵。矩阵的每一行对应于子串yi,(每一行对应一组) 1≤i≤m 如果y1，y2，…，ym按如上方式构造，则m实际上就是密钥字的长度，每一组的CI值大约为0.065。 另外，如果m不是密钥字的长度，那么子串yi看起来更为随机，因为它们是通过不同密钥以移位加密方式获得的。对于一个随机串，其重合指数为0.038 (通过猜测密钥的长度,看是否接近0.065) 在知道了密钥长度n以后，就可将密文分解为n组，每一组都是一个凯撒密码，然后对每一组用字母频度分析进行解密，和在一起就能成功解密凯撒密码 首先对子密文段重各个字母的频率进行统计（记为fi, i∈a – z），查看字母频率分布统计概率表(记pi)，计算子密文段长度为n，使用公式 计算出M0，然后对子密文段移位25次，同样按照上述方法求出M1 — M25的值 根据重合指数的定义知：一个有意义的英文文本，M ≈0.065，所以找出M值接近0.065的移位数，就是秘钥中的对应字母 推荐网站 http://www.richkni.co.uk/php/crypta/vignere.php解密很有用(0.0) ps:这篇文章虽然被分类到python中,但还没有用python实现 orz]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[frequency_analysis(频率分析)]]></title>
    <url>%2F2018%2F09%2F06%2Ffrequency-analysis%2F</url>
    <content type="text"><![CDATA[frequency_analysisIn cryptanalysis, frequency analysis is the study of the frequency of letters or groups of letters in a ciphertext. The method is used as an aid(0.0) to breaking classical ciphers. Frequency analysis is based on the fact that, in any given stretch(范围) of written language, certain letters and combinations of letters occur with varying frequencies. Moreover, there is a characteristic distribution of letters that is roughly the same for almost all samples of that language. For instance, given a section of English language, E, T, A and O are the most common, while Z, Q and X are rare. Likewise, TH, ER, ON, and AN are the most common pairs of letters (termed bigrams or digraphs), and SS, EE, TT, and FF are the most common repeats. The nonsense phrase “ETAOIN SHRDLU” represents the 12 most frequent letters in typical English language text. In some ciphers, such properties of the natural language plaintext are preserved in the ciphertext, and these patterns have the potential to be exploited in a ciphertext-only attack. 代码区: 统计:1234567891011121314151617def start(): filename=input(&quot;输入文件名称:&quot;) print(&quot;-----文件名: &quot;+filename+&quot;-------&quot;) with open(filename, &apos;r&apos;) as f: file = f.read() d=&#123;&#125; for i in file: if(i.isalpha()): if(d.get(i) == None): d[i] = 0 d[i] = d.get(i) + 1 filename = filename+&quot;_result&quot; with open(filename,&quot;w&quot;) as f: for k,v in d.items(): string =str(v)+&quot; &quot;+k+&quot;\n&quot; #将values写在前面有助于sort(偷懒) f.write(string) return &quot;done&quot; 使用 sort 进行排序sort -n -r file 可以参考频率表 频率分析网站 http://www.richkni.co.uk/php/crypta/freq.php然后猜猜猜(划重点)然而对于我来说是太难了 0.0毕竟母语不是英文,对单词模式没有辨识度]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用凯撒加密法进行加密解密]]></title>
    <url>%2F2018%2F09%2F06%2Fcaesar-cipher%2F</url>
    <content type="text"><![CDATA[凯撒加密法凯撒加密法，或称恺撒加密、恺撒变换、变换加密，是一种最简单且最广为人知的加密技术。它是一种替换加密的技术，明文中的所有字母都在字母表上向后（或向前）按照一个固定数目进行偏移后被替换成密文。 代码区主要思想:与rot13方式相似,向后向前偏移进行加密解密加密123456789101112131415161718def encrypt_caesar(): shift=input(&quot;输入偏移量:&quot;) if(not shift.isdigit()): return &quot;请输入数字!&quot; src=input(&quot;输入加密字符串:&quot;) result = &quot;&quot; for x in src: if(x.isalpha()): if(x.islower()): x=x.upper() x=ord(x) x=x+int(shift) if(x &gt; 90): x = x - 26 x = chr(x) result = result + x return result 运行结果 解密123456789101112131415161718def decrypt_caesar(): shift=input(&quot;输入偏移量:&quot;) if(not shift.isdigit()): return &quot;请输入数字!&quot; src=input(&quot;输入解密字符串:&quot;) result = &quot;&quot; for x in src: if(x.isalpha()): if(x.isupper()): x=x.lower() x=ord(x) x=x-int(shift) if(x &lt; 96): x = x + 26 x = chr(x) result = result + x return result 运行结果 ps:linux还可以使用tr进行解密 具体可见rot13下面]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用rot13加密解密]]></title>
    <url>%2F2018%2F09%2F05%2Frot13%2F</url>
    <content type="text"><![CDATA[使用ROT13加密解密ROT13（回转13位）是一种简易的替换式密码算法。它是一种在英文网络论坛用作隐藏八卦、妙句、谜题解答以及某些脏话的工具，目的是逃过版主或管理员的匆匆一瞥。ROT13 也是过去在古罗马开发的凯撒密码的一种变体。ROT13是它自身的逆反，即：要还原成原文只要使用同一算法即可得，故同样的操作可用于加密与解密。该算法并没有提供真正密码学上的保全，故它不应该被用于需要保全的用途上。它常常被当作弱加密示例的典型。 应用ROT13到一段文字上仅仅只需要检查字母顺序并取代它在13位之后的对应字母，有需要超过时则重新绕回26英文字母开头即可。A换成N、B换成O、依此类推到M换成Z，然后串行反转：N换成A、O换成B、最后Z换成M（如图所示）。只有这些出现在英文字母里的字符受影响；数字、符号、空白字符以及所有其他字符都不变。替换后的字母大小写保持不变。 代码区:主要思想:将所有的字符向后移动13位就行了,但要保持大写和小写其余其他字符不变 加密与解密是相同的函数 加密与解密:1234567891011121314151617def encrypt_rot13(): src=input(&quot;输入加密字符串:&quot;) result = &quot;&quot; for x in src: if(x.isalpha()): if(x.isupper()): x = ord(x)+13 if(x&gt;90): x=x-26 else: x = ord(x)+13 if(x&gt;122): x=x-26 result = result + chr(x) else: result = result + x return result 运行过程加密解密 linux下使用加密rot13加密解密加密:tr &#39;A-Za-z&#39; &#39;N-ZA-Mn-za-m&#39; &lt;&lt;&lt; &quot;The Quick Brown Fox Jumps Over The Lazy Dog&quot; 解密:echo &quot;The Quick Brown Fox Jumps Over The Lazy Dog&quot; |tr &#39;N-ZA-Mn-za-m&#39; &#39;A-Za-z&#39;]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用base64编码加密解密]]></title>
    <url>%2F2018%2F09%2F05%2Fencrypt-base64%2F</url>
    <content type="text"><![CDATA[Base64编码简介Base64这个术语最初是在“MIME内容传输编码规范”中提出的。Base64不是一种加密算法，虽然编码后的字符串看起来有点加密的赶脚。它实际上是一种“二进制到文本”的编码方法，它能够将给定的任意二进制数据转换（映射）为ASCII字符串的形式，以便在只支持文本的环境中也能够顺利地传输二进制数据。例如支持MIME的电子邮件应用，或需要在XML中存储复杂数据（例如图片）时。 要实现Base64，首先需要选取适当的64个字符组成字符集。一条通用的原则是从某种常用字符集中选取64个可打印字符，这样就能避免在传输过程中丢失数据（不可打印字符在传输过程中可能会被当做特殊字符处理，从而导致丢失）。例如，MIME的Base64实现选用了大写字母、小写字母和0~9的数字作为前62个字符。其他实现通常会沿用MIME的这种方式，而仅仅在最后2个字符上有所不同，例如UTF-7编码。 我们经常会在Base64编码字符串中看到最后有”=”字符，这就是通过填充生成的。填充就是当出现编码时的情况2和3时，在后面补上”=”字符，使编码后的字符数为4的倍数。 下面这段文本： Man is distinguished, not only by his reason, but by this singular passion from otheranimals, which is a lust of the mind, that by a perseverance of delight in the continued andindefatigable generation of knowledge, exceeds the short vehemence of any carnal pleasure. 通过MIME Base64进行转换后就成为： TWFuIGlzIGRpc3Rpbmd1aXNoZWQsIG5vdCBvbmx5IGJ5IGhpcyByZWFzb24sIGJ1dCBieSB0aGlzIHNpbmd1bGFyIHBhc3Npb24gZnJvbSBvdGhlciBhbmltYWxzLCB3aGljaCBpcyBhIGx1c3Qgb2YgdGhlIG1pbmQsIHRoYXQgYnkgYSBwZXJzZXZlcmFuY2Ugb2YgZGVsaWdodCBpbiB0aGUgY29udGludWVkIGFuZCBpbmRlZmF0aWdhYmxlIGdlbmVyYXRpb24gb2Yga25vd2xlZGdlLCBleGNlZWRzIHRoZSBzaG9ydCB2ZWhlbWVuY2Ugb2YgYW55IGNhcm5hbCBwbGVhc3VyZS4= 开发Base64的目的就不是为了加密，而是为了方便在文本环境中传输二进制数据 所以，与开发一个加密算法不同，安全性并不是Base64的目标，只是它的一个副产物。 实际上，Base64的安全性是非常差的，这就是在实际应用中不用它加密的原因。如果你对常用加密方法有所了解的话，你应该知道有一种古老的加密方法，称为“字符替换法”。即指定一个规则，将每个字符用其他字符替换，例如将a变为c、b变为d等，这样替换后生成的结果就是密文。解密时只需要反过来操作，将c变为a、将d变为b就可以了。用不同的替换规则加密，生成的密文也不同。 用Base64来加密实际上就相当于字符替换，只不过它先对字节做了一些变换，然后再进行替换，对加密过程来说，本质上是一样的。 字符替换法 虽然简单，但却是一个伟大的发明，它被使用了超过1千年，一直都没有有效的方法来破解它。后来人们终于发现了它的弱点：基于词频和字母频率的统计规律，就能够轻松得到它的密钥。从那以后，加密者与解密者之间的战争从来就没有停歇过，加密者不断发明更复杂更安全的加密算法，解密者则绞尽脑汁去破解它们。喽。 代码区:主要思想:将信息的每一个字母抓换为ascii码,并且将所有的ascii码连接起来,补上不足的0,使得长度能被6整除,将他们以6的间距分开,将之转化为array中的字母,加上=号(是0数量的1/2) 加密与解密是相对的 使用同一数组array12345678910array=[ &apos;A&apos;, &apos;B&apos;, &apos;C&apos;, &apos;D&apos;, &apos;E&apos;, &apos;F&apos;, &apos;G&apos;, &apos;H&apos;, &apos;I&apos;, &apos;J&apos;, &apos;K&apos;, &apos;L&apos;, &apos;M&apos;, &apos;N&apos;, &apos;O&apos;, &apos;P&apos;, &apos;Q&apos;, &apos;R&apos;, &apos;S&apos;, &apos;T&apos;, &apos;U&apos;, &apos;V&apos;, &apos;W&apos;, &apos;X&apos;, &apos;Y&apos;, &apos;Z&apos;, &apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;, &apos;f&apos;, &apos;g&apos;, &apos;h&apos;, &apos;i&apos;, &apos;j&apos;, &apos;k&apos;, &apos;l&apos;, &apos;m&apos;, &apos;n&apos;, &apos;o&apos;, &apos;p&apos;, &apos;q&apos;, &apos;r&apos;, &apos;s&apos;, &apos;t&apos;, &apos;u&apos;, &apos;v&apos;, &apos;w&apos;, &apos;x&apos;, &apos;y&apos;, &apos;z&apos;, &apos;0&apos;, &apos;1&apos;, &apos;2&apos;, &apos;3&apos;, &apos;4&apos;, &apos;5&apos;, &apos;6&apos;, &apos;7&apos;, &apos;8&apos;, &apos;9&apos;, &apos;+&apos;, &apos;/&apos;] 加密:123456789101112131415161718192021222324&apos;&apos;&apos;使用base64方式加密&apos;&apos;&apos;def encrypt_base64(): src=input(&quot;加密字符串:&quot;) tmp = &quot;&quot; result=&quot;&quot; eqcount=0 for char in src: char = ord(char) #turn to the ascii char = bin(char) #turn to the bin(二进制) char = char.replace(&quot;0b&quot;,&quot;&quot;) if(len(char) != 8): char = (8-len(char))*&quot;0&quot;+char tmp = tmp + char if(len(tmp)%6!= 0): eqcount = 6 - len(tmp)%6 tmp = tmp + (6-len(tmp)%6)*&quot;0&quot; for i in range(0,len(tmp),6): base64bin=tmp[i:i+6] base64bin=int(base64bin,2) result=result + array[base64bin] result = result + int(eqcount/2)*&quot;=&quot; return result 运行结果 解密:123456789101112131415161718192021222324#-------------------------------------&apos;&apos;&apos;使用base64方式解密&apos;&apos;&apos;def decrypt_base64(): src=input(&quot;解密字符串&quot;) eqcount = 0 for i in src: if(i == &quot;=&quot;): eqcount=eqcount+1 src = src.replace(&quot;=&quot;,&apos;&apos;) tmp = &quot;&quot; result = &quot;&quot; for char in src: for i in range(len(array)): if(array[i] == char): tmpbin=bin(i).replace(&quot;0b&quot;,&quot;&quot;) if(len(tmpbin) !=6): tmpbin = (6-len(tmpbin))*&quot;0&quot; + tmpbin tmp = tmp + tmpbin for i in range(0,len(tmp)-eqcount*2,8): result=result + chr(int(tmp[i:i+8],2)) return result 运行结果]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-09-03 机器人脚本编写]]></title>
    <url>%2F2018%2F09%2F03%2F2018-09-03%2F</url>
    <content type="text"><![CDATA[今天完成文章简介与脑筋急转弯的模块再加上了别人rebot的api的自动回复显得更加人性化预计其他功能的编写需要有存在服务器的情况下进行编写 随机文章 脑筋急转弯以及答案 人性化回复所以说:qqbot脚本的编写可能要告一段落了byebye ^.^]]></content>
      <categories>
        <category>开发日志</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-09-02 机器人脚本编写]]></title>
    <url>%2F2018%2F09%2F02%2F2018-09-02%2F</url>
    <content type="text"><![CDATA[qqbot的脚本逐渐完善今天加入天气查询与随机名人名言的模块明天预计加入推文的功能和脑筋急转弯的功能 天气查询 随机名人名言]]></content>
      <categories>
        <category>开发日志</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-08-31]]></title>
    <url>%2F2018%2F08%2F31%2F2018-08-31%2F</url>
    <content type="text"><![CDATA[安装一个本地包pacman -U /path/to/package/package_name-version.pkg.tar.xz 对于网络安全来讲,似乎构造数据包是一种必要的技术手段 相对于nmap来说,zenmap更适合上手,数据更加明白,但是每个选项神码回事就不知道了(主要是没用熟练,所以不明白) 攻击的方式有许多种类,看样子似乎无线网络更容易收到攻击(我也是猜的….802.11什么的似乎时只是认定mac地址,所以通过伪造数据包可以进行拒绝服务攻击0.0),主要还是利用现有的一些软件的漏洞来进行对系统的控制,费时费力 拜占庭式攻击对于装备精良的军队,当有一次处理不当,后面的一切都会发生意料不到的发展 arp协议必须在相同的网段中才起作用,而且arp协议是一定起作用的,由于主机要进行通讯必须得到与外界的链接,而外界是不知道主机的mac地址的,所以当外界广播目标的ip地址的时候,主机会发出反应,然后通知外界 每一种操作系统都会针对独特的探头(nmap发出)作出不同的反应,所以nmap可以确定操作系统,但也是只能确定大致方向 现在还不知道扫描出端口之后需要干什么,是不是针对端口进行渗透(迷茫脸) ps:昨天在系统上加了blackarch的源(内心不由的激动了起来今天似乎有想法在做一次13k的游戏 0.0(还没有确定下来)]]></content>
      <categories>
        <category>杂食动物</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-08-29]]></title>
    <url>%2F2018%2F08%2F29%2F2018-08-29%2F</url>
    <content type="text"><![CDATA[CIDR一个ISP准备把一些C类网络分配给各个用户群，目前已经分配了三个C类网段给用户，如果没有实施CIDR技术。ISP的路由器的路由表中会有三条下连网段的路由条目，并且会把它通告给Internet上的路由器。通过实施CIDR技术，我们可以在ISP的路由器上把这三个网段198.168.1.0,198.168.2.0,198.168.3.0汇聚成一条路由198.168.0.0/22.这样ISP路由器只向Internet通告198.168.0.0/22这一条路由,大大减少了路由表的数目。从而为网络路由器节省出了存储空间。值得注意的是,使用CIDR技术汇聚的网络地址的比特位必须是一致的，如上例所示。如果ISP连接了一个172.178.1.0网段，这些网段路由将无法汇聚,无法实现CIDR技术。/XX 就是前面有xx位的数字时不变的,形成一个区段 arp地址解析协议，即ARP（Address Resolution Protocol），是根据IP地址获取物理地址的一个TCP/IP协议。主机发送信息时将包含目标IP地址的ARP请求广播到网络上的所有主机，并接收返回消息，以此确定目标的物理地址；收到返回消息后将该IP地址和物理地址存入本机ARP缓存中并保留一定时间，下次请求时直接查询ARP缓存以节约资源。地址解析协议是建立在网络中各个主机互相信任的基础上的，网络上的主机可以自主发送ARP应答消息，其他主机收到应答报文时不会检测该报文的真实性就会将其记入本机ARP缓存；由此攻击者就可以向某一主机发送伪ARP应答报文，使其发送的信息无法到达预期的主机或到达错误的主机，这就构成了一个ARP欺骗。ARP命令可用于查询本机ARP缓存中IP地址和MAC地址的对应关系、添加或删除静态对应关系等。相关协议有RARP、代理ARP。NDP用于在IPv6中代替地址解析协议。(冒充) SYNSYN：同步序列编号（Synchronize Sequence Numbers）。是TCP/IP建立连接时使用的握手信号。在客户机和服务器之间建立正常的TCP网络连接时，客户机首先发出一个SYN消息，服务器使用SYN+ACK应答表示接收到了这个消息，最后客户机再以ACK消息响应。这样在客户机和服务器之间才能建立起可靠的TCP连接，数据才可以在客户机和服务器之间传递。TCP连接的第一个包，非常小的一种数据包。SYN 攻击包括大量此类的包，由于这些包看上去来自实际不存在的站点，因此无法有效进行处理。每个机器的欺骗包都要花几秒钟进行尝试方可放弃提供正常响应。 sctp四次握手]]></content>
      <categories>
        <category>杂食动物</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-08-28]]></title>
    <url>%2F2018%2F08%2F28%2F2018-08-28%2F</url>
    <content type="text"><![CDATA[header中的referer指的是来源的网页 php 中bin2hex 和hex2bin 16进制转换成ascii curl 可以用来下载东西]]></content>
      <categories>
        <category>杂食动物</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git学习]]></title>
    <url>%2F2018%2F08%2F26%2Fgit2%2F</url>
    <content type="text"><![CDATA[git 教程IntroductionGit is currently the most widely used version control system in the world, mostly thanks to GitHub.(感谢github) By that measure, I’d argue that it’s also the most misunderstood version control system in the world. This statement probably doesn’t ring true straight away because on the surface, Git is pretty simple. It’s really easy to pick up if you’ve come from another VCS like Subversion or Mercurial. It’s even relatively easy to pick up if you’ve never used a VCS before. Everybody understands adding, committing, pushing and pulling; but this is about as far as Git’s simplicity goes. Past this point, Git is shrouded by fear, uncertainty and doubt. Once you start talking about branching, merging, rebasing, multiple remotes, remote-tracking branches, detached HEAD states… Git becomes less of an easily-understood tool and more of a feared deity. Anybody who talks about no-fast-forward merges is regarded with quiet superstition, and even veteran hackers would rather stay away from rebasing “just to be safe”. I think a big part of this is due to many people coming to Git from a conceptually simpler VCS – probably Subversion – and trying to apply their past knowledge to Git. It’s easy to understand why people want to do this. Subversion is simple, right? It’s just files and folders. Commits are numbered sequentially. Even branching and tagging is simple – it’s just like taking a backup of a folder. Basically, Subversion fits in nicely with our existing computing paradigms. Everybody understands files and folders. Everybody knows that revision #10 was the one after #9 and before #11. But these paradigms break down when you try to apply them to Git. That’s why trying to understand Git in this way is wrong. Git doesn’t work like Subversion at all. Which can be pretty confusing. You can add and remove files. You can commit your changes. You can generate diffs and patches which look just like the ones that Subversion generates. So how can something which appears so similar really be so different? Complex systems like Git become much easier to understand once you figure out how they really work. The goal of this guide is to shed some light on how Git works under the hood. We’re going to take a look at some of Git’s core concepts including its basic object storage, how commits work, how branches and tags work, and we’ll look at the different kinds of merging in Git including the much-feared rebase. Hopefully at the end of it all, you’ll have a solid understanding of these concepts and will be able to use some of Git’s more advanced features with confidence. It’s worth noting at this point that this guide is not intended to be a beginner’s introduction to Git. This guide was written for people who already use Git, but would like to better understand it by taking a peek under the hood, and learn a few neat tricks along the way. With that said, let’s begin. RepositoriesAt the core of Git, like other VCS, is the repository. A Git repository is really just a simple key-value data store. This is where Git stores, among other things:Blobs, which are the most basic data type in Git. Essentially, a blob is just a bunch of bytes; usually a binary representation of a file.Tree objects, which are a bit like directories. Tree objects can contain pointers to blobs and other tree objects.Commit objects, which point to a single tree object, and contain some metadata including the commit author and any parent commits.Tag objects, which point to a single commit object, and contain some metadata.References, which are pointers to a single object (usually a commit or tag object).You don’t need to worry about all of this just yet; we’ll cover these things in more detail later. The important thing to remember about a Git repository is that it exists entirely in a single .git directory in your project root. There is no central repository like in Subversion or CVS. This is what allows Git to be a distributed version control system – everybody has their own self-contained version of a repository. You can initialize a Git repository anywhere with the git init command. Take a look inside the .git folder to get a glimpse of what a repository looks like.123456789101112$ git initInitialized empty Git repository in /home/demo/demo-repository/.git/$ ls -l .gittotal 32drwxrwxr-x 2 demo demo 4096 May 24 20:10 branches-rw-rw-r-- 1 demo demo 92 May 24 20:10 config-rw-rw-r-- 1 demo demo 73 May 24 20:10 description-rw-rw-r-- 1 demo demo 23 May 24 20:10 HEADdrwxrwxr-x 2 demo demo 4096 May 24 20:10 hooksdrwxrwxr-x 2 demo demo 4096 May 24 20:10 infodrwxrwxr-x 4 demo demo 4096 May 24 20:10 objectsdrwxrwxr-x 4 demo demo 4096 May 24 20:10 refs The important directories are .git/objects, where Git stores all of its objects; and .git/refs, where Git stores all of its references(标记). We’ll see how all of this fits together as we learn about the rest of Git. For now, let’s learn a little bit more about tree objects. Tree ObjectsA tree object in Git can be thought of as a directory. It contains a list of blobs (files) and other tree objects (sub-directories). Imagine we had a simple repository, with a README file and a src/ directory containing a hello.c file.123READMEsrc/ hello.c This would be represented by two tree objects: one for the root directory, and another for the src/ directory. Here’s what they would look like. tree 4da454..blob|976165.. |READMEtree| 81fc8b..| srctree 81fc8b..blob| 1febef..| hello.c Notice how given the root tree object, we can recurse through every tree object to figure out the state of the entire working tree. The root tree object, therefore, is essentially a snapshot of your repository at a given time. Usually when Git refers to “the tree”, it is referring to the root tree object. Now let’s learn how you can track the history of your repository with commit objects.(Tree 是文件夹 ,blob 是文件) CommitsA commit object is essentially(本质上) a pointer that contains a few pieces of important metadata. The commit itself has a hash, which is built from a combination of the metadata that it contains:(每一次提交会有一个hash,这个hash是由所包含的内容决定的,包含以下内容) The hash of the tree (the root tree object) at the time of the commit. As we learned in Tree Objects, this means that with a single commit, Git can build the entire working tree by recursing into the tree.(每一次简单的提交git都会递归的扫描整个tree去建立tree) The hash of any parent commits. This is what gives a repository its history: every commit has a parent commit, all the way back to the very first commit. The author’s name and email address, and the time that the changes were authored. The committer’s name and email address, and the time that the commit was made. The commit message.Let’s see a commit object in action by creating a simple repository.12345678$ git initInitialized empty Git repository in /home/demo/simple-repository/.git/$ echo &apos;This is the readme.&apos; &gt; README$ git add README$ git commit -m &quot;First commit&quot;[master (root-commit) d409ca7] First commit1 file changed, 1 insertion(+)create mode 100644 README When you create a commit, Git will give you the hash of that commit. Using git show with the --format=raw flag, we can see this newly-created commit’s metadata. 12345678910111213141516$ git show --format=raw d409ca7commit d409ca76bc919d9ca797f39ae724b7c65700fd27tree 9d073fcdfaf07a39631ef94bcb3b8268bc2106b1author Joseph Wynn &lt;joseph@wildlyinaccurate.com&gt; 1400976134 -0400committer Joseph Wynn &lt;joseph@wildlyinaccurate.com&gt; 1400976134 -0400 First commitdiff --git a/README b/READMEnew file mode 100644index 0000000..9761654--- /dev/null+++ b/README@@ -0,0 +1 @@+This is the readme. Notice how although we referenced the commit by the partial hash d409ca7, Git was able to figure out that we actually meant d409ca76bc919d9ca797f39ae724b7c65700fd27. This is because the hashes that Git assigns to objects are unique enough to be identified by the first few characters. You can see here that Git is able to find this commit with as few as four characters; after which point Git will tell you that the reference is ambiguous.(git 可以通过commit的hash的部分字母就可以将commit给你列出来,如果不足,他会提醒你这是模糊的)1234$ git show d409c$ git show d409$ git show d40fatal: ambiguous argument &apos;d40&apos;: unknown revision or path not in the working tree. ReferencesIn previous sections, we saw how objects in Git are identified by a hash. Since we want to manipulate objects quite often in Git, it’s important to know their hashes. You could run all your Git commands referencing each object’s hash, like git show d409ca7, but that would require you to remember the hash of every object you want to manipulate. To save you from having to memorize these hashes, Git has references, or “refs”. A reference is simply a file stored somewhere in .git/refs, containing the hash of a commit object. To carry on the example from Commits, let’s figure out the hash of “First commit” using references only.123$ git statusOn branch masternothing to commit, working directory clean git statushas told us that we are on branch master. As we will learn in a later section, branches are just references. We can see this by looking in .git/refs/heads.123$ ls -l .git/refs/heads/total 4-rw-rw-r-- 1 demo demo 41 May 24 20:02 master We can easily see which commit master points to by reading the file.12$ cat .git/refs/heads/masterd409ca76bc919d9ca797f39ae724b7c65700fd27 Sure enough, master contains the hash of the “First commit” object. Of course, it’s possible to simplify this process. Git can tell us which commit a reference is pointing to with the show and rev-parse commands.1234$ git show --oneline masterd409ca7 First commit$ git rev-parse masterd409ca76bc919d9ca797f39ae724b7c65700fd27 Git also has a special reference, HEAD. This is a “symbolic” reference which points to the tip of the current branch rather than an actual commit. If we inspect HEAD, we see that it simply points to refs/head/master.12$ cat .git/HEADref: refs/heads/master It is actually possible for HEAD to point directly to a commit object. When this happens, Git will tell you that you are in a “detached HEAD state”. We’ll talk a bit more about this later, but really all this means is that you’re not currently on a branch.(你不是在branch上) BranchesGit’s branches are often touted as being one of its strongest features. This is because branches in Git are very lightweight, compared to other VCS where a branch is usually a clone of the entire repository. The reason branches are so lightweight in Git is because they’re just references. We saw in References that the master branch was simply a file inside .git/refs/heads. Let’s create another branch to see what happens under the hood.123$ git branch test-branch$ cat .git/refs/heads/test-branchd409ca76bc919d9ca797f39ae724b7c65700fd27 It’s as simple as that. Git has created a new entry in .git/refs/heads and pointed it at the current commit. We also saw in References that HEAD is Git’s reference to the current branch. Let’s see that in action by switching to our newly-created branch.123456$ cat .git/HEAD(0当前branch)ref: refs/heads/master$ git checkout test-branchSwitched to branch &apos;test-branch&apos;$ cat .git/HEADref: refs/heads/test-branch When you create a new commit, Git simply changes the current branch to point to the newly-created commit object.1234567$ echo &apos;Some more information here.&apos; &gt;&gt; README$ git add README$ git commit -m &quot;Update README in a new branch&quot;[test-branch 7604067] Update README in a new branch 1 file changed, 1 insertion(+)$ cat .git/refs/heads/test-branch76040677d717fd090e327681064ac6af9f0083fb Later on we’ll look at the difference between local branches and remote-tracking branches. Before we look at how tags work, let’s switch back to the master branch.12$ git checkout masterSwitched to branch &apos;master&apos; TagsThere are two types of tags in Git – lightweight tags and annotated tags. On the surface, these two types of tags look very similar. Both of them are references stored in .git/refs/tags. However, that’s about as far as the similarities go. Let’s create a lightweight tag to see how they work.123$ git tag 1.0-lightweight$ cat .git/refs/tags/1.0-lightweightd409ca76bc919d9ca797f39ae724b7c65700fd27 We can see that Git has created a tag reference which points to the current commit(指向当前的提交). By default, git tag will create a lightweight tag. Note that this is not a tag object. We can verify this by using git cat-file to inspect(检查) the tag.123456789101112$ git cat-file -p 1.0-lightweighttree 9d073fcdfaf07a39631ef94bcb3b8268bc2106b1author Joseph Wynn &lt;joseph@wildlyinaccurate.com&gt; 1400976134 -0400committer Joseph Wynn &lt;joseph@wildlyinaccurate.com&gt; 1400976134 -0400First commit$ git cat-file -p d409ca7tree 9d073fcdfaf07a39631ef94bcb3b8268bc2106b1author Joseph Wynn &lt;joseph@wildlyinaccurate.com&gt; 1400976134 -0400committer Joseph Wynn &lt;joseph@wildlyinaccurate.com&gt; 1400976134 -0400First commit You can see that as far as Git is concerned, the 1.0-lightweight tag and the d409ca7 commit are the same object. That’s because the lightweight tag is only a reference to the commit object. Let’s compare this to an annotated tag.123$ git tag -a -m &quot;Tagged 1.0&quot; 1.0$ cat .git/refs/tags/1.010589beae63c6e111e99a0cd631c28479e2d11bf We’ve passed the -a (--annotate) flag to git tag to create an annotated tag. Notice how Git creates a reference for the tag just like the lightweight tag, but this reference is not pointing to the same object as the lightweight tag. Let’s use git cat-file again to inspect the object.1234567$ git cat-file -p 1.0object d409ca76bc919d9ca797f39ae724b7c65700fd27type committag 1.0tagger Joseph Wynn &lt;joseph@wildlyinaccurate.com&gt; 1401029229 -0400Tagged 1.0 This is a tag object, separate to the commit that it points to. As well as containing a pointer to a commit, tag objects also store a tag message and information about the tagger. Tag objects can also be signed with a GPG key to prevent commit or email spoofing. Aside from being GPG-signable, there are a few reasons why annotated tags are preferred over lightweight tags. Probably the most important reason is that annotated tags have their own author information. This can be helpful when you want to know who created the tag, rather than who created the commit that the tag is referring to. Annotated tags are also timestamped. Since new versions are usually tagged right before they are released, an annotated tag can tell you when a version was released rather than just when the final commit was made.(annotated tags 比light-weight tags更优先是由于他信息更详细,使项目更容易控制) MergingMerging in Git is the process of joining two histories (usually branches) together. Let’s start with a simple example. Say you’ve created a new feature branch from master, and done some work on it.123456$ git checkout -b feature-branchSwitched to a new branch &apos;feature-branch&apos;$ vim feature.html$ git commit -am &quot;Finished the new feature&quot;[feature-branch 0c21359] Finished the new feature 1 file changed, 1 insertion(+) At the same time, you need to fix an urgent bug. So you create a hotfix branch from master, and do some work in there.12345678$ git checkout masterSwitched to branch &apos;master&apos;$ git checkout -b hotfixSwitched to a new branch &apos;hotfix&apos;$ vim index.html$ git commit -am &quot;Fixed some wording&quot;[hotfix 40837f1] Fixed some wording 1 file changed, 1 insertion(+), 1 deletion(-) Now you want to bring the bug fix into master so that you can tag it and release it.1234567$ git checkout masterSwitched to branch &apos;master&apos;$ git merge hotfixUpdating d939a3a..40837f1Fast-forward index.html | 2 +- 1 file changed, 1 insertion(+), 1 deletion(-) Notice how Git mentions fast-forward during the merge. What this means is that all of the commits in hotfix were directly upstream from master. This allows Git to simply move the master pointer up the tree to hotfix.Now let’s try and merge feature-branch into master.1234$ git merge feature-branchMerge made by the &apos;recursive&apos; strategy. feature.html | 1 + 1 file changed, 1 insertion(+) This time, Git wasn’t able to perform a fast-forward. This is because feature-branch isn’t directly upstream from master. This is clear on the graph above, where master is at commit D which is in a different history tree to feature-branch at commit C. So how did Git handle this merge? Taking a look at the log, we see that Git has actually created a new ”merge” commit, as well as bringing the commit from feature-branch.12345$ git log --oneline8ad0923 Merge branch &apos;feature-branch&apos;0c21359 Finished the new feature40837f1 Fixed some wordingd939a3a Initial commit Upon closer inspection, we can see that this is a special kind of commit object – it has two parent commits. This is referred to as a merge commit.12345678910$ git show --format=raw 8ad0923commit 8ad09238b0dff99e8a99c84d68161ebeebbfc714tree e5ee97c8f9a4173f07aa4c46cb7f26b7a9ff7a17parent 40837f14b8122ac6b37c0919743b1fd429b3bbabparent 0c21359730915c7888c6144aa8e9063345330f1fauthor Joseph Wynn &lt;joseph@wildlyinaccurate.com&gt; 1401134489 +0100committer Joseph Wynn &lt;joseph@wildlyinaccurate.com&gt; 1401134489 +0100 Merge branch &apos;feature-branch&apos; This means that our history graph now looks something like this (commit E is the new merge commit). Some people believe that this sort of history graph is undesirable. In the Rebasing (Continued) section, we’ll learn how to prevent non-fast-forward merges by rebasing feature branches before merging them with master. RebasingRebasing is without a doubt one of Git’s most misunderstood features. For most people, git rebase is a command that should be avoided at all costs. This is probably due to the extraordinary amount of scaremongering around rebasing. “Rebase Considered Harmful”, and “Please, stay away from rebase” are just two of the many anti-rebase articles you will find in the vast archives of the Internet. But rebase isn’t scary, or dangerous, so long as you understand what it does. But before we get into rebasing, I’m going to take a quick digression, because it’s actually much easier to explain rebasing in the context of cherry-picking.(呦) Cherry-PickingWhat git cherry-pick does is take one or more commits, and replay them on top of the current commit. Imagine a repository with the following history graph. If you are on commit D and you run git cherry-pick F, Git will take the changes that were introduced in commit F and replay them as a new commit (shown as F’) on top of commit D. The reason you end up with a copy of commit F rather than commit F itself is due to the way commits are constructed. Recall that the parent commit is part of a commit’s hash. So despite containing the exact same changes, author information and timestamp; F’ will have a different parent to F, giving it a different hash. A common workflow in Git is to develop features on small branches, and merge the features one at a time into the master branch. Let’s recreate this scenario(方案) by adding some branch labels to the graphs. As you can see, master has been updated since foo was created. To avoid potential conflicts when foo is merged with master, we want to bring master’s changes into foo. Because master is the base branch, we want to play foo’s commits on top of master. Essentially, we want to change commit C’s parent from B to F.It’s not going to be easy, but we can achieve this with git cherry-pick. First, we need to create a temporary branch at commit F.12$ git checkout master$ git checkout -b foo-tmp Now that we have a base on commit F, we can cherry-pick all of foo’s commits on top of it.1$ git cherry-pick C D Now all that’s left to do is point foo at commit D’, and delete the temporary branch foo-tmp. We do this with the reset command, which points HEAD (and therefore the current branch) at a specified commit. The --hard flag ensures our working tree is updated as well.123$ git checkout foo$ git reset --hard foo-tmp$ git branch -D foo-tmp This gives the desired result of foo’s commits being upstream of master. Note that the original C and D commits are no longer reachable because no branch points to them. Rebasing (Continued)While the example in Cherry-Picking worked, it’s not practical. In Git, rebasing allows us to replace our verbose cherry-pick workflow…123456$ git checkout master$ git checkout -b foo-tmp$ git cherry-pick C D$ git checkout foo$ git reset --hard foo-tmp$ git branch -D foo-tmp …With a single command.git rebase master fooWith the format git rebase , the rebase command will take all of the commits from and play them on top of one by one. It does this without actually modifying , so the end result is a linear history in which can be fast-forwarded to . In a sense, performing a rebase is like telling Git, “Hey, I want to pretend that was actually branched from . Take all of the commits from , and pretend that they happened after “. Let’s take a look again at the example graph from Merging to see how rebasing can prevent us from having to do a non-fast-forward merge.All we have to do to enable a fast-forward merge of feature-branch into master is run git rebase master feature-branch before performing the merge.123$ git rebase master feature-branchFirst, rewinding head to replay your work on top of it...Applying: Finished the new feature This has brought feature-branch directly upstream of master.Git is now able to perform a fast-forward merge.123456$ git checkout master$ git merge feature-branchUpdating 40837f1..2a534ddFast-forward feature.html | 1 + 1 file changed, 1 insertion(+) RemotesIn order to collaborate(合作) on any Git project, you need to utilise(利用) at least one remote(远程) repository. Unlike centralised(集中) VCS which require a dedicated server daemon, a Git remote is simply another Git repository. In order to demonstrate this, we first need to understand the concept(观点) of a bare repository. Recall that Git stores the entire repository inside the .git directory. Inside this directory are blobs and tree objects which can be traversed to build a snapshot of the entire project. This means that Git doesn’t actually need a working tree – it only uses the working tree to figure out what changes have been made since the last commit. This is easily demonstrated if you delete a file from a repository, and then run git checkout &lt;file&gt;. Despite being removed from the file system, Git can still restore the file because it has previously stored it in the repository. You can do the same thing with entire directories and Git will still be able to restore everything by traversing its tree objects. It is therefore possible to have a repository which can store your project’s history without actually having a working tree. This is called a bare repository. Bare repositories are most commonly used as a “central” repository where collaborators can share changes. The mechanism for sharing these changes will be explained in detail in the Pushing and Pulling sections. For now, let’s look at creating a bare repository. 123456789101112$ git init --bareInitialised empty Git repository in /home/demo/bare-repo/$ ls -ltotal 12drwxrwxr-x 1 demo demo 0 May 31 12:58 branches-rw-rw-r-- 1 demo demo 66 May 31 12:58 config-rw-rw-r-- 1 demo demo 73 May 31 12:58 description-rw-rw-r-- 1 demo demo 23 May 31 12:58 HEADdrwxrwxr-x 1 demo demo 328 May 31 12:58 hooksdrwxrwxr-x 1 demo demo 14 May 31 12:58 infodrwxrwxr-x 1 demo demo 16 May 31 12:58 objectsdrwxrwxr-x 1 demo demo 18 May 31 12:58 refs Notice how rather than creating a .git directory for the repository, git init –bare simply treats the current directory as the .git directory. There’s really not much to this repository. The only interesting things it contains are a HEAD reference which points to the master branch (which doesn’t exist yet), and a config file which has the bare flag set to true. The other files aren’t of much interest to us.12345678910111213141516171819202122$ find . -type f./info/exclude./hooks/commit-msg.sample./hooks/pre-commit.sample./hooks/pre-push.sample./hooks/pre-rebase.sample./hooks/pre-applypatch.sample./hooks/applypatch-msg.sample./hooks/post-update.sample./hooks/prepare-commit-msg.sample./hooks/update.sample./description./HEAD./config$ cat HEADref: refs/heads/master$ cat config[core] repositoryformatversion = 0 filemode = true bare = true So what can we do with this repository? Well, nothing much right now. Git won’t let us modify the repository because it doesn’t have a working tree to modify. (Note: this isn’t strictly true. We could painstakingly use Git’s low-level commands to manually create and store objects in Git’s data store, but that is beyond the scope of this guide. If you’re really interested, read Git Internals - Git Objects).123$ touch README$ git add READMEfatal: This operation must be run in a work tree The intended use of this repository is for other collaborators to clone and pull changes from, as well as push their own changes to. CloningNow that we’ve set up a bare repository, let’s look at the concept of cloning a repository. The git clone command is really just a shortcut which does a few things for you. With its default configuration, it will: Create remote-tracking branches for each branch in the remote. Check out the branch which is currently active (HEAD) on the remote. Perform a git pull to bring the current branch and working tree up-to-date with the remote. The clone command takes a URL and supports a number of transport protocols including HTTP, SSH, and Git’s own protocol. It also supports plain old file paths, which is what we’ll use.12345$ cd ..$ git clone bare-repo/ clone-of-bare-repoCloning into &apos;clone-of-bare-repo&apos;...warning: You appear to have cloned an empty repository.done. Let’s inspect this cloned repository to see how Git has set it up.1234567891011121314151617181920212223242526272829303132$ cd clone-of-bare-repo/$ find . -type f./.git/info/exclude./.git/hooks/commit-msg.sample./.git/hooks/pre-commit.sample./.git/hooks/pre-push.sample./.git/hooks/pre-rebase.sample./.git/hooks/pre-applypatch.sample./.git/hooks/applypatch-msg.sample./.git/hooks/post-update.sample./.git/hooks/prepare-commit-msg.sample./.git/hooks/update.sample./.git/description./.git/HEAD./.git/config$ cat .git/HEADref: refs/heads/master$ ls -l .git/refs/heads/total 0$ cat .git/config[core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true[remote &quot;origin&quot;] url = /home/demo/bare-repo/ fetch = +refs/heads/*:refs/remotes/origin/*[branch &quot;master&quot;] remote = origin merge = refs/heads/master This is quite literally a clone of bare-repo. The only difference is that this repository contains a few extra lines in .git/config. First, it contains a remote listing for “origin”, which is the default name given to a repository’s main remote. This tells Git the URL of the repository, and which references it should retrieve when performing a git fetch. Below that is a branch listing. This is the configuration for a remote-tracking branch. But before we get into that, let’s store some data in the remote repository. PushingWe’ve just cloned a completely empty repository, and we want to start working on it.123456$ echo &apos;Project v1.0&apos; &gt; README$ git add README$ git commit -m &quot;Add readme&quot;[master (root-commit) 5d591d5] Add readme 1 file changed, 1 insertion(+) create mode 100644 README Notice that even though it didn’t technically exist (there was nothing in .git/refs/heads), this commit has been made to the master branch. That’s because the HEAD of this repository pointed to master, so Git has gone ahead and created the branch for us.12$ cat .git/refs/heads/master5d591d5fafd538610291f45bec470d1b4e77891e Now that we’ve completed some work, we need to share this with our collaborators who have also cloned this repository. Git makes this really easy.123456$ git push origin masterCounting objects: 3, done.Writing objects: 100% (3/3), 231 bytes | 0 bytes/s, done.Total 3 (delta 0), reused 0 (delta 0)To /home/demo/bare-repo/ * [new branch] master -&gt; master Notice how we specified both the remote (origin) and the branch (master) that we want Git to push. It is possible to simply run git push, but this can be dangerous and is generally advised against. Running git push without any arguments can (depending on your configuration) push all remote-tracking branches. This is usually okay, but it can result in you pushing changes which you don’t want collaborators to pull. In the worst case, you can destroy other collaborators’ changes if you specify the –force flag. So, let’s take a look at the remote repository to see what happened.123456789101112131415161718$ cd ../bare-repo/$ cat refs/heads/master5d591d5fafd538610291f45bec470d1b4e77891e$ git show 5d591d5commit 5d591d5fafd538610291f45bec470d1b4e77891eAuthor: Joseph Wynn &lt;joseph@wildlyinaccurate.com&gt;Date: Sat May 31 14:08:34 2014 +0100 Add readmediff --git a/README b/READMEnew file mode 100644index 0000000..5cecdfb--- /dev/null+++ b/README@@ -0,0 +1 @@+Project v1.0 As we expected, the remote repository now contains a master branch which points to the commit that we just created. Essentially what happened when we ran git push, is Git updated the remote’s references, and sent it any objects required to build those references. In this case, git push updated the remote’s master to point at 5d591d5, and sent it the 5d591d5 commit object as well as any tree and blob objects related to that commit. Remote-Tracking BranchesAs we saw in Cloning, a remote-tracking branch is essentially just a few lines in .git/config. Let’s take a look at those lines again.123[branch &quot;master&quot;] remote = origin merge = refs/heads/master The line [branch &quot;master&quot;] denotes that the following configuration applies to the local master branch. The rest of the configuration specifies that when this remote-tracking branch is fetched, Git should fetch the master branch from the origin remote. Besides storing this configuration, Git also stores a local copy of the remote branch. This is simply stored as a reference in .git/refs/remotes//. We’ll see more about how this works in Fetching. FetchingThe git fetch command is fairly simple. It takes the name of a remote (unless used with the –all flag, which fetches all remotes), and retrieves any new references and all objects necessary to complete them. Recall what a remote’s configuration looks like.123[remote &quot;origin&quot;] url = /home/demo/bare-repo/ fetch = +refs/heads/*:refs/remotes/origin/* The fetch parameter here specifies a mapping of :. The example above simply states that the references found in origin’s refs/heads/ should be stored locally in refs/remotes/origin/ . We can see this in the repository that we cloned earlier. 123$ ls -l .git/refs/remotes/origin/total 4-rw-rw-r-- 1 demo demo 41 May 31 14:12 master Let’s see a fetch in action to get a better idea of what happens. First, we’ll create a new branch on the remote repository.12$ cd ../bare-repo/$ git branch feature-branch Now we’ll run git fetch from the clone.1234$ cd ../clone-of-bare-repo/$ git fetch originFrom /home/demo/bare-repo * [new branch] feature-branch -&gt; origin/feature-branch This has done a couple of things. First, it has created a reference for the remote branch in .git/refs/remotes/origin.12$ cat .git/refs/remotes/origin/feature-branch5d591d5fafd538610291f45bec470d1b4e77891e It has also updated a special file, .git/FETCH_HEAD with some important information. We’ll talk about this file in more detail soon.123$ cat .git/FETCH_HEAD5d591d5fafd538610291f45bec470d1b4e77891e branch &apos;master&apos; of /home/demo/bare-repo5d591d5fafd538610291f45bec470d1b4e77891e not-for-merge branch &apos;feature-branch&apos; of /home/demo/bare-repo What is hasn’t done is created a local branch. This is because Git understands that even though the remote has a feature-branch, you might not want it in your local repository. But what if we do want a local branch which tracks the remote feature-branch? Git makes this easy. If we run git checkout feature-branch, rather than failing because no local feature-branch exists, Git will see that there is a remote feature-branch available and create a local branch for us.1234$ git checkout feature-branchBranch feature-branch set up to track remote branch feature-branch from origin.Switched to a new branch &apos;feature-branch&apos; Git has done a couple of things for us here. First, it has created a local feature-branch reference which points to the same commit as the remote feature-branch.1234$ cat .git/refs/remotes/origin/feature-branch5d591d5fafd538610291f45bec470d1b4e77891e$ cat .git/refs/heads/feature-branch5d591d5fafd538610291f45bec470d1b4e77891e It has also created a remote-tracking branch entry in .git/config.123456789101112131415$ cat .git/config[core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true[remote &quot;origin&quot;] url = /home/demo/bare-repo/ fetch = +refs/heads/*:refs/remotes/origin/*[branch &quot;master&quot;] remote = origin merge = refs/heads/master[branch &quot;feature-branch&quot;] remote = origin merge = refs/heads/feature-branch PullingThe git pull command is, like git clone, a nice shortcut which essentially just runs a few lower-level commands. In short, with the format git pull , the git pull command does the following: Runs git fetch . Reads .git/FETCH_HEAD to figure out if has a remote-tracking branch which should be merged. Runs git merge if required, otherwise quits with an appropriate message.At this point, it helps to understand Git’s FETCH_HEAD. Every time you run git fetch, Git stores information about the fetched branches in .git/FETCH_HEAD. This is referred to as a short-lived reference, because by default Git will override the contents of FETCH_HEAD every time you run git fetch. Let’s introduce some new commits to our remote repository so that we can see this in practice.12345678910111213141516171819$ git clone bare-repo/ new-clone-of-bare-repoCloning into &apos;new-clone-of-bare-repo&apos;...done.$ cd new-clone-of-bare-repo/$ git checkout feature-branchBranch feature-branch set up to track remote branch feature-branch from origin.Switched to a new branch &apos;feature-branch&apos;$ echo &apos;Some more information.&apos; &gt;&gt; README$ git commit -am &quot;Add more information to readme&quot;[feature-branch 7cd83c2] Add more information to readme 1 file changed, 1 insertion(+)$ git push origin feature-branchCounting objects: 5, done.Writing objects: 100% (3/3), 298 bytes | 0 bytes/s, done.Total 3 (delta 0), reused 0 (delta 0)To /home/demo/bare-repo/ 5d591d5..7cd83c2 feature-branch -&gt; feature-branch At this point, Git has updated our local copy of the remote branch, and updated the information in FETCH_HEAD.12345$ git merge FETCH_HEADUpdating 5d591d5..7cd83c2Fast-forward README | 1 + 1 file changed, 1 insertion(+) And that’s it – we’ve just performed a git pull without actually running git pull. Of course, it is much easier to let Git take care of these details. Just to be sure that the outcome is the same, we can run git pull as well.123456789$ git reset --hard HEAD^1HEAD is now at 5d591d5 Add readme$ git pull origin feature-branchFrom /home/demo/bare-repo * branch feature-branch -&gt; FETCH_HEADUpdating 5d591d5..7cd83c2Fast-forward README | 1 + 1 file changed, 1 insertion(+) ToolkitWith a solid understanding of Git’s inner workings, some of the more advanced Git tools start to make more sense. git-reflogWhenever you make a change in Git that affects the tip of a branch, Git records information about that change in what’s called the reflog. Usually you shouldn’t need to look at these logs, but sometimes they can come in very handy. Let’s say you have a repository with a few commits.12345$ git log --onelined6f2a84 Add empty LICENSE file51c4b49 Add some actual content to readme3413f46 Add TODO note to readme322c826 Add empty readme You decide, for some reason, to perform a destructive action on your master branch.1$ git reset --hard 3413f46 HEAD is now at 3413f46 Add TODO note to readmeSince performing this action, you’ve realised that you lost some commits and you have no idea what their hashes were. You never pushed the changes; they were only in your local repository. git log is no help, since the commits are no longer reachable from HEAD.123$ git log --oneline3413f46 Add TODO note to readme322c826 Add empty readme This is where git reflog can be useful.123456$ git reflog3413f46 HEAD@&#123;0&#125;: reset: moving to 3413f46d6f2a84 HEAD@&#123;1&#125;: commit: Add empty LICENSE file51c4b49 HEAD@&#123;2&#125;: commit: Add some actual content to readme3413f46 HEAD@&#123;3&#125;: commit: Add TODO note to readme322c826 HEAD@&#123;4&#125;: commit (initial): Add empty readme The reflog shows a list of all changes to HEAD in reverse chronological order. The hash in the first column is the value of HEAD after the action on the right was performed. We can see, therefore, that we were at commit d6f2a84 before the destructive change. How you want to recover commits depends on the situation. In this particular example, we can simply do a git reset --hard d6f2a84 to restore HEAD to its original position. However if we have introduced new commits since the destructive change, we may need to do something like cherry-pick all the commits that were lost. Note that Git’s reflog is only a record of changes for your local repository. If your local repository becomes corrupt or is deleted, the reflog won’t be of any use (if the repository is deleted the reflog won’t exist at all!) Depending on the situation, you may find git fsck more suitable for recovering lost commits. git-fsckIn a way, Git’s object storage works like a primitive file system – objects are like files on a hard drive, and their hashes are the objects’ physical address on the disk. The Git index is exactly like the index of a file system, in that it contains references which point at an object’s physical location. By this analogy, git fsck is aptly named after fsck (“file system check”). This tool is able to check Git’s database and verify the validity and reachability of every object that it finds. When a reference (like a branch) is deleted from Git’s index, the object(s) they refer to usually aren’t deleted, even if they are no longer reachable by any other references. Using a simple example, we can see this in practice. Here we have a branch, feature-branch, which points at f71bb43. If we delete feature-branch, the commit will no longer be reachable.1234567$ git branch feature-branch* master$ git rev-parse --short feature-branchf71bb43$ git branch -D feature-branchDeleted branch feature-branch (was f71bb43). At this point, commit f71bb43 still exists in our repository, but there are no references pointing to it. By searching through the database, git fsck is able to find it.123$ git fsck --lost-foundChecking object directories: 100% (256/256), done.dangling commit f71bb43907bffe0bce2967504341a0ece7a8cb68 For simple cases, git reflog may be preferred. Where git fsck excels over git reflog, though, is when you need to find objects which you never referenced in your local repository (and therefore would not be in your reflog). An example of this is when you delete a remote branch through an interface like GitHub. Assuming the objects haven’t been garbage-collected, you can clone the remote repository and use git fsck to recover the deleted branch. git-stashgit stash takes all changes to your working tree and index, and “stashes” them away, giving you a clean working tree. You can then retrieve those changes from your stash and re-apply them to the working tree at any time with git stash apply. A common use for the stash command is to save some half-finished changes in order to checkout another branch. This seems fairly simple at first, but the mechanism behind the stash command is actually quite complex. Let’s build a simple repository to see how it works.12345678$ git initInitialised empty Git repository in /home/demo/demo-repo/.git/$ echo &apos;Foo&apos; &gt; test.txt$ git add test.txt$ git commit -m &quot;Initial commit&quot;[master (root-commit) 2522332] Initial commit 1 file changed, 1 insertion(+) create mode 100644 test.txt Now let’s make some changes, and stash them.1234$ echo &apos;Bar&apos; &gt;&gt; test.txt$ git stashSaved working directory and index state WIP on master: 2522332 Initial commitHEAD is now at 2522332 Initial commit Stashes in Git are put onto a stack, with the most recently-stashed on top. You can list all current stashes with git stash list.12$ git stash liststash@&#123;0&#125;: WIP on master: 2522332 Initial commit Right now we only have one stash: stash@{0}. This is actually a reference, which we can inspect.123456789101112131415$ git show stash@&#123;0&#125;commit f949b46a417a4f1595a9d12773c89cce4454a958Merge: 2522332 1fbe1ccAuthor: Joseph Wynn &lt;joseph@wildlyinaccurate.com&gt;Date: Sat Jul 5 00:15:51 2014 +0100 WIP on master: 2522332 Initial commitdiff --cc test.txtindex bc56c4d,bc56c4d..3b71d5b--- a/test.txt+++ b/test.txt@@@ -1,1 -1,1 +1,2 @@@ Foo++Bar From this we can see that the stash is pointing to a commit object. What’s interesting is that the stash commit is a merge commit. We’ll look into that in a bit, but first: where is this commit?12345678$ git log --oneline2522332 Initial commit$ git branch* master$ git fsck --lost-foundChecking object directories: 100% (256/256), done. It’s not in the current branch, and there are no other branches it could be in. git-fsck hasn’t found any dangling commits, so it must be referenced somewhere. But where? The answer is simple: Git creates a special reference for the stash which isn’t seen by commands like git branch and git tag. This reference lives in .git/refs/stash. We can verify this with git show-ref.123$ git show-ref25223321ec2fbcb718b7fbf99485f1cb4d2f2042 refs/heads/masterf949b46a417a4f1595a9d12773c89cce4454a958 refs/stash So why does Git create a merge commit for a stash? The answer is relatively simple: as well as recording the state of the working tree, git stash also records the state of the index (also known as the “staging area”). Since it’s possible for the index and the working tree to contain changes to the same file, Git needs to store the states separately. git-describeGit’s describe command is summed up pretty neatly in the documentation: git-describe - Show the most recent tag that is reachable from a commit This can be helpful for things like build and release scripts, as well as figuring out which version a change was introduced in. git describe will take any reference or commit hash, and return the name of the most recent tag. If the tag points at the commit you gave it, git describe will return only the tag name. Otherwise, it will suffix the tag name with some information including the number of commits since the tag and an abbreviation of the commit hash.1234$ git describe v1.2.15v1.2.15$ git describe 2db66fv1.2.15-80-g2db66f5 If you want to ensure that only the tag name is returned, you can force Git to remove the suffix by passing –abbrev=0.12$ git describe --abbrev=0 2db66fv1.2.15 git-rev-parsegit rev-parse is an ancillary plumbing command which takes a wide range of inputs and returns one or more commit hashes. The most common use case is figuring out which commit a tag or branch points to.1234$ git rev-parse v1.2.152a46f5e2fbe83ccb47a1cd42b81f815f2f36ee9d$ git rev-parse --short v1.2.152a46f5e git-bisectgit bisect is an indispensable tool when you need to figure out which commit introduced a breaking change. The bisect command does a binary search through your commit history to help you find the breaking change as quickly as possible. To get started, simply run git bisect start, and tell Git that the commit you’re currently on is broken with git bisect bad. Then, you can give Git a commit that you know is working with git bisect good .12345$ git bisect start$ git bisect bad$ git bisect good v1.2.15Bisecting: 41 revisions left to test after this (roughly 5 steps)[b87713687ecaa7a873eeb3b83952ebf95afdd853] docs(misc/index): add header; general links Git will then checkout a commit and ask you to test whether it’s broken or not. If the commit is broken, run git bisect bad. If the commit is fine, run git bisect good. After doing this a few times, Git will be able to pinpoint the commit which first introduced the breaking change.12$ git bisect bade145a8df72f309d5fb80eaa6469a6148b532c821 is the first bad commit Once the bisect is finished (or when you want to abort it), be sure to run git bisect reset to reset HEAD to where it was before the bisect. Useful commandsFind which commit a reference points at12345$ git rev-parse HEAD0f64e9e759c904553309858070f444e5e64847c4$ git rev-parse --short HEAD0f64e9e Find which branches a commit is in123$ git branch --contains HEAD master* other-branch Find commits that are in one branch but not another1234$ git log --oneline --right-only master...hotfix-10f64e9e Apply hotfix patch from #2914 to hotfix-1bc3bff5 [Cherry-pick] Fix issue #2926 Exclude commits that were cherry-picked123$ git log --oneline --cherry-pick --right-only master...hotfix-10f64e9e Apply hotfix patch from #2914 to hotfix-1 View details of an object1234567$ git cat-file -p HEADtree af22d0482b89640c95986f3b4663026bcb7f764bparent bc3bff555f573ac76f0d3e71f0e54d63f50b8434author Foo Bar &lt;foo@bar.com&gt; 1436294582 +0100committer Foo Bar &lt;foo@bar.com&gt; 1436294582 +0100Lorem ipsum dolor sit amet, consectetur adipiscing elit Show an object’s type12$ git cat-file -t HEADcommit Show an object’s size12$ git cat-file -s HEAD253 Print the tree of a given reference123456789101112131415161718192021$ git ls-tree -t -r HEAD100644 blob 59e004af21a725c9b378001a1b231967f955b992 .gitignore100644 blob 9f5d366d261317d8ff881ee2945ef2c7960fa2ea .travis.yml100644 blob 148fc67781eba8c08bbb4db1fc9e92b9781ec28d LICENSE100644 blob 4a2727bb9afae5782510e7ce764608540dd7c04a Makefile100644 blob b07b8d5f39d6f62a18a1b0791f33a784ec34556e README.md100644 blob 9a994af677b0dfd41b4e3b76b3e7e604003d64e1 Setup.hs100644 blob 66820894ca2c16725e8527e16785f2eebd89871a lishp.cabal040000 tree eda8357ddb73fa384f283291bac84d7fe1bce436 src040000 tree 6a84cb52f17c7e1ff691fe45f4c70df5269bdab0 src/Lishp100644 blob 1f7677ed0c7f3e713c7e6a16d94cc3db89d911cd src/Lishp/Interpreter.hs100644 blob 1a0fe9394bc2d7789d783a467626301204de700a src/Lishp/Primitives.hs100644 blob caf0fcf1cd265436efbaa6cc39338e653197a3ee src/Lishp/Readline.hs100644 blob 92cf693d29169066356bed3f1ae794a624db49a5 src/Lishp/Types.hs100644 blob 82ec4d14e01598f38a083ab2dea326615cd048dd src/Main.hs040000 tree ba0974cba2e671b840bb3cefa69569f62cec29b5 test100644 blob 2589b7aeb1562a3aa951c2fa52f64891db87d1c6 test/assignment.sh100644 blob d54fa0437e13ba5ee15ad13330ac07b9a6abcdcc test/equality.sh100644 blob ee61aa5dffca43032b5aeef6f4e79ff1f9f2df85 test/functions.sh100644 blob 22fbec9189d2641c590836c4bb19431d3c6d3df3 test/primitives.sh100644 blob c455f01b3d88b5d510ff4ebb50e93c5d7f8f0b26 test/types.sh Find the first tag that contains a reference12$ git describe HEADv1.6.1]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-08-24]]></title>
    <url>%2F2018%2F08%2F24%2F2018-08-24%2F</url>
    <content type="text"><![CDATA[bandit 已经通关了 但是还是借助其他人的帮助下才做出来的自己思考估计时间要花出几倍之后要学习 git的操作 还有scrapy 还有ip?(似乎有得多写一下杂食动物的一类文章,增加自己经验 that’s all]]></content>
      <categories>
        <category>不定时的记录</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-08-21 文字判断游戏]]></title>
    <url>%2F2018%2F08%2F21%2F2018-08-21%2F</url>
    <content type="text"><![CDATA[今天将小游戏的界面和声音添加上去还可以接受本次开发就结束了总结:主要是用了kontra的框架结构总的来说远远时小于13kb的…(就算不压缩的话感觉有点一般不过毕竟才花了两天的时间才搞出来也不好太要求过高(借口.. 游戏启动界面 游戏主界面ps:之后的安排就是更新博客了scrapy的还没有写完,额(感觉好多要写,之后就是介绍各个部件了还有一些碎片的知识 wiki上的看得头大英文wiki没有被强,中文的wiki倒是被强了.还好就是中文wiki垃圾的很,可能这就是理由吧(手动狗头]]></content>
      <categories>
        <category>开发日志</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-08-20 文字判断游戏]]></title>
    <url>%2F2018%2F08%2F20%2F2018-08-20%2F</url>
    <content type="text"><![CDATA[开发时间花了两天时间前一天没有写日志总的来说,还行,主要还是框架的功劳今天pressed的判断上想了一天时间….(真菜主要是要press放在了loop中之后时放在外面监听,实现了press一次之后还要添加音乐,应该还算是可以吧…]]></content>
      <categories>
        <category>开发日志</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-08-19]]></title>
    <url>%2F2018%2F08%2F19%2F2018-08-19%2F</url>
    <content type="text"><![CDATA[scrapy json中文乱码可以在setting.py文件中修改默认的输出编码方式，只需要在setting.py中增加如下语句（默认似乎是没有指定的，所以要增加，如果默认有，就直接修改）FEED_EXPORT_ENCODING = &#39;utf-8&#39; rot13ROT13 (“rotate by 13 places”, sometimes hyphenated ROT-13) is a simple letter substitution cipher that replaces a letter with the 13th letter after it, in the alphabet. ROT13 is a special case of the Caesar cipher which was developed in ancient Rome. Because there are 26 letters (2×13) in the basic Latin alphabet, ROT13 is its own inverse; that is, to undo ROT13, the same algorithm is applied, so the same action can be used for encoding and decoding. The algorithm provides virtually no cryptographic security, and is often cited as a canonical example of weak encryption. ROT13 is used in online forums as a means of hiding spoilers, punchlines, puzzle solutions, and offensive materials from the casual glance. ROT13 has inspired a variety of letter and word games on-line, and is frequently mentioned in newsgroup conversations. 加密:tr &#39;A-Za-z&#39; &#39;N-ZA-Mn-za-m&#39; &lt;&lt;&lt; &quot;The Quick Brown Fox Jumps Over The Lazy Dog&quot; 解密:echo &quot;The Quick Brown Fox Jumps Over The Lazy Dog&quot; |tr &#39;N-ZA-Mn-za-m&#39; &#39;A-Za-z&#39; Hex dump(十六进制转储)In computing,a hex dump is a hexadecimal view (on screen or paper) of computer data, from RAM or from a file or storage device. Looking at a hex dump of data is commonly done as a part of debugging, or of reverse engineering. In a hex dump, each byte (8-bits) is represented as a two-digit hexadecimal number.(每个字节==&gt;两个十六进制的数) Hex dumps are commonly organized into rows of 8 or 16 bytes, sometimes separated by whitespaces. Some hex dumps have the hexadecimal memory address at the beginning and/or a checksum byte at the end of each line. Although the name implies the use of base-16 output, some hex dumping software may have options for base-8 (octal) or base-10 (decimal) output. Some common names for this program function are hexdump, od, xxd and simply dump or even D. xxd 得到文件的头 得到的xxd文件要先-r出原来的文件哦 解压方式gzip -d(gunzip)tar -zxvftar -xvfbzip2 -d]]></content>
      <categories>
        <category>杂食动物</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssh教程(another)]]></title>
    <url>%2F2018%2F08%2F19%2Fssh2%2F</url>
    <content type="text"><![CDATA[ssh(another page)Public and Private KeysPublic key authentication is more secure than password authentication. This is particularly important if the computer is visible on the internet. If you don’t think it’s important, try logging the login attempts you get for the next week. My computer - a perfectly ordinary desktop PC - had over 4,000 attempts to guess my password and almost 2,500 break-in attempts in the last week alone. With public key authentication, the authenticating entity has a public key and a private key. Each key is a large number with special mathematical properties. The private key is kept on the computer you log in from, while the public key is stored on the .ssh/authorized_keys file on all the computers you want to log in to. When you log in to a computer, the SSH server uses the public key to “lock” messages in a way that can only be “unlocked” by your private key - this means that even the most resourceful attacker can’t snoop(探听) on, or interfere(接入) with, your session. As an extra security measure, most SSH programs store the private key in a passphrase-protected format, so that if your computer is stolen or broken in to, you should have enough time to disable your old public key before they break the passphrase and start using your key. Wikipedia has a more detailed explanation of how keys work. Public key authentication is a much better solution than passwords for most people. In fact, if you don’t mind leaving a private key unprotected on your hard disk, you can even use keys to do secure automatic log-ins - as part of a network backup, for example. Different SSH programs generate public keys in different ways, but they all generate public keys in a similar format:&lt;ssh-rsa or ssh-dss&gt; &lt;really long string of nonsense&gt; &lt;username&gt;@&lt;host&gt; Key-Based SSH LoginsKey-based authentication is the most secure of several modes of authentication usable with OpenSSH, such as plain password and Kerberos tickets(一种安全方式). Key-based authentication has several advantages over password authentication, for example the key values are significantly more difficult to brute-force, or guess than plain passwords, provided an ample key length. Other authentication methods are only used in very specific situations. SSH can use either “RSA” (Rivest-Shamir-Adleman)(人名) or “DSA” (“Digital Signature Algorithm”) keys. Both of these were considered state-of-the-art(最先进的) algorithms when SSH was invented, but DSA has come to be seen as less secure in recent years. RSA is the only recommended choice for new keys, so this guide uses “RSA key” and “SSH key” interchangeably. Key-based authentication uses two keys, one “public” key that anyone is allowed to see, and another “private” key that only the owner is allowed to see. To securely communicate using key-based authentication, one needs to create a key pair, securely store the private key on the computer one wants to log in from, and store the public key on the computer one wants to log in to. Using key based logins with ssh is generally considered more secure than using plain password logins. This section of the guide will explain the process of generating a set of public/private RSA keys, and using them for logging into your Ubuntu computer(s) via OpenSSH. Generating RSA KeysThe first step involves creating a set of RSA keys for use in authentication. This should be done on the client. To create your public and private SSH keys on the command-line:123mkdir ~/.sshchmod 700 ~/.sshssh-keygen -t rsa You will be prompted for a location to save the keys, and a passphrase for the keys. This passphrase will protect your private key while it’s stored on the hard drive:1234567Generating public/private rsa key pair.Enter file in which to save the key (/home/b/.ssh/id_rsa):Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /home/b/.ssh/id_rsa.Your public key has been saved in /home/b/.ssh/id_rsa.pub.Your public key is now available as .ssh/id_rsa.pub in your home folder. Congratulations! You now have a set of keys. Now it’s time to make your systems allow you to login with them Choosing a good passphraseYou need to change all your locks if your RSA key is stolen. Otherwise the thief could impersonate(扮演) you wherever you authenticate with that key. An SSH key passphrase is a secondary form of security that gives you a little time when your keys are stolen. If your RSA key has a strong passphrase, it might take your attacker a few hours to guess by brute force. That extra time should be enough to log in to any computers you have an account on, delete your old key from the .ssh/authorized_keys file, and add a new key. Your SSH key passphrase is only used to protect your private key from thieves. It’s never transmitted over the Internet, and the strength of your key has nothing to do with the strength of your passphrase. The decision to protect your key with a passphrase involves convenience x security. Note that if you protect your key with a passphrase, then when you type the passphrase to unlock it, your local computer will generally leave the key unlocked for a time. So if you use the key multiple times without logging out of your local account in the meantime, you will probably only have to type the passphrase once. If you do adopt a passphrase, pick a strong one and store it securely in a password manager. You may also write it down on a piece of paper and keep it in a secure place. If you choose not to protect the key with a passphrase, then just press the return when ssh-keygen asks. Key Encryption LevelNote: The default is a 2048 bit key. You can increase this to 4096 bits with the -b flag (Increasing the bits makes it harder to crack the key by brute force methods).ssh-keygen -t rsa -b 4096 Password AuthenticationThe main problem with public key authentication is that you need a secure way of getting the public key onto a computer before you can log in with it. If you will only ever use an SSH key to log in to your own computer from a few other computers (such as logging in to your PC from your laptop), you should copy your SSH keys over on a memory stick, and disable password authentication altogether. If you would like to log in from other computers from time to time (such as a friend’s PC), make sure you have a strong password. Transfer Client Key to HostThe key you need to transfer to the host is the public one. If you can log in to a computer over SSH using a password, you can transfer your RSA key by doing the following from your own computer:ssh-copy-id &lt;username&gt;@&lt;host&gt;Where &lt;username&gt; and &lt;host&gt; should be replaced by your username and the name of the computer you’re transferring your key to. (i) Due to this bug, you cannot specify a port other than the standard port 22. You can work around this by issuing the command like this: ssh-copy-id &quot;&lt;username&gt;@&lt;host&gt; -p &lt;port_nr&gt;&quot;. If you are using the standard port 22, you can ignore this tip. Another alternative is to copy the public key file to the server and concatenate it onto the authorized_keys file manually. It is wise to back that up first:12cp authorized_keys authorized_keys_Backupcat id_rsa.pub &gt;&gt; authorized_keys You can make sure this worked by doing:ssh &lt;username&gt;@&lt;host&gt;You should be prompted for the passphrase for your key:Enter passphrase for key ‘/home//.ssh/id_rsa’:Enter your passphrase, and provided host is configured to allow key-based logins, you should then be logged in as usual. TroubleshootingEncrypted Home DirectoryIf you have an encrypted home directory, SSH cannot access your authorized_keys file because it is inside your encrypted home directory and won’t be available until after you are authenticated. Therefore, SSH will default to password authentication. To solve this, create a folder outside your home named /etc/ssh/&lt;username&gt; (replace &quot;&lt;username&gt;&quot; with your actual username). This directory should have 755 permissions and be owned by the user. Move the authorized_keys file into it. The authorized_keys file should have 644 permissions and be owned by the user.Then edit your /etc/ssh/sshd_config and add:AuthorizedKeysFile /etc/ssh/%u/authorized_keysFinally, restart ssh with:sudo service ssh restartThe next time you connect with SSH you should not have to enter your password. username@host’s password:If you are not prompted for the passphrase, and instead get just theusername@host&#39;s password:prompt as usual with password logins, then read on. There are a few things which could prevent this from working as easily as demonstrated above. On default Ubuntu installs however, the above examples should work. If not, then check the following condition, as it is the most frequent cause: On the host computer, ensure that the /etc/ssh/sshd_config contains the following lines, and that they are uncommented;PubkeyAuthentication yesRSAAuthentication yesIf not, add them, or uncomment them, restart OpenSSH, and try logging in again. If you get the passphrase prompt now, then congratulations, you’re logging in with a key! Permission denied (publickey)If you’re sure you’ve correctly configured sshd_config, copied your ID, and have your private key in the .ssh directory, and still getting this error:Permission denied (publickey).Chances are, your /home/&lt;user&gt; or ~/.ssh/authorized_keys permissions are too open by OpenSSH standards. You can get rid of this problem by issuing the following commands: 123chmod go-w ~/chmod 700 ~/.sshchmod 600 ~/.ssh/authorized_keys Error: Agent admitted failure to sign using the key.This error occurs when the ssh-agent on the client is not yet managing the key. Issue the following commands to fix:ssh-addThis command should be entered after you have copied your public key to the host computer. Debugging and sorting out further problemsThe permissions of files and folders is crucial to this working. You can get debugging information from both the client and server.if you think you have set it up correctly , yet still get asked for the password, try starting the server with debugging output to the terminal.sudo /usr/sbin/sshd -dTo connect and send information to the client terminalssh -v ( or -vv) username@host&#39;s Where to From Here?No matter how your public key was generated, you can add it to your Ubuntu system by opening the file .ssh/authorized_keys in your favourite text editor and adding the key to the bottom of the file. You can also limit the SSH features that the key can use, such as disallowing port-forwarding or only allowing a specific command to be run. This is done by adding “options” before the SSH key, on the same line in the authorized_keys file. For example, if you maintain a CVS repository, you could add a line like this:command=&quot;/usr/bin/cvs server&quot;,no-agent-forwarding,no-port-forwarding,no-X11-forwarding,no-user-rc ssh-dss &lt;string of nonsense&gt;...When the user with the specified key logged in, the server would automatically run /usr/bin/cvs server, ignoring any requests from the client to run another command such as a shell. For more information, see the sshd man page.]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssh教程]]></title>
    <url>%2F2018%2F08%2F18%2Fssh%2F</url>
    <content type="text"><![CDATA[sshIf you’re connecting to another computer over the Internet, you’ll probably want to keep your data safe. SSH is one way to help do that. To make it happen, you’ll need to set up SSH properly on your computer, and then create an encrypted(加密) connection to your server. Just remember, in order for the connection to be secure, both ends of the connection need to have SSH enabled. Follow this guide to make sure that your connection is as safe as possible. Connecting for the First TimeInstall SSH.For Windows, you will need to download and install an SSH client program. The most popular one is Cygwin, which is available for free from the developer’s website. Download and install it like you would any other program. Another popular free program is PuTTY.During the Cygwin installation, you must choose to install OpenSSH from the Net section.Linux and Mac OS X come with SSH already installed on the system. This is because SSH is a UNIX system, and Linux and OS X are derived from UNIX.If you have Windows 10 with the Anniversary Update, you can install the Windows Subsystem for Linux which comes with SSH preinstalled. Run SSH.Open the terminal program that is installed by Cygwin, or Bash on Ubuntu on Windows for Windows 10, or open the Terminal in OS X or Linux. SSH uses the terminal interface to interact with other computers. There is no graphical interface for SSH, so you will need to get comfortable typing in commands. Test the connection.Before you dive into creating secure keys and moving files, you’ll want to test that SSH is properly configured on your computer as well as the system you are connecting to. Enter the following command, replacing with your username on the remote computer, and with the address for the remote computer or server:$ ssh &lt;username&gt;@&lt;remote&gt;You will be asked for your password once the connection is established. You will not see the cursor move or any characters input when you type your password.If this step fails, then either SSH is configured incorrectly on your computer or the remote computer is not accepting SSH connections. basic commands(a part)Copy files from your location to the remote computer. If you need to copy files from your local computer to the computer you are accessing remotely, you can use the scp command:scp /localdirectory/example1.txt &lt;username&gt;@&lt;remote&gt;:&lt;path&gt;will copy example1.txt to the specified on the remote computer. You can leave blank to copy to the root folder of the remote computer.scp &lt;username&gt;@&lt;remote&gt;:/home/example1.txt ./ will move example1.txt from the home directory on the remote computer to the current directory on the local computer. Creating Encrypted KeysCreate your SSH keys.These keys will allow you to connect to the remote location without having to enter your password each time. This is a much more secure way to connect to the remote computer, as the password will not have to transmitted over the network.Create the key folder on your computer by entering the command $ mkdir .sshCreate the public and private keys by using the command $ ssh-keygen –t rsaYou will be asked if you would like to create a passphrase for the keys; this is optional. If you don’t want to create a passphrase, press Enter. This will create two keys in the .ssh directory: id_rsa and id_rsa.pubChange your private key’s permissions. In order to ensure that the private key is only readable by you, enter the command $ chmod 600 .ssh/id_rsa Place the public key on the remote computer.Once your keys are created, you’re ready to place the public key on the remote computer so that you can connect without a password. Enter the following command, replacing the appropriate parts as explained earlier:$ scp .ssh/id_rsa.pub &lt;username&gt;@&lt;remote&gt;:Make sure to include the colon (:) at the end of the command.You will be asked to input your password before the file transfer starts. Install the public key on the remote computer.Once you’ve placed the key on the remote computer, you will need to install it so that it works correctly. First, log in to the remote computer the same way.Create an SSH folder on the remote computer, if it does not already exist: $ mkdir .sshAppend your key to the authorized keys file. If the file does not exist yet, it will be created: $ cat id_rsa.pub &gt;&gt; .ssh/authorized_keysChange the permissions for the SSH folder to allow access: $ chmod 700 .ssh Check that the connection works.Once the key has been installed on the remote computer, you should be able to initiate a connection without being asked to enter your password. Enter the following command to test the connection: $ ssh &lt;username&gt;@&lt;remote&gt;If you connect without being prompted for the password, then the keys are configured correctly.]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-08-18]]></title>
    <url>%2F2018%2F08%2F18%2F2018-08-18%2F</url>
    <content type="text"><![CDATA[SQL注入所谓SQL注入，就是通过把SQL命令插入到Web表单提交或输入域名或页面请求的查询字符串，最终达到欺骗服务器执行恶意的SQL命令。select * from username = ____ and password=_____select * from username &quot;test&quot; or &quot;&quot;=&quot;&quot; and password=&quot;123456&quot; XSSXSS则是攻击者往Web页面里插入恶意Script代码，当用户浏览该页之时，嵌入Web里面的Script代码会被执行，从而达到恶意攻击用户的目的。&lt;p style=&#39;color:red&#39;&gt;你好啊，尊敬的______&lt;p&gt;&lt;p style=&#39;color:red&#39;&gt;你好啊，尊敬的 xxx&lt;script&gt;alert(1)&lt;/script&gt;&lt;p&gt; 远程命令执行而远程命令执行，是用户通过浏览器提交执行命令，由于服务器端没有针对执行函数做过滤，导致执行命令。ping ___ping www.baidu.com &amp; wget xxxxxxxxxxx 越权越权漏洞是比较常见的漏洞类型，越权漏洞可以理解为，一个正常的用户A通常只能够对自己的一些信息进行增删改查，但是由于程序员的一时疏忽 ，对信息进行增删改查的时候没有进行一个判断，判断所需要操作的信息是否属于对应的用户，可以导致用户A可以操作其他人的信息。Cookie: uid=11426;Cookie: uid=1;]]></content>
      <categories>
        <category>杂食动物</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy Pass 4]]></title>
    <url>%2F2018%2F08%2F17%2Fscrapy4%2F</url>
    <content type="text"><![CDATA[Command line toolNew in version 0.10. Scrapy is controlled through the scrapy command-line tool, to be referred here as the “Scrapy tool” to differentiate it from the sub-commands, which we just call “commands” or “Scrapy commands”. The Scrapy tool provides several commands, for multiple purposes, and each one accepts a different set of arguments and options. Configuration settingsScrapy will look for configuration parameters in ini-style scrapy.cfg files in standard locations: /etc/scrapy.cfg or c:\scrapy\scrapy.cfg (system-wide), ~/.config/scrapy.cfg ($XDG_CONFIG_HOME) and ~/.scrapy.cfg ($HOME) for global (user-wide) settings, and scrapy.cfg inside a scrapy project’s root (see next section).Settings from these files are merged in the listed order of preference: user-defined values have higher priority than system-wide defaults and project-wide settings will override all others, when defined.(优先级依次递增)Scrapy also understands, and can be configured through, a number of environment variables. Currently these are: SCRAPY_SETTINGS_MODULE (see Designating the settings)SCRAPY_PROJECTSCRAPY_PYTHON_SHELL (see Scrapy shell) Default structure of Scrapy projectsBefore delving into the command-line tool and its sub-commands, let’s first understand the directory structure of a Scrapy project. Though it can be modified, all Scrapy projects have the same file structure by default, similar to this:123456789101112scrapy.cfgmyproject/ __init__.py items.py middlewares.py pipelines.py settings.py spiders/ __init__.py spider1.py spider2.py ... The directory where the scrapy.cfg file resides is known as the project root directory. That file contains the name of the python module that defines the project settings. Here is an example:12[settings]default = myproject.settings Using the scrapy toolYou can start by running the Scrapy tool with no arguments and it will print some usage help and the available commands:123456789Scrapy X.Y - no active projectUsage: scrapy &lt;command&gt; [options] [args]Available commands: crawl Run a spider fetch Fetch a URL using the Scrapy downloader[...] The first line will print the currently active project if you’re inside a Scrapy project. In this example it was run from outside a project. If run from inside a project it would have printed something like this:123456Scrapy X.Y - project: myprojectUsage: scrapy &lt;command&gt; [options] [args][...] Creating projectsThe first thing you typically do with the scrapy tool is create your Scrapy project:scrapy startproject myproject [project_dir]That will create a Scrapy project under the project_dir directory. If project_dir wasn’t specified, project_dir will be the same as myproject.Next, you go inside the new project directory:cd project_dirAnd you’re ready to use the scrapy command to manage and control your project from there(project_dir). Controlling projectsYou use the scrapy tool from inside your projects to control and manage them.For example, to create a new spider:scrapy genspider mydomain mydomain.comSome Scrapy commands (like crawl) must be run from inside a Scrapy project. Also keep in mind that some commands may have slightly different behaviours when running them from inside projects. For example, the fetch command will use spider-overridden behaviours (such as the user_agent attribute to override the user-agent) if the url being fetched is associated with some specific spider. This is intentional, as the fetch command is meant to be used to check how spiders are downloading pages.(注意某一些特殊命令会有不同的行为,这不废话吗) Available tool commandsThis section contains a list of the available built-in commands with a description and some usage examples. Remember, you can always get more info about each command by running:scrapy &lt;command&gt; -hAnd you can see all available commands with:scrapy -hThere are two kinds of commands, those that only work from inside a Scrapy project (Project-specific commands) and those that also work without an active Scrapy project (Global commands), though they may behave slightly different when running from inside a project (as they would use the project overridden settings). Global commands:12345678startprojectgenspidersettingsrunspidershellfetchviewversion Project-only commands:123456crawlchecklisteditparsebench startproject(创建项目) Syntax: scrapy startproject &lt;project_name&gt; [project_dir]Requires project: no Creates a new Scrapy project named project_name, under the project_dir directory. If project_dir wasn’t specified, project_dir will be the same as project_name. Usage example:$ scrapy startproject myproject genspider Syntax: scrapy genspider [-t template] Requires project: no Create a new spider in the current folder or in the current project’s spiders folder, if called from inside a project. The parameter is set as the spider’s name, while is used to generate the allowed_domains and start_urls spider’s attributes. Usage example:123456789101112$ scrapy genspider -lAvailable templates: basic crawl csvfeed xmlfeed$ scrapy genspider example example.comCreated spider &apos;example&apos; using template &apos;basic&apos;$ scrapy genspider -t crawl scrapyorg scrapy.orgCreated spider &apos;scrapyorg&apos; using template &apos;crawl&apos; This is just a convenience shortcut command for creating spiders based on pre-defined templates, but certainly not the only way to create spiders. You can just create the spider source code files yourself, instead of using this command. crawl Syntax: scrapy crawl Requires project: yesStart crawling using a spider. Usage examples:12$ scrapy crawl myspider[ ... myspider starts crawling ... ] check Syntax: scrapy check [-l] Requires project: yesRun contract checks.(链接测试) Usage examples:1234567891011121314$ scrapy check -lfirst_spider * parse * parse_itemsecond_spider * parse * parse_item$ scrapy check[FAILED] first_spider:parse_item&gt;&gt;&gt; &apos;RetailPricex&apos; field is missing[FAILED] first_spider:parse&gt;&gt;&gt; Returned 92 requests, expected 0..4 list(列出项目中的所有spider) Syntax: scrapy listRequires project: yesList all available spiders in the current project. The output is one spider per line. Usage example:123$ scrapy listspider1spider2 edit(编辑) Syntax: scrapy edit Requires project: yesEdit the given spider using the editor defined in the EDITOR environment variable or (if unset) the EDITOR setting.(我觉得我可能不会用到) This command is provided only as a convenience shortcut for the most common case, the developer is of course free to choose any tool or IDE to write and debug spiders. Usage example:1$ scrapy edit spider1 fetch (下载页面代码) Syntax: scrapy fetch Requires project: noDownloads the given URL using the Scrapy downloader and writes the contents to standard output. The interesting thing about this command is that it fetches the page how the spider would download it. For example, if the spider has a USER_AGENT attribute which overrides the User Agent, it will use that one. So this command can be used to “see” how your spider would fetch a certain page. If used outside a project, no particular per-spider behaviour would be applied and it will just use the default Scrapy downloader settings. Supported options: --spider=SPIDER: bypass spider autodetection and force use of specific spider--headers: print the response’s HTTP headers instead of the response’s body--no-redirect: do not follow HTTP 3xx redirects (default is to follow them) Usage examples:12345678910111213$ scrapy fetch --nolog http://www.example.com/some/page.html[ ... html content here ... ]$ scrapy fetch --nolog --headers http://www.example.com/&#123;&apos;Accept-Ranges&apos;: [&apos;bytes&apos;], &apos;Age&apos;: [&apos;1263 &apos;], &apos;Connection&apos;: [&apos;close &apos;], &apos;Content-Length&apos;: [&apos;596&apos;], &apos;Content-Type&apos;: [&apos;text/html; charset=UTF-8&apos;], &apos;Date&apos;: [&apos;Wed, 18 Aug 2010 23:59:46 GMT&apos;], &apos;Etag&apos;: [&apos;&quot;573c1-254-48c9c87349680&quot;&apos;], &apos;Last-Modified&apos;: [&apos;Fri, 30 Jul 2010 15:30:18 GMT&apos;], &apos;Server&apos;: [&apos;Apache/2.2.3 (CentOS)&apos;]&#125; view Syntax: scrapy view Requires project: noOpens the given URL in a browser, as your Scrapy spider would “see” it. Sometimes spiders see pages differently from regular users, so this can be used to check what the spider “sees” and confirm it’s what you expect. Supported options: --spider=SPIDER: bypass spider autodetection and force use of specific spider--no-redirect: do not follow HTTP 3xx redirects (default is to follow them)Usage example:12$ scrapy view http://www.example.com/some/page.html[ ... browser starts ... ] shell Syntax: scrapy shell [url]Requires project: noStarts the Scrapy shell for the given URL (if given) or empty if no URL is given. Also supports UNIX-style local file paths, either relative with ./ or ../ prefixes or absolute file paths. See Scrapy shell for more info. Supported options: --spider=SPIDER: bypass spider autodetection and force use of specific spider-c code(设置返回状态): evaluate the code in the shell, print the result and exit--no-redirect: do not follow HTTP 3xx redirects (default is to follow them); this only affects the URL you may pass as argument on the command line; once you are inside the shell, fetch(url) will still follow HTTP redirects by default.Usage example:1234567891011121314$ scrapy shell http://www.example.com/some/page.html[ ... scrapy shell starts ... ]$ scrapy shell --nolog http://www.example.com/ -c &apos;(response.status, response.url)&apos;(200, &apos;http://www.example.com/&apos;)# shell follows HTTP redirects by default$ scrapy shell --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c &apos;(response.status, response.url)&apos;(200, &apos;http://example.com/&apos;)# you can disable this with --no-redirect# (only for the URL passed as command line argument)$ scrapy shell --no-redirect --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c &apos;(response.status, response.url)&apos;(302, &apos;http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F&apos;) parse Syntax: scrapy parse [options]Requires project: yesFetches the given URL and parses it with the spider that handles it, using the method passed with the –callback option, or parse if not given. Supported options: --spider=SPIDER: bypass spider autodetection and force use of specific spider--a NAME=VALUE: set spider argument (may be repeated)--callback or -c: spider method to use as callback for parsing the response--meta or -m: additional request meta that will be passed to the callback request. This must be a valid json string. Example: –meta=’{“foo” : “bar”}’--pipelines: process items through pipelines--rules or -r: use CrawlSpider rules to discover the callback (i.e. spider method) to use for parsing the response--noitems: don’t show scraped items--nolinks: don’t show extracted links--nocolour: avoid using pygments to colorize the output--depth or -d: depth level for which the requests should be followed recursively (default: 1)--verbose or -v: display information for each depth levelUsage example:1234567891011$ scrapy parse http://www.example.com/ -c parse_item[ ... scrapy log lines crawling example.com spider ... ]&gt;&gt;&gt; STATUS DEPTH LEVEL 1 &lt;&lt;&lt;# Scraped Items ------------------------------------------------------------[&#123;&apos;name&apos;: u&apos;Example item&apos;, &apos;category&apos;: u&apos;Furniture&apos;, &apos;length&apos;: u&apos;12 cm&apos;&#125;]# Requests -----------------------------------------------------------------[] settings Syntax: scrapy settings [options]Requires project: noGet the value of a Scrapy setting. If used inside a project it’ll show the project setting value, otherwise it’ll show the default Scrapy value for that setting. Example usage:1234$ scrapy settings --get BOT_NAMEscrapybot$ scrapy settings --get DOWNLOAD_DELAY0 runspider(单纯的运行文件) Syntax: scrapy runspider &lt;spider_file.py&gt;Requires project: noRun a spider self-contained in a Python file, without having to create a project. Example usage:12$ scrapy runspider myspider.py[ ... spider starts crawling ... ] version Syntax: scrapy version [-v]Requires project: noPrints the Scrapy version. If used with -v it also prints Python, Twisted and Platform info, which is useful for bug reports. bench(估计用不着)New in version 0.17. Syntax: scrapy benchRequires project: noRun a quick benchmark test. Benchmarking.]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-08-17]]></title>
    <url>%2F2018%2F08%2F17%2F2018-08-17%2F</url>
    <content type="text"><![CDATA[暴露真实IP会遭到DDos攻击(DDoS:Distributed Denial of Service)分布式拒绝攻击 Windows 用户“一般被默认授予管理员权限，那意味着他们几乎可以访问系统中的一切”。Linux，反而很好地限制了“root”权限 macos “通过隐匿实现的安全”，这秉承了“让软件内部运作保持专有，从而不为人知是抵御攻击的最好方法”的理念 Windows 的流行本身就是个问题，操作系统的安全性可能很大程度上依赖于装机用户量的规模。对于恶意软件作者来说，Windows 提供了大的施展平台。专注其中可以让他们的努力发挥最大作用。 手机IMEI码在手机拨号处输入”*#06#”得到手机IMEI码,相当于手机的身份证号码,移动运营商通过IMEI码分辨用户设备，追踪用户地理位置，记录用户拨打电话、发送短信、上网等行为。(可怕) python控制浏览器12import webbrowserwebbrowser.open(&apos;http://baidu.com&apos;) pychram 安装第三方库失败解决方法pip install -i https://pypi.tuna.tsinghua.edu.cn/simple some-package ed2ked2k全称叫“eDonkey2000 network”(电驴???)，是一种文件共享网络，最初用于共享音乐、电影和软件。与多数文件共享网络一样，它是分布式的；文件基于P2P原理存放于用户的电脑上而不是存储于一个中枢服务器。 eDonkey客户端程序连接到这个网络来共享文件。而eDonkey服务器作为一个通讯中心，使用户在ed2k网络内查找文件。它的客户端和服务端可以工作于Windows、Macintosh(这是mac..汗颜)、Linux、UNIX操作系统。任何人 都可以作为服务器加入这个网络。由于服务器经常变化，客户端会经常更新它的服务器列表。eDonkey用混合MD4摘要算法检查来识别文件。这使ed2k网络可以将不同文件名的同一文件成功识别为一个文件，并使同一文件名的不同文件得以区分(厉害)。eDonkeyd的另一特性是：对大于9.8MB的文件，它在下载完成前将其分割；这将加速大型文件的发送。为了便于文件搜索，一些Web站点对比较热门的文件建立 ed2k链接 ，这些网站通常也提供热门服务器列表便于用户更新。 fork boomfork炸弹（fork bomb）在计算机领域中是一种利用系统调用fork（或其他等效的方式）进行的拒绝服务攻击。与病毒与蠕虫不同的是，fork炸弹没有传染性，而且fork炸弹会使对同时执行进程、程序数设限的系统无法执行新程序，对于不设限的系统则使之停止响应。fork炸弹通过进程递归式派生(fork，亦即自我复制)，以使系统拒绝服务甚至崩溃。 fork炸弹以极快的速度创建大量进程（进程数呈以2为底数的指数增长趋势），并以此消耗系统分配予进程的可用空间使进程表饱和，而系统在进程表饱和后就无法运行新程序，除非进程表中的某一进程终止；但由于fork炸弹程序所创建的所有实例都会不断探测空缺的进程槽并尝试取用以创建新进程，因而即使在某进程终止后也基本不可能运行新进程。fork炸弹生成的子程序在消耗进程表空间的同时也会占用CPU和内存，从而导致系统与现有进程运行速度放缓，响应时间也会随之大幅增加，以致于无法正常完成任务，从而使系统的正常运作受到严重影响。由于现代Unix操作系统普遍采用运用写实拷贝技术，fork炸弹通常不会使进程表饱和。除了恶意触发fork炸弹破坏的情况外，软件开发中有时也会不慎在程序中嵌入fork炸弹，如在用于监听网络套接字并行使客户端-服务器结构系统中服务器端职责的应用程序中可能需要无限地进行循环（loop）与派生（fork）操作（类似下节示例程序所示），而在这种情况下源代码内的细微错误就可能在测试中“引爆”fork炸弹。以下程序段就是由Jaromil所作的在类UNIX系统的shell环境下触发fork炸弹的shell脚本代码，总共只用了13个字符（包括空格）：:(){ :|:&amp; };:注解如下： :() # 定义函数,函数名为”:”,即每当输入”:”时就会自动调用{}内代码{ # “:”函数开始标识: # 用递归方式调用”:”函数本身| # 并用管道(pipe)将其输出引至…: # 另一次递归调用的”:”函数综上,”:|:”表示的即是每次调用函数”:”的时候就会生成两份拷贝&amp; # 调用间脱钩,以使最初的”:”函数被杀死后为其所调用的两个”:”函数还能继续执行} # “:”函数结束标识; # “:”函数定义结束后将要进行的操作…: # 调用”:”函数,”引爆”fork炸弹其中函数名“:”只是简化的一例，实际实现时可以随意设定，一个较易理解（将函数名替换为“forkbomb”）的版本如下：forkbomb(){ forkbomb|forkbomb &amp;} ; forkbomb(自身调用自身?….理解成死循环好了) Windows下则可以批处理命令如下实现：%0|%0 POSIX标准下的C与C++的实现：#include &lt;unistd.h&gt;int main(){while(1) fork();return0;} Perl语言的实现：fork while fork 在系统中成功“引爆”fork炸弹后，我们可重启来使系统恢复正常运行；而若要以手动的方法使fork炸弹“熄火”，那前提就是必须杀死fork炸弹产生的所有进程。为此我们可以考虑使用程序来杀死fork炸弹产生的进程，但由于这一般需要创建新进程，且由于fork炸弹一直在探测与占用进程槽与内存空间，因而这一方法几乎不可能实现，而且用kill命令杀死进程后，释放出的进程槽又会被余下的fork炸弹线程所产生的新进程占用，在Windows下，用户可以退出当前用户会话的方式使系统恢复正常，但此法奏效的前提是fork炸弹是在该用户的特定会话内触发的]]></content>
      <categories>
        <category>杂食动物</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy Pass 3]]></title>
    <url>%2F2018%2F08%2F16%2Fscrapy3%2F</url>
    <content type="text"><![CDATA[Learn Scrapy pass 3接上文Extract dataExtracting quotes and authorsNow that you know a bit about selection and extraction, let’s complete our spider by writing the code to extract the quotes from the web page. Each quote in http://quotes.toscrape.comis represented by HTML elements that look like this:123456789101112131415&lt;div class=&quot;quote&quot;&gt; &lt;span class=&quot;text&quot;&gt;“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”&lt;/span&gt; &lt;span&gt; by &lt;small class=&quot;author&quot;&gt;Albert Einstein&lt;/small&gt; &lt;a href=&quot;/author/Albert-Einstein&quot;&gt;(about)&lt;/a&gt; &lt;/span&gt; &lt;div class=&quot;tags&quot;&gt; Tags: &lt;a class=&quot;tag&quot; href=&quot;/tag/change/page/1/&quot;&gt;change&lt;/a&gt; &lt;a class=&quot;tag&quot; href=&quot;/tag/deep-thoughts/page/1/&quot;&gt;deep-thoughts&lt;/a&gt; &lt;a class=&quot;tag&quot; href=&quot;/tag/thinking/page/1/&quot;&gt;thinking&lt;/a&gt; &lt;a class=&quot;tag&quot; href=&quot;/tag/world/page/1/&quot;&gt;world&lt;/a&gt; &lt;/div&gt;&lt;/div&gt; Let’s open up scrapy shell and play a bit to find out how to extract the data we want:$ scrapy shell &#39;http://quotes.toscrape.com&#39;We get a list of selectors for the quote HTML elements with:1&gt;&gt;&gt; response.css(&quot;div.quote&quot;) Each of the selectors returned by the query above allows us to run further queries over their sub-elements. Let’s assign the first selector to a variable, so that we can run our CSS selectors directly on a particular quote:1&gt;&gt;&gt; quote = response.css(&quot;div.quote&quot;)[0] Now, let’s extract title, author and the tags from that quote using the quote object we just created:123456&gt;&gt;&gt; title = quote.css(&quot;span.text::text&quot;).extract_first()&gt;&gt;&gt; title&apos;“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”&apos;&gt;&gt;&gt; author = quote.css(&quot;small.author::text&quot;).extract_first()&gt;&gt;&gt; author&apos;Albert Einstein&apos; Given that the tags are a list of strings, we can use the .extract() method to get all of them:123&gt;&gt;&gt; tags = quote.css(&quot;div.tags a.tag::text&quot;).extract()&gt;&gt;&gt; tags[&apos;change&apos;, &apos;deep-thoughts&apos;, &apos;thinking&apos;, &apos;world&apos;] Having figured out how to extract each bit, we can now iterate over all the quotes elements and put them together into a Python dictionary:123456789&gt;&gt;&gt; for quote in response.css(&quot;div.quote&quot;):... text = quote.css(&quot;span.text::text&quot;).extract_first()... author = quote.css(&quot;small.author::text&quot;).extract_first()... tags = quote.css(&quot;div.tags a.tag::text&quot;).extract()... print(dict(text=text, author=author, tags=tags))&#123;&apos;tags&apos;: [&apos;change&apos;, &apos;deep-thoughts&apos;, &apos;thinking&apos;, &apos;world&apos;], &apos;author&apos;: &apos;Albert Einstein&apos;, &apos;text&apos;: &apos;“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”&apos;&#125;&#123;&apos;tags&apos;: [&apos;abilities&apos;, &apos;choices&apos;], &apos;author&apos;: &apos;J.K. Rowling&apos;, &apos;text&apos;: &apos;“It is our choices, Harry, that show what we truly are, far more than our abilities.”&apos;&#125; ... a few more of these, omitted for brevity&gt;&gt;&gt; Extracting data in our spiderLet’s get back to our spider. Until now, it doesn’t extract any data in particular, just saves the whole HTML page to a local file (额) . Let’s integrate the extraction logic(逻辑) above into our spider. A Scrapy spider typically generates many dictionaries containing the data extracted from the page. To do that, we use the yield Python keyword in the callback, as you can see below:12345678910111213141516import scrapyclass QuotesSpider(scrapy.Spider): name = &quot;quotes&quot; start_urls = [ &apos;http://quotes.toscrape.com/page/1/&apos;, &apos;http://quotes.toscrape.com/page/2/&apos;, ] def parse(self, response): for quote in response.css(&apos;div.quote&apos;): yield &#123; &apos;text&apos;: quote.css(&apos;span.text::text&apos;).extract_first(), &apos;author&apos;: quote.css(&apos;small.author::text&apos;).extract_first(), &apos;tags&apos;: quote.css(&apos;div.tags a.tag::text&apos;).extract(), &#125; (yield 不断调用,似乎不用储存) If you run this spider, it will output the extracted data with the log:12342016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://quotes.toscrape.com/page/1/&gt;&#123;&apos;tags&apos;: [&apos;life&apos;, &apos;love&apos;], &apos;author&apos;: &apos;André Gide&apos;, &apos;text&apos;: &apos;“It is better to be hated for what you are than to be loved for what you are not.”&apos;&#125;2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://quotes.toscrape.com/page/1/&gt;&#123;&apos;tags&apos;: [&apos;edison&apos;, &apos;failure&apos;, &apos;inspirational&apos;, &apos;paraphrased&apos;], &apos;author&apos;: &apos;Thomas A. Edison&apos;, &apos;text&apos;: &quot;“I have not failed. I&apos;ve just found 10,000 ways that won&apos;t work.”&quot;&#125; Storing the scraped dataThe simplest way to store the scraped data is by using Feed exports, with the following command:scrapy crawl quotes -o quotes.json That will generate an quotes.json file containing all scraped items, serialized in JSON. For historic reasons, Scrapy appends to a given file instead of overwriting its contents. If you run this command twice without removing the file before the second time, you’ll end up with a broken JSON file.(两次使用会损坏json文件)You can also use other formats, like JSON Lines:scrapy crawl quotes -o quotes.jl The JSON Lines format is useful because it’s stream-like, you can easily append new records to it. It doesn’t have the same problem of JSON when you run twice. Also, as each record is a separate line, you can process big files without having to fit everything in memory, there are tools like JQ to help doing that at the command-line. In small projects (like the one in this tutorial), that should be enough. However, if you want to perform more complex things with the scraped items, you can write an Item Pipeline. A placeholder file for Item Pipelines has been set up for you when the project is created, in tutorial/pipelines.py.(预先创建好了pipelines文件) Though you don’t need to implement any item pipelines if you just want to store the scraped items. Following linksLet’s say, instead of just scraping the stuff from the first two pages from http://quotes.toscrape.com, you want quotes from all the pages in the website. Now that you know how to extract data from pages, let’s see how to follow links from them. First thing is to extract the link to the page we want to follow. Examining our page, we can see there is a link to the next page with the following markup:12345&lt;ul class=&quot;pager&quot;&gt; &lt;li class=&quot;next&quot;&gt; &lt;a href=&quot;/page/2/&quot;&gt;Next &lt;span aria-hidden=&quot;true&quot;&gt;&amp;rarr;&lt;/span&gt;&lt;/a&gt; &lt;/li&gt;&lt;/ul&gt; We can try extracting it in the shell:12&gt;&gt;&gt; response.css(&apos;li.next a&apos;).extract_first()&apos;&lt;a href=&quot;/page/2/&quot;&gt;Next &lt;span aria-hidden=&quot;true&quot;&gt;→&lt;/span&gt;&lt;/a&gt;&apos; This gets the anchor element, but we want the attribute href. For that, Scrapy supports a CSS extension that let’s you select the attribute contents, like this:12&gt;&gt;&gt; response.css(&apos;li.next a::attr(href)&apos;).extract_first()&apos;/page/2/&apos; Let’s see now our spider modified to recursively follow the link to the next page, extracting data from it:123456789101112131415161718192021import scrapyclass QuotesSpider(scrapy.Spider): name = &quot;quotes&quot; start_urls = [ &apos;http://quotes.toscrape.com/page/1/&apos;, ] def parse(self, response): for quote in response.css(&apos;div.quote&apos;): yield &#123; &apos;text&apos;: quote.css(&apos;span.text::text&apos;).extract_first(), &apos;author&apos;: quote.css(&apos;small.author::text&apos;).extract_first(), &apos;tags&apos;: quote.css(&apos;div.tags a.tag::text&apos;).extract(), &#125; next_page = response.css(&apos;li.next a::attr(href)&apos;).extract_first() if next_page is not None: next_page = response.urljoin(next_page) yield scrapy.Request(next_page, callback=self.parse) Now, after extracting the data, the parse() method looks for the link to the next page, builds a full absolute URL using the urljoin() method (since the links can be relative) and yields a new request to the next page, registering itself as callback to handle the data extraction for the next page and to keep the crawling going through all the pages. What you see here is Scrapy’s mechanism(机制) of following links: when you yield a Request in a callback method, Scrapy will schedule that request to be sent and register a callback method to be executed when that request finishes. Using this, you can build complex crawlers that follow links according to rules you define, and extract different kinds of data depending on the page it’s visiting. In our example, it creates a sort of loop, following all the links to the next page until it doesn’t find one – handy for crawling blogs, forums and other sites with pagination. A shortcut for creating RequestsAs a shortcut for creating Request objects you can use response.follow:1234567891011121314151617181920import scrapyclass QuotesSpider(scrapy.Spider): name = &quot;quotes&quot; start_urls = [ &apos;http://quotes.toscrape.com/page/1/&apos;, ] def parse(self, response): for quote in response.css(&apos;div.quote&apos;): yield &#123; &apos;text&apos;: quote.css(&apos;span.text::text&apos;).extract_first(), &apos;author&apos;: quote.css(&apos;span small::text&apos;).extract_first(), &apos;tags&apos;: quote.css(&apos;div.tags a.tag::text&apos;).extract(), &#125; next_page = response.css(&apos;li.next a::attr(href)&apos;).extract_first() if next_page is not None: yield response.follow(next_page, callback=self.parse) Unlike scrapy.Request, response.follow supports relative URLs directly - no need to call urljoin.(震惊,似乎挺厉害的) Note that response.follow just returns a Request instance; you still have to yield this Request. You can also pass a selector to response.follow instead of a string; this selector should extract necessary attributes:12for href in response.css(&apos;li.next a::attr(href)&apos;): yield response.follow(href, callback=self.parse) For elements there is a shortcut: response.follow uses their href attribute automatically. So the code can be shortened further:12for a in response.css(&apos;li.next a&apos;): yield response.follow(a, callback=self.parse) Noteresponse.follow(response.css(&#39;li.next a&#39;)) is not valid because response.css returns a list-like object with selectors for all results, not a single selector. A for loop like in the example above, or response.follow(response.css(&#39;li.next a&#39;)[0]) is fine. More examples and patternsHere is another spider that illustrates callbacks and following links, this time for scraping author information:12345678910111213141516171819202122import scrapyclass AuthorSpider(scrapy.Spider): name = &apos;author&apos; start_urls = [&apos;http://quotes.toscrape.com/&apos;] def parse(self, response): # follow links to author pages for href in response.css(&apos;.author + a::attr(href)&apos;): yield response.follow(href, self.parse_author) # follow pagination links for href in response.css(&apos;li.next a::attr(href)&apos;): yield response.follow(href, self.parse) def parse_author(self, response): def extract_with_css(query): return response.css(query).extract_first().strip() yield &#123; &apos;name&apos;: extract_with_css(&apos;h3.author-title::text&apos;), &apos;birthdate&apos;: extract_with_css(&apos;.author-born-date::text&apos;), &apos;bio&apos;: extract_with_css(&apos;.author-description::text&apos;), &#125; This spider will start from the main page, it will follow all the links to the authors pages calling the parse_author callback for each of them, and also the pagination links with the parse callback as we saw before. Here we’re passing callbacks to response.follow as positional arguments to make the code shorter; it also works for scrapy.Request. The parse_author callback defines a helper function to extract and cleanup the data from a CSS query and yields the Python dict with the author data. Another interesting thing this spider demonstrates(显示) is that, even if there are many quotes from the same author, we don’t need to worry about visiting the same author page multiple times. By default, Scrapy filters out duplicated requests to URLs already visited, avoiding the problem of hitting servers too much because of a programming mistake. This can be configured by the setting DUPEFILTER_CLASS.(不用担心死循环) Hopefully by now you have a good understanding of how to use the mechanism of following links and callbacks with Scrapy. As yet another example spider that leverages the mechanism of following links, check out the CrawlSpider class for a generic spider that implements a small rules engine that you can use to write your crawlers on top of it.(可以写自己的规则?是这个意思吧)Also, a common pattern is to build an item with data from more than one page, using a trick to pass additional data to the callbacks. Using spider argumentsYou can provide command line arguments to your spiders by using the -a option when running them:scrapy crawl quotes -o quotes-humor.json -a tag=humor(使用-a选项 添加attr)These arguments are passed to the Spider’s __init__ method and become spider attributes by default. In this example, the value provided for the tag argument will be available via self.tag. You can use this to make your spider fetch only quotes with a specific tag, building the URL based on the argument:1234567891011121314151617181920import scrapyclass QuotesSpider(scrapy.Spider): name = &quot;quotes&quot; def start_requests(self): url = &apos;http://quotes.toscrape.com/&apos; tag = getattr(self, &apos;tag&apos;, None) if tag is not None: url = url + &apos;tag/&apos; + tag yield scrapy.Request(url, self.parse) def parse(self, response): for quote in response.css(&apos;div.quote&apos;): yield &#123; &apos;text&apos;: quote.css(&apos;span.text::text&apos;).extract_first(), &apos;author&apos;: quote.css(&apos;small.author::text&apos;).extract_first(), &#125;(先处理得到的文本) next_page = response.css(&apos;li.next a::attr(href)&apos;).extract_first() (再去获取下一个page) if next_page is not None: yield response.follow(next_page, self.parse) If you pass the tag=humor argument to this spider, you’ll notice that it will only visit URLs from the humor tag, such as http://quotes.toscrape.com/tag/humor. You can learn more about handling spider arguments here.]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy Pass 2]]></title>
    <url>%2F2018%2F08%2F16%2Fscrapy2%2F</url>
    <content type="text"><![CDATA[Learn Scrapy pass 2安装(虚拟环境)Python packages can be installed either globally (a.k.a system wide), or in user-space. We do not recommend installing scrapy system wide.(并不推荐把python package安装成系统的一部分) Instead, we recommend that you install scrapy within a so-called “virtual environment” (virtualenv). Virtualenvs allow you to not conflict with already-installed Python system packages (which could break some of your system tools and scripts), and still install packages normally with pip (without sudo and the likes).(不会对系统现有的包产生冲突,可以使用普通用户的形式进行安装) Once you have created a virtualenv, you can install scrapy inside it with pip, just like any other Python package.(这里不介绍安装env,提一下,pycharm控制台自带虚拟环境) Python virtualenvs can be created to use Python 2 by default, or Python 3 by default.If you want to install scrapy with Python 3, install scrapy within a Python 3 virtualenv.And if you want to install scrapy with Python 2, install scrapy within a Python 2 virtualenv. Scrapy 使用示例We are going to scrape quotes.toscrape.com, a website that lists quotes from famous authors. This tutorial will walk you through these tasks: Creating a new Scrapy project Writing a spider to crawl a site and extract data Exporting the scraped data using the command line Changing spider to recursively follow links Using spider arguments Creating a projectBefore you start scraping, you will have to set up a new Scrapy project. Enter a directory where you’d like to store your code and run:scrapy startproject tutorialThis will create a tutorial directory with the following contents:1234567891011121314151617得到默认的初始化目录tutorial/ scrapy.cfg # deploy configuration file 配置文件 tutorial/ # project&apos;s Python module, you&apos;ll import your code from here __init__.py items.py # project items definition file middlewares.py # project middlewares file pipelines.py # project pipelines file settings.py # project settings file 配置文件 spiders/ # a directory where you&apos;ll later put your spiders __init__.py Our first SpiderSpiders are classes that you define and that Scrapy uses to scrape information from a website (or a group of websites). They must subclass scrapy.Spider and define the initial requests to make, optionally how to follow links in the pages, and how to parse the downloaded page content to extract data.(Spiders目录是自己定义的,通过爬虫来爬取website)This is the code for our first Spider. Save it in a file named quotes_spider.py under the (注意位置) tutorial/spidersdirectory in your project:12345678910111213141516171819import scrapyclass QuotesSpider(scrapy.Spider): name = &quot;quotes&quot; def start_requests(self): urls = [ &apos;http://quotes.toscrape.com/page/1/&apos;, &apos;http://quotes.toscrape.com/page/2/&apos;, ] for url in urls: yield scrapy.Request(url=url, callback=self.parse) def parse(self, response): page = response.url.split(&quot;/&quot;)[-2] filename = &apos;quotes-%s.html&apos; % page with open(filename, &apos;wb&apos;) as f: f.write(response.body) self.log(&apos;Saved file %s&apos; % filename) 一个带有 yield 的函数就是一个 generator，它和普通函数不同，生成一个 generator 看起来像函数调用，但不会执行任何函数代码，直到对其调用 next()（在 for 循环中会自动调用 next()）才开始执行。虽然执行流程仍按函数的流程执行，但每执行到一个 yield 语句就会中断，并返回一个迭代值，下次执行时从 yield 的下一个语句继续执行。看起来就好像一个函数在正常执行的过程中被 yield 中断了数次，每次中断都会通过 yield 返回当前的迭代值。 yield 的好处是显而易见的，把一个函数改写为一个 generator 就获得了迭代能力，比起用类的实例保存状态来计算下一个 next() 的值，不仅代码简洁，而且执行流程异常清晰。As you can see, our Spider subclasses scrapy.Spider and defines some attributes and methods: name: identifies the Spider.(定义爬虫名字) It must be unique within a project, that is, you can’t set the same name for different Spiders. start_requests(): must return an iterable of Requests (you can return a list of requests or write a generator function) which the Spider will begin to crawl from. Subsequent requests will be generated successively from these initial requests. parse(): a method that will be called to handle the response downloaded for each of the requests made. The response parameter is an instance of TextResponse that holds the page content and has further helpful methods to handle it. The parse() method usually parses the response, extracting the scraped data as dicts and also finding new URLs to follow and creating new requests (Request) from them.(提取信息并得到下一步url) How to run our spiderTo put our spider to work, go to the project’s top level directory and run:scrapy crawl quotes(quotes就是之前定义的name)This command runs the spider with name quotes that we’ve just added, that will send some requests for the quotes.toscrape.com domain. You will get an output similar to this:12345678910... (omitted for brevity)2016-12-16 21:24:05 [scrapy.core.engine] INFO: Spider opened2016-12-16 21:24:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)2016-12-16 21:24:05 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:60232016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (404) &lt;GET http://quotes.toscrape.com/robots.txt&gt; (referer: None)2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://quotes.toscrape.com/page/1/&gt; (referer: None)2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://quotes.toscrape.com/page/2/&gt; (referer: None)2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-1.html2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-2.html2016-12-16 21:24:05 [scrapy.core.engine] INFO: Closing spider (finished) Now, check the files in the current directory. You should notice that two new files have been created: quotes-1.html and quotes-2.html, with the content for the respective URLs, as our parse method instructs. What just happened under the hood?Scrapy schedules the scrapy.Request objects returned by the start_requests method of the Spider. Upon receiving a response for each one, it instantiates Response objects and calls the callback method associated with the request (in this case, the parse method) passing the response as argument.(通过the start_requests method of the Spider将request的请求发出去,得到response,然后对于每个response调用callback) A shortcut to the start_requests methodInstead of implementing(执行) a start_requests() method that generates scrapy.Request objects from URLs, you can just define a start_urls class attribute with a list of URLs. This list will then be used by the default implementation of start_requests() to create the initial requests for your spider:(这个start_urls会自动调用默认的方法去生成初始的requests)123456789101112131415import scrapyclass QuotesSpider(scrapy.Spider): name = &quot;quotes&quot; start_urls = [ &apos;http://quotes.toscrape.com/page/1/&apos;, &apos;http://quotes.toscrape.com/page/2/&apos;, ] def parse(self, response): page = response.url.split(&quot;/&quot;)[-2] filename = &apos;quotes-%s.html&apos; % page with open(filename, &apos;wb&apos;) as f: f.write(response.body) The parse() method will be called to handle each of the requests for those URLs, even though we haven’t explicitly told Scrapy to do so. This happens because parse() is Scrapy’s default callback method, which is called for requests without an explicitly assigned callback.(默认会处理response,无需明显调用) Extracting dataThe best way to learn how to extract data with Scrapy is trying selectors using the shell (Scrapy shell). Run:scrapy shell &#39;http://quotes.toscrape.com/page/1/&#39;`You will see something like:123456789101112131415[ ... Scrapy log here ... ]2016-09-19 12:09:27 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://quotes.toscrape.com/page/1/&gt; (referer: None)[s] Available Scrapy objects:[s] scrapy scrapy module (contains scrapy.Request, scrapy.Selector, etc)[s] crawler &lt;scrapy.crawler.Crawler object at 0x7fa91d888c90&gt;[s] item &#123;&#125;[s] request &lt;GET http://quotes.toscrape.com/page/1/&gt;[s] response &lt;200 http://quotes.toscrape.com/page/1/&gt;[s] settings &lt;scrapy.settings.Settings object at 0x7fa91d888c10&gt;[s] spider &lt;DefaultSpider &apos;default&apos; at 0x7fa91c8af990&gt;[s] Useful shortcuts:[s] shelp() Shell help (print this help)[s] fetch(req_or_url) Fetch request (or URL) and update local objects[s] view(response) View response in a browser&gt;&gt;&gt; Using the shell, you can try selecting elements using CSS with the response object:12&gt;&gt;&gt; response.css(&apos;title&apos;)[&lt;Selector xpath=&apos;descendant-or-self::title&apos; data=&apos;&lt;title&gt;Quotes to Scrape&lt;/title&gt;&apos;&gt;] (弱弱的说一句,有点像选择器啊)The result of running response.css(&#39;title&#39;) is a list-like object called SelectorList, which represents a list of Selector objects that wrap around XML/HTML elements and allow you to run further queries to fine-grain the selection or extract the data.(果然)To extract the text from the title above, you can do:12&gt;&gt;&gt; response.css(&apos;title::text&apos;).extract()[&apos;Quotes to Scrape&apos;] There are two things to note here:one is that we’ve added ::text to the CSS query, to mean we want to select only the text elements directly inside element. If we don’t specify ::text, we’d get the full title element, including its tags:12&gt;&gt;&gt; response.css(&apos;title&apos;).extract()[&apos;&lt;title&gt;Quotes to Scrape&lt;/title&gt;&apos;] The other thing is that the result of calling .extract() is a list, because we’re dealing with an instance of SelectorList. When you know you just want the first result, as in this case, you can do:12&gt;&gt;&gt; response.css(&apos;title::text&apos;).extract_first()&apos;Quotes to Scrape&apos; As an alternative, you could’ve written:12&gt;&gt;&gt; response.css(&apos;title::text&apos;)[0].extract()&apos;Quotes to Scrape&apos; However, using .extract_first() avoids an IndexError and returns None when it doesn’t find any element matching the selection. There’s a lesson here: for most scraping code, you want it to be resilient to errors due to things not being found on a page, so that even if some parts fail to be scraped, you can at least get some data.(即使你没有得到你想要的东西,你也可以得到一些data) Besides the extract() and extract_first() methods, you can also use the re() method to extract using regular expressions:123456&gt;&gt;&gt; response.css(&apos;title::text&apos;).re(r&apos;Quotes.*&apos;)[&apos;Quotes to Scrape&apos;]&gt;&gt;&gt; response.css(&apos;title::text&apos;).re(r&apos;Q\w+&apos;)[&apos;Quotes&apos;]&gt;&gt;&gt; response.css(&apos;title::text&apos;).re(r&apos;(\w+) to (\w+)&apos;)[&apos;Quotes&apos;, &apos;Scrape&apos;] In order to find the proper CSS selectors to use, you might find useful opening the response page from the shell in your web browser using view(response). You can use your browser developer tools or extensions like Firebug (see sections about Using Firebug for scraping and Using Firefox for scraping). Selector Gadget is also a nice tool to quickly find CSS selector for visually selected elements, which works in many browsers. 不止可以使用css选择器,还可以使用xpath1234&gt;&gt;&gt; response.xpath(&apos;//title&apos;)[&lt;Selector xpath=&apos;//title&apos; data=&apos;&lt;title&gt;Quotes to Scrape&lt;/title&gt;&apos;&gt;]&gt;&gt;&gt; response.xpath(&apos;//title/text()&apos;).extract_first()&apos;Quotes to Scrape&apos; XPath expressions are very powerful, and are the foundation of Scrapy Selectors. In fact, CSS selectors are converted to XPath under-the-hood. You can see that if you read closely the text representation of the selector objects in the shell.(没想到啊,竟然是这样) we encourage you to learn XPath even if you already know how to construct CSS selectors, it will make scraping much easier.]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy Pass 1]]></title>
    <url>%2F2018%2F08%2F16%2Fscrapy1%2F</url>
    <content type="text"><![CDATA[Learn Scrapy pass 1Walk-through of an example spider1234567891011121314151617import scrapyclass QuotesSpider(scrapy.Spider): name = &quot;quotes&quot; start_urls = [ &apos;http://quotes.toscrape.com/tag/humor/&apos;, ] def parse(self, response): for quote in response.css(&apos;div.quote&apos;): yield &#123; &apos;text&apos;: quote.css(&apos;span.text::text&apos;).extract_first(), &apos;author&apos;: quote.xpath(&apos;span/small/text()&apos;).extract_first(), &#125; next_page = response.css(&apos;li.next a::attr(&quot;href&quot;)&apos;).extract_first() if next_page is not None: yield response.follow(next_page, self.parse) Put this in a text file, name it to something like quotes_spider.py and run the spider using the runspider command:1scrapy runspider quotes_spider.py -o quotes.json When this finishes you will have in the quotes.json file a list of the quotes in JSON format, containing text and author, looking like this (reformatted here for better readability):12345678910111213[&#123; &quot;author&quot;: &quot;Jane Austen&quot;, &quot;text&quot;: &quot;\u201cThe person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.\u201d&quot;&#125;,&#123; &quot;author&quot;: &quot;Groucho Marx&quot;, &quot;text&quot;: &quot;\u201cOutside of a dog, a book is man&apos;s best friend. Inside of a dog it&apos;s too dark to read.\u201d&quot;&#125;,&#123; &quot;author&quot;: &quot;Steve Martin&quot;, &quot;text&quot;: &quot;\u201cA day without sunshine is like, you know, night.\u201d&quot;&#125;,...] When you ran the command scrapy runspider quotes_spider.py, Scrapy looked for a Spider definition inside it and ran it through its crawler engine.(在爬取引擎中运行爬虫) The crawl started by making requests to the URLs defined in the start_urls attribute (in this case, only the URL for quotes in humor category)(发送请求给start_urls) and called the default callback method parse, passing the response object as an argument.(回调函数处理response) In the parse callback, we loop through the quote elements using a CSS Selector (通过使用css选择器), yield a Python dict with the extracted quote text and author(使用python字典去提取text文本与作者) , look for a link to the next page and schedule another request using the same parse method as callback.(找到下一个page,调用相同的parse) Here you notice one of the main advantages about Scrapy: requests are scheduled and processed asynchronously. This means that Scrapy doesn’t need to wait for a request to be finished and processed, it can send another request or do other things in the meantime. This also means that other requests can keep going even if some request fails or an error happens while handling it.(处理异步,请求储存在调度器里面,不知道是不是队列形式) While this enables you to do very fast crawls (sending multiple concurrent requests at the same time, in a fault-tolerant way) Scrapy also gives you control over the politeness of the crawl through a few settings. You can do things like setting a download delay between each request, limiting amount of concurrent requests per domain or per IP, and even using an auto-throttling extension that tries to figure out these automatically.(可控制性:可以控制scrapy每个域名,ip访问的数量等) This is using feed exports to generate the JSON file, you can easily change the export format (XML or CSV, for example) or the storage backend (FTP or Amazon S3, for example). You can also write an item pipeline to store the items in a database.(输出形式可以多样) 还有等等等等的特性,先不了解(看了也不懂),之后学了再看看]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bumblebee控制独显]]></title>
    <url>%2F2018%2F08%2F15%2FBumblebee%E6%8E%A7%E5%88%B6%E7%8B%AC%E6%98%BE%2F</url>
    <content type="text"><![CDATA[Bumblebee使用控制独显安装:bumblebee - 提供守护进程以及程序的主要安装包。mesa - 开源的 OpenGL 标准实现。对于合适的NVIDIA驱动。xf86-video-intel - Intel 驱动（可选）。对于32位程序 (必须启用Multilib）在64位机器上的支持，安装: lib32-virtualgl - 为32位应用提供的渲染/显示桥。lib32-nvidia-utils 或者 lib32-nvidia-340xx-utils（和64位对应）。要使用Bumblebee，请确保添加你的用户到 bumblebee 组：gpasswd -a user bumblebee并启用(enable) bumblebeed.service。之后重启系统 可以明显的感受到风扇转速下降 重启之后测试安装 mesa-demos并使用 glxgears 测试 Bumblebee 是否工作：optirun glxgears -info看到有图形出现,并且风扇开始转动,独显开始工作,成功!!]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-08-15 plane war日志]]></title>
    <url>%2F2018%2F08%2F15%2F2018-08-15%2F</url>
    <content type="text"><![CDATA[小游戏制作完成算是一个类似躲避球的游戏吧..花了两三天时间(大部分在上个学期完成了这次算是重新构建了一下,变了一下形式总的来说还可以接受吧]]></content>
      <categories>
        <category>开发日志</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-08-14 plane war日志]]></title>
    <url>%2F2018%2F08%2F14%2F2018-08-14%2F</url>
    <content type="text"><![CDATA[基本构建完成判断碰撞的逻辑没有作用,需要改进]]></content>
      <categories>
        <category>开发日志</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java_swing 绘制图形]]></title>
    <url>%2F2018%2F08%2F12%2Fjava-swing%2F</url>
    <content type="text"><![CDATA[在java的jpanel绘制图形使用paint方法,在里面使用super.paint清除之前的残留在while里使用repaint进行循环绘画12345678public void display() &#123; this.repaint(); &#125; public void paint(Graphics g) &#123; super.paint(g); dao.drawBackground(g); dao.drawState(g, hero); &#125; 123456789101112while(true) &#123; start.display(); try &#123; Thread.sleep(300); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux下/srv、/var和/tmp的职责区分]]></title>
    <url>%2F2018%2F08%2F12%2F%E5%88%86%E5%8C%BA%E7%9B%AE%E5%BD%95%2F</url>
    <content type="text"><![CDATA[(记录备忘)关于linux下/srv、/var和/tmp的职责区分 (转载自这儿)/srv ：主要用来存储本机或本服务器提供的服务或数据。（用户主动生产的数据、对外提供服务） /srv contains site-specific data which is served by this system. /var ：系统产生的不可自动销毁的缓存文件、日志记录。（系统和程序运行后产生的数据、不对外提供服务、只能用户手动清理）（包括mail、数据库文件、日志文件） /var contains variable data files. This includes spool directories and files, administrative and logging data, and transient and temporary files.Some portions of /var are not shareable between different systems. For instance, /var/log, /var/lock, and /var/run. Other portions may be shared, notably /var/mail, /var/cache/man, /var/cache/fonts, and /var/spool/news./var is specified here in order to make it possible to mount /usr read-only. Everything that once went into /usr that is written to during system operation (as opposed to installation and software maintenance) must be in /var.If /var cannot be made a separate partition, it is often preferable to move /var out of the root partition and into the /usr partition. (This is sometimes done to reduce the size of the root partition or when space runs low in the root partition.) However, /var must not be linked to /usr because this makes separation of /usr and /var more difficult and is likely to create a naming conflict. Instead, link /var to /usr/var.Applications must generally not add directories to the top level of /var. Such directories should only be added if they have some system-wide implication, and in consultation with the FHS mailing list. /tmp ：保存在使用完毕后可随时销毁的缓存文件。（有可能是由系统或程序产生、也有可能是用户主动放入的临时数据、系统会自动清理） The /tmp directory must be made available for programs that require temporary files.Programs must not assume that any files or directories in /tmp are preserved between invocations of the program. 所以，服务器被用作Web开发时，html文件更应该被放在/srv/www下，而不是/var/www下（因为/srv目录是新标准中才有的，出现较晚；而且Apache将/var/www设为了web默认目录，所以现在绝大多数人都把web文件放在/var/www，这是个历史遗留问题）。 如ftp、流媒体服务等也应该被放在/srv对应的目录下。如果对应目录太大，应该另外挂载分区。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-08-12 java plane war记录日志]]></title>
    <url>%2F2018%2F08%2F12%2F2018-08-12%2F</url>
    <content type="text"><![CDATA[之前旧的游戏没有完成,里面用的是timer.schdule本次重新写打算用thread现在还没有想好使用什么方法写子弹和敌人(是不是上一次一样使用list)linux下似乎swing的绘制有点卡,不知道是什么问题…. 还有就是遇到了linux下workbanch闪退的问题解决方法是: rm -rf .mysql/workbench/]]></content>
      <categories>
        <category>开发日志</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[备份arch系统]]></title>
    <url>%2F2018%2F08%2F10%2F%E5%A4%87%E4%BB%BDarch%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[使用rsync进行备份rsync的六种模式123456rsync [OPTION]... SRC DESTrsync [OPTION]... SRC [USER@]host:DESTrsync [OPTION]... [USER@]HOST:SRC DESTrsync [OPTION]... [USER@]HOST::SRC DESTrsync [OPTION]... SRC [USER@]HOST::DESTrsync [OPTION]... rsync://[USER@]HOST[:PORT]/SRC [DEST] 本地备份的话我们使用第一种 大佬使用的备份脚本,通过指定/的目录来进行备份123456789101112131415161718192021222324252627#!/bin/zsh -e#进入主目录cd $(dirname $0)#判断参数个数时候小于2或者大于3(也就是可以是2个参数或者是3个参数)if [[ $# -lt 2 || $# -gt 3 ]]; then echo &quot;usage: $0 SRC_DIR DEST_DIR [-w]&quot; exit 1fi#得到参数src=$1dest=$2doit=$3if [[ $doit == -w ]]; then dry=else dry=&apos;-n&apos;firsync --archive --one-file-system --inplace --hard-links \ --human-readable --numeric-ids --delete --delete-excluded \ --acls --xattrs --sparse \ --itemize-changes --verbose --progress \ --exclude=&apos;*~&apos; --exclude=__pycache__ \ --exclude-from=root.exclude \ $src $dest $dry 以下列出rsync参数,使用到的将使用加粗列出 -v, –verbose 详细模式输出。-i, –itemize-changes 列出改变(不知道是不是这个意思)-q, –quiet 精简输出模式。-c, –checksum 打开校验开关，强制对文件传输进行校验。-a, –archive 归档模式，表示以递归方式传输文件，并保持所有文件属性，等于-rlptgoD。-A –acls This option causes rsync to update the destination ACLs to be the same as the source ACLs. The option also implies –perms.-X –xattrs This option causes rsync to update the destination extended attributes to be the same as the source ones.(上面两个选项保留文件 ACL 和扩展属性)-r, –recursive 对子目录以递归模式处理。-R, –relative 使用相对路径信息。-b, –backup 创建备份，也就是对于目的已经存在有同样的文件名时，将老的文件重新命名为~filename。可以使用–suffix选项来指定不同的备份文件前缀。–backup-dir 将备份文件(如~filename)存放在在目录下。-suffix=SUFFIX 定义备份文件前缀。-u, –update 仅仅进行更新，也就是跳过所有已经存在于DST，并且文件时间晚于要备份的文件，不覆盖更新的文件。–inplace 更新目标文件,RSYNC将写入的更新数据直接写入目标文件。-l, –links 保留软链结。-L, –copy-links 想对待常规文件一样处理软链结。–copy-unsafe-links 仅仅拷贝指向SRC路径目录树以外的链结。–safe-links 忽略指向SRC路径目录树以外的链结。-H, –hard-links 保留硬链结。-p, –perms 保持文件权限。就是 –acls-o, –owner 保持文件属主信息。-g, –group 保持文件属组信息。-D, –devices 保持设备文件信息。-t, –times 保持文件时间信息。-S, –sparse 对稀疏文件进行特殊处理以节省DST的空间。-n, –dry-run 进行try的尝试,不做改变。-w, –whole-file 拷贝文件，不进行增量检测。-x, –one-file-system 不要跨越文件系统边界。-B, –block-size=SIZE 检验算法使用的块尺寸，默认是700字节。-e, –rsh=command 指定使用rsh、ssh方式进行数据同步。–rsync-path=PATH 指定远程服务器上的rsync命令所在路径信息。-C, –cvs-exclude 使用和CVS一样的方法自动忽略文件，用来排除那些不希望传输的文件。–existing 仅仅更新那些已经存在于DST的文件，而不备份那些新创建的文件。–delete 删除那些DST中SRC没有的文件。–delete-excluded 同样删除接收端那些被该选项指定排除的文件。–delete-after 传输结束以后再删除。–ignore-errors 及时出现IO错误也进行删除。–max-delete=NUM 最多删除NUM个文件。–partial 保留那些因故没有完全传输的文件，以是加快随后的再次传输。–force 强制删除目录，即使不为空。–numeric-ids 将数字的用户和组id匹配为用户名和组名。–timeout=time ip超时时间，单位为秒。-I, –ignore-times 不跳过那些有同样的时间和长度的文件。–size-only 当决定是否要备份文件时，仅仅察看文件大小而不考虑文件时间。–modify-window=NUM 决定文件是否时间相同时使用的时间戳窗口，默认为0。-T –temp-dir=DIR 在DIR中创建临时文件。–compare-dest=DIR 同样比较DIR中的文件来决定是否需要备份。-P 等同于 –partial。–progress 显示备份过程。-z, –compress 对备份的文件在传输时进行压缩处理。–include=PATTERN 指定不排除而需要传输的文件模式。–exclude-from=FILE 排除FILE中指定模式的文件。–include-from=FILE 不排除FILE指定模式匹配的文件。–version 打印版本信息。–address 绑定到特定的地址。–config=FILE 指定其他的配置文件，不使用默认的rsyncd.conf文件。–port=PORT 指定其他的rsync服务端口。–blocking-io 对远程shell使用阻塞IO。-stats 给出某些文件的传输状态。–progress 在传输时现实传输过程。–log-format=formAT 指定日志文件格式。–password-file=FILE 从FILE中得到密码。–bwlimit=KBPS 限制I/O带宽，KBytes per second。-h, –help 显示帮助信息。–human-readable 以更可读的格式输出数字。 看了好多教程rsync+btrfs单纯使用rsync..发现还是单纯的使用rsync比较省事 排除的目录12345/media/*/sys/*/proc/*/mnt/*/tmp/* 但 @依云仙子 所说 --one-file-system 已经排除了特殊的目录 目前就先这样备份了123456sudo rsync --archive --one-file-system --hard-links \ --acls --xattrs --sparse \ --human-readable --numeric-ids --delete --delete-excluded \ --itemize-changes --verbose --progress \ --exclude-from=backup.exclude \ / /run/media/s/5ff9a14c-20ba-41c5-8b7e-f6b095326200/Backup/backup (讲道理应该可以) 再把代码备份到github上就差不多了(溜了)]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-08-10 arch备份日志]]></title>
    <url>%2F2018%2F08%2F10%2F2018-08-10%2F</url>
    <content type="text"><![CDATA[看了好多教程,还是不懂与其这样不如使用最简单的备份操作进行备份(借口,就是懒)还是得学习啊,感觉神码也不懂..]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-08-09 arch备份日志]]></title>
    <url>%2F2018%2F08%2F09%2F2018-08-09-1%2F</url>
    <content type="text"><![CDATA[备份准备工作已开始分区已经完成,也已经加密还需要详细了解rsync,增量式备份由于属于本地备份,或许未能完全了解rsync个人想法 网上全都是简单的参数缩写,还是得查man page,记倒不是问题ps:十分害怕硬盘gg,听说希捷的容易坏0-0]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-08-09 lfs编译日志]]></title>
    <url>%2F2018%2F08%2F09%2F2018-08-09%2F</url>
    <content type="text"><![CDATA[已经完成lfs最小的构建偷偷的懒散的躺了好十几天..愧疚(逃开始下一个任务==&gt;备份系统]]></content>
      <categories>
        <category>lfs编译日志</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux For Scratch Pass 6]]></title>
    <url>%2F2018%2F08%2F09%2Flfs6%2F</url>
    <content type="text"><![CDATA[让 LFS 系统可引导创建 /etc/fstab 文件/etc/fstab 文件的作用是让其它程序确定存储设备的默认挂载点、挂载参数和检查信息（例如完整性检测）。12345678910111213141516cat &gt; /etc/fstab &lt;&lt; &quot;EOF&quot;# Begin /etc/fstab# file system mount-point type options dump fsck# order/dev/&lt;xxx&gt; / &lt;fff&gt; defaults 1 1/dev/&lt;yyy&gt; swap swap pri=1 0 0proc /proc proc nosuid,noexec,nodev 0 0sysfs /sys sysfs nosuid,noexec,nodev 0 0devpts /dev/pts devpts gid=5,mode=620 0 0tmpfs /run tmpfs defaults 0 0devtmpfs /dev devtmpfs mode=0755,nosuid 0 0# End /etc/fstabEOF 其中，， 和 请使用适当的值替换。例如 sda2，sda5 和 ext4。关于文件中六个字段的含义，请查看 man 5 fstab（译者注：fsck 列的数值来决定需要检查的文件系统的检查顺序。允许的数字是0, 1, 和2。根目录应当获得最高的优先权 1, 其它所有需要被检查的设备设置为 2。0 表示设备不会被 fsck 所检查）。 基于 MS-DOS 或者是来源于 Windows 的文件系统（例如：vfat，ntfs，smbfs，cifs，iso9660，udf）需要在挂载选项中添加「iocharset」，才能让非 ASCII 字符的文件名正确解析。此选项的值应该与语言区域设置的值相同，以便让内核能正确处理。此选项在相关字符集定义已为内核内建或是编译为模块时生效（在文件系统 -&gt; 本地语言支持中查看）。此外，vfat 和 smbfs 还需启用「codepage」支持。例如，想要挂载 USB 闪存设备，zh-CN.GBK 用户需要在 /etc/fstab 中添加以下的挂载选项：noauto,user,quiet,showexec,iocharset=koi8r,codepage=866对于 zh_CN.UTF-8 用户的对应选项是：noauto,user,quiet,showexec,iocharset=utf8,codepage=936主要是设置挂载参数本机12345678910$ cat /etc/fstab# Static information about the filesystems.# See fstab(5) for details.# &lt;file system&gt; &lt;dir&gt; &lt;type&gt; &lt;options&gt; &lt;dump&gt; &lt;pass&gt;# /dev/sda5UUID=c36eedfb-08b3-4e28-8483-03f6d5f1ad0c / ext4 rw,relatime,data=ordered 0 1# /dev/sda2UUID=608A-F457 /boot vfat rw,relatime,fmask=0022,dmask=0022,codepage=437,iocharset=iso8859-1,shortname=mixed,utf8,errors=remount-ro 0 0 之后就是1.编译内核2.安装grub引导(注意备份)3.重新启动 之后安装其他的小程序 sudo,dhcp,wget等 结束]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-07-25 lfs编译日志]]></title>
    <url>%2F2018%2F07%2F25%2F2018-07-25%2F</url>
    <content type="text"><![CDATA[lfs编译告一段落系统大半构建完成]]></content>
      <categories>
        <category>lfs编译日志</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux For Scratch Pass 5]]></title>
    <url>%2F2018%2F07%2F25%2Flfs5%2F</url>
    <content type="text"><![CDATA[System V is the classic boot process that has been used in Unix and Unix-like systems such as Linux since about 1983.It consists of a small program, init, that sets up basic programs such as login (via getty) and runs a script. This script, usually named rc, controls the execution of a set of additional scripts that perform the tasks required to initialize the system. The init program is controlled by the /etc/inittab file and is organized into run levels that can be run by the user: 123456780 — halt1 — Single user mode(单用户模式)2 — Multiuser, without networking3 — Full multiuser mode4 — User definable5 — Full multiuser mode with display manager6 — rebootThe usual default run level is 3 or 5. 安装LFS-Bootscripts该软件包里面有一系列系统启动和关机的脚本 概述传统的 Linux 不管硬件是否真实存在，都以创建静态设备的方法来处理硬件，因此需要在 /dev 目录下创建大量的设备节点文件(有时会有上千个)。这通常由 MAKEDEV 脚本完成，它通过大量调用 mknod 程序 为这个世界上可能存在的每一个设备建立对应的主设备号和次设备号。 而使用 udev方法，只有当内核检测到硬件接入，才 会建立对应的节点文件。因为需要在系统启动的时候重新建立设备节点文件(动态建立文件)，所以将它存储在 devtmpfs 文件系统中（完全存在于内存中的虚拟文件系统）。设备节点文件无需太多的空间，所以占用的内存也很小。(动态建立) 历史2000 年 2 月，一种名叫 devfs 的文件系统被合并到 2.3.46 内核版本，2.4 系列的稳定内核中基本可用。尽管它存在于内核的源代码中，但是这种动态创建设备的方法却从来都得不到核心内核开发者的大力支持。 问题存在于它处理设备的检测、创建和命令的方式，其中最大的问题莫过于它对设备节点的命名方式。大部分开发者的观点是，设备的命名应该由系统的所有者决定，而不是开发者。 devfs 存在一个严重的设计缺陷：它存在严重的 race conditions 问题(有两个方面的问题,两个不相干的进程争取一个相同资源,两个相干的进程竞争同一资源,相互等待,命名的方式会使得目标不明确)，如不对内核做大量的修改就无法修正这一问题。最终，因为缺乏有效的维护，在 2006 年 6 月终被移出内核源代码。再后来，随着非稳定的 2.5 版本的内核开发，到稳定的 2.6 内核，又出现了一种全新的虚拟文件系统 sysfs。sysfs 的工作是将系统的硬件配置导入到用户空间进程。通过对用户空间可视化的改善，代替devfs变得更加现实。 Sysfs上文简单的提及了 sysfs 文件系统。有些人可能会问，sysfs 到底是如何知道当前系统有哪些设备，这些设备又该使用什么设备号呢。对于那些已经编译进内核的设备，会在内核检测到时直接注册为 sysfs 对象（devtmpfs 内建）(通过保存在内存的设备系统,来注册设备)。对于编译为内核模块的设备，将会在模块载入的时候注册。一旦 sysfs 文件系统挂载到 /sys，已经在 sysfs 注册的硬件数据就可以被用户空间的进程使用， udevd也就能够处理了（包括对设备节点进行修改）。(总的来说,是先通过devtmpfs来注册电脑上有的设备,动态注册设备,之后一旦 sysfs 文件系统挂载到 /sys,就将这些注册的设备给用户进程使用) 设备节点的创建设备文件是通过内核中的 devtmpfs 文件系统创建的。任何想要注册的设备都需要通过 devtmpfs （通过驱动程序核心）实现。每当一个devtmpfs实例挂载到 /dev，就 会建立一个设备节点文件，它拥有固定 的名称、权限、所有者。很短的时间之后，内核将给 udevd 一个 uevent。基于 /etc/udev/rules.d，/lib/udev/rules.d 和 /run/udev/rules.d目录内文件指定的规则，udevd 将会建立到设备节点文件的额外符号链接，这有可能更改其权限，所有者，所在组，或者是更改 udevd 内建接口（名称）。(创建符号链接的时候可能会出现问题)这三个目录下的规则都像 LFS-Bootscripts 包那样标有数字，所有三个目录都会统一到一起。如果 udevd 找不到 和所创建设备相应的规则，它会保留 devtmpfs 里初始化时使用过的权限和属主。 加载模块编译成模块的设备驱动可能会包含别名。别名可以通过modinfo打印出来查看，一般是模块支持的特定总线的设备描述符。举个例子，驱动snd-fm801支持厂商 ID 为 0x1319 以及设备 ID 为 0x0801 的设备，它包含一个 “pci:v00001319d00000801sv*sd*bc04sc01i*” (vesion,device,sv*sd*bc04sc01i*)的别名，总线驱动导出该驱动别名并通过 sysfs 处理相关设备。例如，文件 /sys/bus/pci/devices/0000:00:0d.0/modalias 应该会包含字符串 “pci:v00001319d00000801sv00001319sd00001319bc04sc01i00”。Udev 采用的默认规则会让 udevd 根据 uevent 环境变量 MODALIAS 的内容（它应该和 sysfs 里的 modalias 文件内容一样）调用 /sbin/modprobe，这样就可以加载在通配符扩展后能和这个字符串一致的所有模块。(modprobe命令 用于智能地向内核中加载模块或者从内核中移除模块。)在这个例子里，意味着，除了 snd-fm801 之外，一个已经废弃的（不是我们所希望的）驱动 forte 如果存在的话也会被加载。下面有几种可以避免加载多余驱动的方式。 内核本身也能够根据需要加载网络协议，文件系统以及 NLS 支持模块。 处理热插拔/动态设备在你插入一个设备时，例如一个通用串行总线（USB）MP3 播放器，内核检测到设备已连接就会生成一个uevent。这个 uevent 随后会被上面所说的udevd处理。 加载模块和创建设备时可能碰到的问题在自动创建设备节点时可能会碰到一些问题。 内核模块没有自动加载Udev 只会加载包含有特定总线别名而且已经被总线驱动导出到 sysfs 下的模块。在其它情况下，你应该考虑用其它方式加载模块。采用 Linux-4.15.3，Udev 可以加载编写合适的 INPUT、IDE、PCI、USB、SCSI、SERIO 和 FireWire设备驱动。 要确定你希望加载的驱动是否支持 Udev，可以用模块名字作为参数运行 modinfo。然后查看/sys/bus下的设备目录里是否有个modalias文件。 如果在 sysfs 下能找到modalias文件，那么就能驱动这个设备并可以直接操作它，但是如果该文件里没有包含设备别名，那意味着这个驱动有问题。我们可以先尝试不依靠 Udev直接加载驱动，等这个问题以后解决。 如果在 /sys/bus 下的相应目录里没有 modalias 的话，意味着内核开发人员还没有为这个总线类型增加 modalias 支持。使用 Linux-4.15.3 内核，应该是 ISA 总线的问题。希望这个可以在后面的内核版本里得到解决。 Udev 不会尝试加载类似 snd-pcm-oss 这样的“封装”驱动，也不会加载类似 loop 这样的非硬件驱动。(loop是指循环?) 内核驱动没有自动加载，Udev 也没有尝试加载如果是 “封装” 模块只是强化其它模块的功能（比如，snd-pcm-oss 模块通过允许 OSS 应用直接访问声卡的方式加强了 snd-pcm 模块的功能），需要配置 modprobe 在 Udev 加载硬件驱动模块后再加载相应的封装模块。可以在任意 /etc/modprobe.d/&lt;filename&gt;.conf文件里增加一行 “softdep”，例如：softdep snd-pcm post: snd-pcm-oss(设置加载模块方式)请注意 “softdep” 也支持 pre: 的依赖方式，或者混合 pre: 和 post:。查看 modprobe.d(5) 手册了解更多关于 “softdep” 语法和功能的信息。 如果问题模块不是一个封装而且也是有用的，配置 modules开机脚本在引导系统的时候加载模块。这样需要把模块名字添加到 /etc/sysconfig/modules 文件里的单独一行。这个也可以用于封装模块，但是只是备用方式。 Udev 加载了一些无用模块要么不要编译该模块，或者把它加入到模块黑名单 /etc/modprobe.d/blacklist.conf 里，像下面的例子里屏蔽了 forte 模块：blacklist forte 被屏蔽的模块仍然可以用 modprobe 强行加载。 Udev 创建了错误的设备节点，或错误的软链接这个情况通常是因为设备匹配错误。例如，一条写的不好的规则可能同时匹配到 SCSI 磁盘（希望加载的）和对应厂商的 SCSI 通用设备（错误的）。找出这条问题规则，并通过 udevadm info 命令的帮助改得更具体一些。(一条规则对应到两个设备) Udev 规则工作不可靠这可能是上个问题的另一种表现形式。如果不是，而且你的规则使用了 sysfs 特性，那可能是内核时序问题，希望在后面版本内核里能解决。目前的话，你可以暂时建立一条规则等待使用的 sysfs 特性，并附加到/etc/udev/rules.d/10-wait_for_sysfs.rules 文件里（如果没有这个文件就创建一个）。如果你碰到这种情形请通知 LFS 开发邮件列表，这个对我们有帮助。 Udev 没有创建设备后面的内容会假设驱动已经静态编译进内核或已经作为模块加载，而且你也已经确认 Udev 没有创建相应的设备节点。如果内核驱动没有将自己的数据导出到 sysfs 里，Udev 就没有相关信息来创建设备节点。这通常发生在内核树之外的第三方驱动里。我们可以使用合适的主/副设备数字 ID（查看内核文档里或第三方驱动厂商提供的文档里的 devices.txt 文件） 在 /lib/udev/devices 目录里创建一个静态设备节点。这个静态设备节点随后会被 udev 引导脚本复制到 /dev 里。 设备名称顺序在重启后随机改变这是因为 Udev被设计成并行处理 uevents 并加载模块，所以是不可预期的顺序。这个不会“修复”。你不应该依赖稳定的内核模块名称。而是，在检测到设备的稳定特征，比如序列号或 Udev 安装的一些*_id应用的输出，来判断设备的稳定名称，之后 创建自己的规则生成相应的软链接。 管理设备网络设备Udev, by default, names network devices according to Firmware/BIOS data or physical characteristics like the bus, slot, or MAC address. (通过固件/bios或者说是设备自带的属性进行命名)这种方式使得命名一致,而不是基于发现网卡的时间来确定(老方法) 新的方法一般会变成enp5s0 or wlp3s0这样的形式你也可以禁用新的方法使用老的 创建传统的udev规则根据现有的初始化规则来创建bash /lib/udev/init-net-rules.shcat /etc/udev/rules.d/70-persistent-net.rules 具有相同功能的设备出现在 /dev 目录下的顺序是随机的。假如你有一个 USB 摄像头和一个电视调谐器，/dev/video0 有可能是 USB 摄像头，/dev/video1 是电视调谐器，有时候又可能是反过来的。对于除声卡和网卡外的设备，都可以通过创建自定义持久性符号链接的 udev 规则来固定。 处理类似的设备对于你所有的硬件，都有可能遇到此问题（尽管此问题可能在你当前的 Linux 发行版上不存在），在 /sys/class 或 /sys/block 目录下找到对应目录，比如，显卡可能的路径为 /sys/class/video4linux/videoX。找到该设备的唯一设备标识（通常，厂商和产品 ID 以及/或 序列号会有用）：udevadm info -a -p /sys/class/video4linux/video0然后通过写入规则建立符号链接：12345678cat &gt; /etc/udev/rules.d/83-duplicate_devs.rules &lt;&lt; &quot;EOF&quot;# Persistent symlinks for webcam and tunerKERNEL==&quot;video*&quot;, ATTRS&#123;idProduct&#125;==&quot;1910&quot;, ATTRS&#123;idVendor&#125;==&quot;0d81&quot;, \ SYMLINK+=&quot;webcam&quot;KERNEL==&quot;video*&quot;, ATTRS&#123;device&#125;==&quot;0x036f&quot;, ATTRS&#123;vendor&#125;==&quot;0x109e&quot;, \ SYMLINK+=&quot;tvtuner&quot;EOF 最终，/dev/video0 和 /dev/video1 依旧会随机分配给 USB 摄像头和电视调谐器，但是 /dev/tvtuner 和 /dev/webcam 将会固定的分配给正确的设备。 通用网络设置网络脚本启动和关闭哪些接口通常取决于/etc/sysconfig/中的文件。此目录应包含要配置的每个接口的文件，例如ifconfig.xyz，其中“xyz”应描述网卡。使用ip link 或者 ls /sys/class/net 查看接口名称 通过配置文件配置接口(静态ip)具体按照自己情况来12345678910cd /etc/sysconfig/cat &gt; ifconfig.eth0 &lt;&lt; &quot;EOF&quot;ONBOOT=yes 是否在booting的时候调用nic(网卡全称 network interface card)IFACE=eth0 接口名称SERVICE=ipv4-static 获取IP地址的方法IP=192.168.1.2GATEWAY=192.168.1.1 默认网关IP地址PREFIX=24 网络掩码BROADCAST=192.168.1.255 广播地址EOF For more information see the ifup man page. DNS解析通过将ISP服务器或网络管理员提供的DNS服务器的IP地址放入/etc/resolv.conf来实现ip地址的解析。123456789cat &gt; /etc/resolv.conf &lt;&lt; &quot;EOF&quot;# Begin /etc/resolv.confdomain &lt;Your Domain Name&gt;nameserver &lt;IP address of your primary nameserver&gt;nameserver &lt;IP address of your secondary nameserver&gt;# End /etc/resolv.confEOF Google 公开 IPv4 DNS 解析服务器地址为 8.8.8.8 和 8.8.4.4。译者注：国内也有一些 IT 公司提供公开可用的 DNS 解析服务：114 DNS：114.114.114.114 和 114.114.115.115阿里 DNS：223.5.5.5 和 223.6.6.6百度 DNS：180.76.76.76OpenDNS：208.67.220.220） 自定义host文件(一般是自动从dns的服务器中得到缓存,也可以手动配置)就算没有网卡，也应该提供有效的完整域名，否则某些软件可能无法正常运行。1234Private Network Address Range Normal Prefix10.0.0.1 - 10.255.255.254 8172.x.0.1 - 172.x.255.254 16192.168.y.1 - 192.168.y.254 24 x can be any number in the range 16-31. y can be any number in the range 0-255. System V BootscriptSysVinit(init)默认处于3的级别12345670: halt the computer 关闭计算机1: single-user mode 单人模式2: multi-user mode without networking 多人单机3: multi-user mode with networking 多人联机4: reserved for customization, otherwise does the same as 35: same as 4, it is usually used for GUI login (like X&apos;s xdm or KDE&apos;s kdm)6: reboot the computer 重启 内核初始化的时候，无论是命令行指定运行的第一个程序，还是默认的 init。该程序会读入初始化文件 /etc/inittab初始化文件的解释可以参考 inittab 的 man 手册页面。对于LFS，运行的核心命令是 rc。上面的初始化文件将指示 rc 运行 /etc/rc.d/rcS.d 目录中，所有以 S 开头的脚本，然后便是 /etc/rc.d/rc?.d 目录中，所有以 S 开头的脚本。目录中的问号由指定的 initdefault 值来决定。 为了方便，rc 会从 /lib/lsb/init-functions 中读取函数库。该库还会读取一个可选的配置文件 /etc/sysconfig/rc.site。任何在后续章节中描述到的系统配置文件中的参数，都可以放在这个文件中，允许将所有的系统参数合并到该文件中。 为了调试方便，函数脚本会将日志输出到 /run/var/bootlog 文件中。由于 /run 目录是个 tmpfs（临时文件系统），所以该文件在启动之后就不会持续存在了，但在启动过程的最后，这些内容会被添附到更为持久的 /var/log/boot.log 文件中。 想要改变运行级，可以使用命令 init &lt;runlevel&gt;，其中的 &lt;runlevel&gt; 便是想要切换到的运行级。举个例子，若是想要重启计算机，可以使用命令 init 6，这是 reboot 命令的别名。就像，init 0 是 halt 的别名一样。 对于中文，日语，韩语以及一些其他的语言需要的字符，Linux 的控制台还无法通过配置是之正常显示。用户若要使用这些语言，需要安装 X Window 系统，用于扩充所需字符域的字体，以及合适的输入法. 可选的 /etc/sysconfig/rc.site 文件中包含着那些为每个 System V 启动脚本自动设置好的设定。这些设定也可以在 /etc/sysconfig/ 目录中的 hostname，console，和clock 文件中设置。如果这些设定值同时在以上的这些文件和 rc.site 中设定了，那么脚本中的设定值将会被优先采用 自定义启动和关闭的脚本 LFS 启动脚本会以相当效率的方式启动和关闭系统，不过你可以在 rc.site 文件中进行调整，来提升速度或是根据需求调整消息。若是有这样的需求，就去调整上面 /etc/sysconfig/rc.site 文件的设置吧！在启动脚本 udev 运行时，会调用一次 udev settle，完成检测需要很长时间。这段时间根据当前系统的设备，可花可不花。如果你需要的仅仅是简单的分区和单个网卡，在启动的过程中，就没有必要等待这个命令执行。通过设置变量 OMIT_UDEV_SETTLE=y，可以跳过此命令。启动脚本udev_retry 默认也执行udev settle。该命令仅在 /var 目录是分开挂载的情况下需要。因为这种情况下时钟需要文件 /var/lib/hwclock/adjtime。其他的自定义设置可能也需要等待 udev 执行完成，但是在许多的安装中不需要。设置变量 OMIT_UDEV_RETRY_SETTLE=y 跳过命令。默认情况下，文件系统检测静默执行。看上去就像是启动过程中的一个延迟。想要打开 fsck 的输出，设置变量。重起时，你可能想完全的跳过文件系统检测 fsck。为此，可以创建 /fastboot 文件或是以 /sbin/shutdown -f -r now 命令重启系统。另一方面，你也可以通过创建 /forcefsck，或是在运行 shutdown 命令时，用 -F 参数代替-f，以此来强制检测整个文件系统。设置变量 FASTBOOT=y 将禁用启动过程中的 fsck，直至你将其移除。不推荐长时间地使用该方式。通常，/tmp 目录中的所有文件会在启动时删除。根据存在目录与文件的数量，该操作会导致启动过程中明显的延迟。如果要跳过移除文件的操作，设置变量 SKIPTMPCLEAN=y。在关机的过程中，init 程序会向每一个已经启动的程序（例如，agetty）发送一个 TERM 信号，等一段时间（默认为 3 秒），然后给每个进程发送 KILL 信号。对没有被自身脚本关闭的进程，sendsignals 脚本会重复此过程。init 的延迟可以通过参数来设置。比方说，想去掉 init 的延迟，可以通过在关机或重启时使用 -t0 参数（如，/sbin/shutdown -t0 -r now）。sendsignals 脚本的延迟可以通过设置参数 KILLDELAY=0 跳过。 创建 /etc/inputrc 文件 可编辑控制行的语句创建 /etc/shells 文件 shell的类似索引的东西]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[观药神有感]]></title>
    <url>%2F2018%2F07%2F25%2Fxiaoshi2%2F</url>
    <content type="text"><![CDATA[从迷雾中来黑暗指引着我但即使我步入光明也仍是黑暗的子嗣我体会过胆怯也害怕孤独但为了亲人我竭尽全力前行逆行中我见过许多红的白的原以为这一切与我无关但我还是能感受到懊悔愤怒从我那丁点的左胸膛迸发我们摆脱不来穷病也脱离不了社会而我能做的只是用我的脊梁去分担那微不足道的分量]]></content>
      <categories>
        <category>诗歌集</category>
      </categories>
      <tags>
        <tag>小诗</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-07-18 lfs编译日志]]></title>
    <url>%2F2018%2F07%2F18%2F2018-07-18%2F</url>
    <content type="text"><![CDATA[主要在写md文档,有点无聊,写了几天只写了20个左右的工具编译打算不写了,主要还是看文档,之后是配置系统的过程]]></content>
      <categories>
        <category>lfs编译日志</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux For Scratch Pass 4]]></title>
    <url>%2F2018%2F07%2F18%2Flfs4%2F</url>
    <content type="text"><![CDATA[安装基本系统软件过程(重点过程)部分在中文版中没有,取自英文版 Linux-4.15.3 API HeadersLinux API 头文件（在 linux-3.19.tar.xz 里）会将内核 API 导出给 Glibc 使用。 Linux 内核需要提供一个应用编程接口（API）供系统的 C 库（LFS 中的 Glibc）调用。这通过整理 Linux 内核源码包中的多个 C 头文件来完成。 确保在之前的动作里没有留下旧文件和依赖关系：make mrproper现在要从源代码里解压出用户需要的内核头文件。因为解压过程会删除目标目录下所有文件，所以我们会先输出到一个本地中间目录后再拷贝到需要的地方。而且里面还有一些隐藏文件是给内核开发人员用的，而 LFS 不需要，所以会将它们从中间目录里删除。 123make INSTALL_HDR_PATH=dest headers_installfind dest/include \( -name .install -o -name ..install.cmd \) -deletecp -rv dest/include/* /usr/include Man-pages-4.15Man-pages 软件包里包含了超过 2,200 份 man 手册页面。 Install Man-pages by running:make install Glibc-2.21Glibc 软件包包含了主要的 C 函数库。这个库提供了分配内存、搜索目录、打开关闭文件、读写文件、操作字符串、模式匹配、基础算法等基本程序。注意有些 LFS 之外的软件包会建议安装 GNU libiconv 来转换不同编码的文字。根据项目主页（http://www.gnu.org/software/libiconv/ ）上的说法“这个库会提供函数 iconv() 的实现，应用于那些没有这个函数的系统，或者函数实现中不支持 Unicode 转换的系统。” Glibc 提供了函数 iconv() 的实现而且支持 Unicode 转换，所以对于 LFS 系统来说并不需要 libiconv 库。 有些 Glibc 程序会用到和 FHS 不兼容的 /var/db 目录来存储它们的运行时数据。打上如下的补丁让这些程序在 FHS 兼容的位置存储它们的运行时数据。 patch -Np1 -i ../glibc-2.21-fhs-1.patch 创建链接:ln -sfv /tools/lib/gcc /usr/lib 创建系统链接(create a symlink for LSB compliance):123456789case $(uname -m) in i?86) GCC_INCDIR=/usr/lib/gcc/$(uname -m)-pc-linux-gnu/7.3.0/include ln -sfv ld-linux.so.2 /lib/ld-lsb.so.3 ;; x86_64) GCC_INCDIR=/usr/lib/gcc/x86_64-pc-linux-gnu/7.3.0/include ln -sfv ../lib/ld-linux-x86-64.so.2 /lib64 ln -sfv ../lib/ld-linux-x86-64.so.2 /lib64/ld-lsb-x86-64.so.3 ;;esac 删除前面可能遗留的文件:rm -f /usr/include/limits.h 编译:1234567CC=&quot;gcc -isystem $GCC_INCDIR -isystem /usr/include&quot; \../configure --prefix=/usr \ --disable-werror \ --enable-kernel=3.2 \ --enable-stack-protector=strong \ libc_cv_slibdir=/libunset GCC_INCDIR CC=&quot;gcc -isystem $GCC_INCDIR -isystem /usr/include&quot;Setting the location of both gcc and system include directories avoids introduction of invalid(无效) paths in debugging symbols. --disable-werrorThis option disables the -Werror option passed to GCC. This is necessary for running the test suite. --enable-stack-protector=strongThis option increases system security by adding extra code to check for buffer overflows(缓冲溢出), such as stack smashing attacks. libc_cv_slibdir=/libThis variable sets the correct library for all systems. We do not want lib64 to be used. 在安装 Glibc 时会抱怨找不到/etc/ld.so.conf文件，这只是无关紧要的输出信息。下面的方式可以避免这个警告：touch /etc/ld.so.conf Fix the generated Makefile to skip an unneeded sanity check that fails in the LFS partial environment: sed &#39;/test-installation/s@$(PERL)@echo not running@&#39; -i ../Makefile Install the configuration file and runtime directory for nscd: cp -v ../nscd/nscd.conf /etc/nscd.confmkdir -pv /var/cache/nscd 安装语言环境上面的命令并没有安装可以让你的电脑用不同语言响应的语言环境。语言环境并不是必须的，只是如果有些语言环境缺失，后续的测试套件可能会跳过一些重要测试用例。 单独的语言环境可以用 localedef 程序安装。例如，下面第一个 localedef 命令将 /usr/share/i18n/locales/cs_CZ 字符无关的语言环境定义和 /usr/share/i18n/charmaps/UTF-8.gz 字符表定义组合在一起，并将结果附加到 /usr/lib/locale/locale-archive 文件末尾。下面的命令将安装能完美覆盖测试所需语言环境的最小集合：12345678910111213141516171819202122mkdir -pv /usr/lib/localelocaledef -i cs_CZ -f UTF-8 cs_CZ.UTF-8localedef -i de_DE -f ISO-8859-1 de_DElocaledef -i de_DE@euro -f ISO-8859-15 de_DE@eurolocaledef -i de_DE -f UTF-8 de_DE.UTF-8localedef -i en_GB -f UTF-8 en_GB.UTF-8localedef -i en_HK -f ISO-8859-1 en_HKlocaledef -i en_PH -f ISO-8859-1 en_PHlocaledef -i en_US -f ISO-8859-1 en_USlocaledef -i en_US -f UTF-8 en_US.UTF-8localedef -i es_MX -f ISO-8859-1 es_MXlocaledef -i fa_IR -f UTF-8 fa_IRlocaledef -i fr_FR -f ISO-8859-1 fr_FRlocaledef -i fr_FR@euro -f ISO-8859-15 fr_FR@eurolocaledef -i fr_FR -f UTF-8 fr_FR.UTF-8localedef -i it_IT -f ISO-8859-1 it_ITlocaledef -i it_IT -f UTF-8 it_IT.UTF-8localedef -i ja_JP -f EUC-JP ja_JPlocaledef -i ru_RU -f KOI8-R ru_RU.KOI8-Rlocaledef -i ru_RU -f UTF-8 ru_RU.UTF-8localedef -i tr_TR -f UTF-8 tr_TR.UTF-8localedef -i zh_CN -f GB18030 zh_CN.GB18030 另外，安装适合你自己国家、语言和字符集的语言环境。 或者，也可以一次性安装在 glibc-2.21/localedata/SUPPORTED 文件里列出的所有语言环境（包括以上列出的所有语言环境以及其它更多），执行下面这个非常耗时的命令：make localedata/install-locales你需要的语言环境几乎不大可能没列在 glibc-2.21/localedata/SUPPORTED 文件中，但如果真的没有可以使用 localedef 命令创建和安装。 配置 Glibc尽管 Glibc 在文件 /etc/nsswitch.conf 丢失或损坏的情况下会创建一个默认的，但是我们需要手动该创建文件，因为 Glibc 的默认文件在网络环境下工作时有问题。另外，也需要设置一下时区。 运行下面的命令创建一个新文件 /etc/nsswitch.conf： 1234567891011121314151617cat &gt; /etc/nsswitch.conf &lt;&lt; &quot;EOF&quot;#Begin /etc/nsswitch.confpasswd: filesgroup: filesshadow: fileshosts: files dnsnetworks: filesprotocols: filesservices: filesethers: filesrpc: files#End /etc/nsswitch.confEOF 安装时区数据： 123456789101112131415tar -xf ../../tzdata2018c.tar.gzZONEINFO=/usr/share/zoneinfomkdir -pv $ZONEINFO/&#123;posix,right&#125;for tz in etcetera southamerica northamerica europe africa antarctica \ asia australasia backward pacificnew systemv; do zic -L /dev/null -d $ZONEINFO -y &quot;sh yearistype.sh&quot; $&#123;tz&#125; zic -L /dev/null -d $ZONEINFO/posix -y &quot;sh yearistype.sh&quot; $&#123;tz&#125; zic -L leapseconds -d $ZONEINFO/right -y &quot;sh yearistype.sh&quot; $&#123;tz&#125;donecp -v zone.tab zone1970.tab iso3166.tab $ZONEINFOzic -d $ZONEINFO -p America/New_Yorkunset ZONEINFO zic -L /dev/null ...这会创建没有时间补偿的 posix 时区数据。一般将它们同时放在 zoneinfo 和 zoneinfo/posix 目录下。另外需要将 POSIX 时区数据放到 zoneinfo 目录下，否则很多测试套件会报错。在嵌入式平台，如果存储空间紧张而且你也不准备更新时区，也可以不用 posix 目录从而节省 1.9MB，但是一些应用程序或测试套件也许会出错。 zic -L leapseconds ...这会创建包含时间补偿的 right 时区数据。在嵌入式平台，空间比较紧张而且你也不打算更新时区或者不需要准确时间，你可以忽略 right 目录从而节省 1.9MB。 zic ... -p ...这会创建 posixrules 文件。我们使用纽约是因为 POSIX 要求夏令时规则与 US 标准一致。 一种确定本地时区的方式是运行下面的脚本：tzselect 然后运行下面的命令创建 /etc/localtime 文件：ln -sfv /usr/share/zoneinfo/&lt;xxx&gt; /etc/localtime将命令中的 替换成你所在实际时区的名字（比如 Canada/Eastern）。我的是 Asia/Shanghai. 配置动态库加载器默认情况下，动态库加载器（/lib/ld-linux.so.2）会搜索目录 /lib 和 /usr/lib 查找程序运行时所需的动态库文件。不过，如果库文件不在 /lib 和 /usr/lib 目录下，需要把它所在目录加到 /etc/ld.so.conf 文件里，保证动态库加载器能找到这些库。通常有两个目录包含额外的动态库，/usr/local/lib 和 /opt/lib，把这两个目录加到动态库加载器的搜索路径中。 运行下面的命令创建一个新文件/etc/ld.so.conf：123456cat &gt; /etc/ld.so.conf &lt;&lt; &quot;EOF&quot;# Begin /etc/ld.so.conf/usr/local/lib/opt/libEOF 如果需要的话，动态库加载器也可以查找目录并包含里面配置文件的内容。通常在这个包含目录下的文件只有一行字指向库目录。运行下面的命令增加这个功能： 1234567cat &gt;&gt; /etc/ld.so.conf &lt;&lt; &quot;EOF&quot;# Add an include directoryinclude /etc/ld.so.conf.d/*.confEOFmkdir -pv /etc/ld.so.conf.d 调整工具链(测试)现在最后的 C 语言库已经装好了，是时候调整工具链，让新编译的程序链接到这些新的库上。 首先,备份 /tools 链接器，然后用我们在第五章调整过的链接器代替它。我们还会创建一个链接，链接到 /tools/$(gcc -dumpmachine)/bin 的副本： mv -v /tools/bin/{ld,ld-old}mv -v /tools/$(gcc -dumpmachine)/bin/{ld,ld-old}mv -v /tools/bin/{ld-new,ld}ln -sv /tools/bin/ld /tools/$(gcc -dumpmachine)/bin/ld 接下来，修改 GCC 参数文件，让它指向新的动态连接器。只需删除所有 “/tools” 的实例，这样应该可以留下到达动态链接器的正确路径。还要调整参数文件，这样 GCC 就知道怎样找到正确的头文件和 Glibc 启动文件。一个 sed 命令就能完成这些:1234gcc -dumpspecs | sed -e &apos;s@/tools@@g&apos; \-e &apos;/\*startfile_prefix_spec:/&#123;n;s@.*@/usr/lib/ @&#125;&apos; \-e &apos;/\*cpp:/&#123;n;s@$@ -isystem /usr/include@&#125;&apos; &gt; \`dirname $(gcc --print-libgcc-file-name)`/specs 确保已调整的工具链的基本功能（编译和链接）都能如期进行是非常必要的。 怎样做呢？执行下面这条命令：123echo &apos;main()&#123;&#125;&apos; &gt; dummy.ccc dummy.c -v -Wl,--verbose &amp;&gt; dummy.logreadelf -l a.out | grep &apos;: /lib&apos; 如果没有任何错误，上条命令的输出应该是（不同的平台上的动态链接器可能名字不同）： [Requesting program interpreter: /lib/ld-linux.so.2]注意 /lib 现在是我们动态链接库的前缀。 现在确保我们已经设置好了启动文件：grep -o &#39;/usr/lib.*/crt[1in].*succeeded&#39; dummy.log上一条命令的输出应该是：123/usr/lib/crt1.o succeeded/usr/lib/crti.o succeeded/usr/lib/crtn.o succeeded 确保链接器能找到正确的头文件：grep -B1 &#39;^ /usr/include&#39; dummy.log这条命令应该返回如下输出：12#include &lt;...&gt; search starts here: /usr/include 接下来，确认新的链接器已经在使用正确的搜索路径： grep &#39;SEARCH.*/usr/lib&#39; dummy.log |sed &#39;s|; |\n|g&#39;应该忽略指向带有 ‘-linux-gnu’ 的路径，上条命令的输出应该是：12SEARCH_DIR(&quot;/usr/lib&quot;)SEARCH_DIR(&quot;/lib&quot;); 然后我们要确定我们使用的是正确的 libc:grep &quot;/lib.*/libc.so.6 &quot; dummy.log 上条命令的输出应该是（在 64 位主机上会有 lib64 目录）：attempt to open /lib/libc.so.6 succeeded 最后，确保 GCC 使用的是正确的动态链接器：grep found dummy.log 上条命令的结果应该是（不同的平台上链接器名字可以不同，64 位主机上是 lib64 目录）：found ld-linux.so.2 at /lib/ld-linux.so.2 如果显示的结果不一样或者根本没有显示，那就出了大问题。检查并回溯之前的步骤，找到出错的地方并改正。最有可能的原因是参数文件的调整出了问题。在进行下一步之前所有的问题都要解决。一旦所有的事情都正常了，清除测试文件：rm -v dummy.c a.out dummy.log Zlib-1.2.11Zlib 软件包包括一些程序所使用的压缩和解压缩例程。 共享库需要移动到 /lib，因此需要重建 /usr/lib 里面的 .so 文件：12mv -v /usr/lib/libz.so.* /libln -sfv ../../lib/$(readlink /usr/lib/libz.so) /usr/lib/libz.so File-5.22File 软件包包括一个判断给定的某个或某些文件文件类型的工具。 Readline-7.0Readline 软件包是一个提供命令行编辑和历史能力的一些库 sed -i &#39;/MV.*old/d&#39; Makefile.insed -i &#39;/{OLDSUFF}/c:&#39;support/shlib-install 编译:123./configure --prefix=/usr \ --disable-static \ --docdir=/usr/share/doc/readline-7.0 1make SHLIB_LIBS=&quot;-L/tools/lib -lncursesw&quot; SHLIB_LIBS=&quot;-L/tools/lib -lncursesw&quot;This option forces Readline to link against the libncursesw library. 转移动态库到合适位置,修复一些链接符号:123mv -v /usr/lib/lib&#123;readline,history&#125;.so.* /libln -sfv ../../lib/$(readlink /usr/lib/libreadline.so) /usr/lib/libreadline.soln -sfv ../../lib/$(readlink /usr/lib/libhistory.so ) /usr/lib/libhistory.so 如果需要的话,安装文档:install -v -m644 doc/*.{ps,pdf,html,dvi} /usr/share/doc/readline-7.0 M4-1.4.18M4软件包包含一个宏处理器 Bc-1.07.1Bc软件包用于数学处理change an internal script to use sed instead of ed:1234567891011cat &gt; bc/fix-libmath_h &lt;&lt; &quot;EOF&quot;#! /bin/bashsed -e &apos;1 s/^/&#123;&quot;/&apos; \ -e &apos;s/$/&quot;,/&apos; \ -e &apos;2,$ s/^/&quot;/&apos; \ -e &apos;$ d&apos; \ -i libmath.hsed -e &apos;$ s/$/0&#125;/&apos; \ -i libmath.hEOF Create temporary symbolic links so the package can find the readline library and confirm that its required libncurses library is available. Even though the libraries are in /tools/lib at this point, the system will use /usr/lib at the end of this chapter.12ln -sv /tools/lib/libncursesw.so.6 /usr/lib/libncursesw.so.6ln -sfv libncurses.so.6 /usr/lib/libncurses.so Fix an issue in configure due to missing files in the early stages of LFS:sed -i -e &#39;/flex/s/as_fn_error/: ;; # &amp;/&#39; configure编译:1234./configure --prefix=/usr \ --with-readline \ --mandir=/usr/share/man \ --infodir=/usr/share/info --with-readlineThis option tells Bc to use the readline library that is already installed on the system rather than using its own readline version. To test bc, run the commands below. There is quite a bit of output, so you may want to redirect it to a file. There are a very small percentage of tests (10 of 12,144) that will indicate a round off error at the last digit.echo &quot;quit&quot; | ./bc/bc -l Test/checklib.b Binutils-2.25Binutils 软件包包含一个链接器、一个汇编器、以及其它处理目标文件的工具。验证:expect -c &quot;spawn ls“正常输出:spawn ls假如输出包括下面的信息，那么表示没有为 PTY 操作设置好环境。在运行 Binutils 和 GCC 的测试套件之前需要解决这个问题：12The system has no more ptys.Ask your system administrator to create more. 编译:12345678../configure --prefix=/usr \ --enable-gold \ --enable-ld=default \ --enable-plugins \ --enable-shared \ --disable-werror \ --enable-64-bit-bfd \ --with-system-zlib --enable-goldBuild the gold linker and install it as ld.gold (along side the default linker). --enable-ld=defaultBuild the original bdf linker and install it as both ld (the default linker) and ld.bfd. --enable-pluginsEnables plugin support for the linker. --enable-64-bit-bfdEnables 64-bit support (on hosts with narrower word sizes). May not be needed on 64-bit systems, but does no harm. --with-system-zlibUse the installed zlib library rather than building the included version. 编译:make tooldir=/usr tooldir=/usr一般来说，tooldir (最终存放可执行文件的目录) 设置为 $(exec_prefix)/$(target_alias)。例如,x86_64机器会把它扩展为/usr/x86_64-unknown-linux-gnu。因为这是个自定制的系统，并不需要 /usr 中的特定目标目录。如果系统用于交叉编译（例如，在 Intel 机器上编译能生成在 PowerPC 机器上运行的代码的软件包）会使用 $(exec_prefix)/$(target_alias)。 测试make -k check安装:make tooldir=/usr install GMP-6.1.2GMP 软件包包含一些数学库。这里有对任意精度数值计算很有用的函数。 如果你是为 32 位的 x86 系统编译，但是你的 CPU 可以运行 64 位代码 而且 环境中你有指定的 CFLAGS，那么配置脚本会尝试配置为 64 位并导致失败。用下面的方式执行配置命令来避免这个问题:ABI=32 ./configure ... The default settings of GMP produce libraries optimized for the host processor. If libraries suitable for processors less capable than the host’s CPU are desired,(库对于处理器的适合度小于宿主的cpu的能力,然后创建通用的库),generic libraries can be created by running the following:12cp -v configfsf.guess config.guesscp -v configfsf.sub config.sub --enable-cxx这个参数启用 C++ 支持 --docdir=/usr/share/doc/gmp-6.1.2这个变量指定保存文档的正确位置。 Ensure that all 190 tests in the test suite passed.awk &#39;/# PASS:/{total+=$3} ; END{print total}&#39; gmp-check-log 1234./configure --prefix=/usr \ --enable-cxx \ --disable-static \ --docdir=/usr/share/doc/gmp-6.1.2 MPFR-4.0.1编译:1234./configure --prefix=/usr \ --disable-static \ --enable-thread-safe \ --docdir=/usr/share/doc/mpfr-4.0.1 MPC-1.1.0编译:123./configure --prefix=/usr \ --disable-static \ --docdir=/usr/share/doc/mpc-1.1.0 GCC-7.3.0If building on x86_64, change the default directory name for 64-bit libraries to “lib”:123456case $(uname -m) in x86_64) sed -e &apos;/m64=/s/lib64/lib/&apos; \ -i.orig gcc/config/i386/t-linux64 ;;esac Remove the symlink created earlier as the final gcc includes will be installed here:rm -f /usr/lib/gcc 编译:123456SED=sed \../configure --prefix=/usr \ --enable-languages=c,c++ \ --disable-multilib \ --disable-bootstrap \ --with-system-zlib SED=sedSetting this environment variable prevents a hard-coded path to /tools/bin/sed. --with-system-zlibThis switch tells GCC to link to the system installed copy of the Zlib library, rather than its own internal copy. One set of tests in the GCC test suite is known to exhaust the stack, so increase the stack size prior to running the tests:(一个测试会用尽tests,需要扩大栈的容量)ulimit -s 32768一些意料之外的错误总是难以避免。GCC 开发者通常意识到了这些问题，但还没有解决。除非测试结果和上面 URL 中的相差很大，不然就可以安全继续。 On some combinations of kernel configuration and AMD processors there may be more than 1100 failures in the gcc.target/i386/mpx tests (which are designed to test the MPX option on recent Intel processors). These can safely be ignored on AMD processors.我表示震惊…. 一些软件包希望 GCC 安装在 /lib 目录。为了支持那些软件包，可以建立一个符号链接： ln -sv ../usr/bin/cpp /lib译者注：如果还在 gcc-build 目录，这里应该是 ln -sv ../../usr/bin/cpp /lib 。很多软件包用命令 cc 调用 C 编译器。为了满足这些软件包，创建一个符号链接：ln -sv gcc /usr/bin/cc 增加一个兼容符号链接启用编译程序时进行链接时间优化（Link Time Optimization，LTO）：123install -v -dm755 /usr/lib/bfd-pluginsln -sfv ../../libexec/gcc/$(gcc -dumpmachine)/7.3.0/liblto_plugin.so \ /usr/lib/bfd-plugins/ 然后进行检验(步骤省略)最后，移动位置放错的文件：12mkdir -pv /usr/share/gdb/auto-load/usr/libmv -v /usr/lib/*gdb.py /usr/share/gdb/auto-load/usr/lib Bzip2-1.0.6Bzip2 软件包包含压缩和解压缩的程序。用 bzip2 压缩文本文件能获得比传统的 gzip 更好的压缩比。使用能为这个软件包安装帮助文档的补丁：patch -Np1 -i ../bzip2-1.0.6-install_docs-1.patch 下面的命令确保安装的符号链接是相对链接：sed -i &#39;s@\(ln -s -f \)$(PREFIX)/bin/@\1@&#39; Makefile 确认 man 页面安装到了正确的位置：sed -i &quot;s@(PREFIX)/man@(PREFIX)/share/man@g&quot; Makefile 编译:12make -f Makefile-libbz2_somake clean -f Makefile-libbz2_so这会使用不同的 Makefile 文件编译 Bzip2，在这里是 Makefile-libbz2_so，它会创建动态 libbz2.so 库，并把它链接到 Bzip2 工具。 安装使用动态链接库的 bzip2 二进制文件到 /bin 目录， 创建一些必须的符号链接并清理：123456cp -v bzip2-shared /bin/bzip2cp -av libbz2.so* /libln -sv ../../lib/libbz2.so.1.0 /usr/lib/libbz2.sorm -v /usr/bin/&#123;bunzip2,bzcat,bzip2&#125;ln -sv bzip2 /bin/bunzip2ln -sv bzip2 /bin/bzcat Pkg-config-0.29.2pkg-config 软件包包含一个在配置和 make 文件运行时把 include 路径和库路径传递给编译工具的工具。1234./configure --prefix=/usr \ --with-internal-glib \ --disable-host-tool \ --docdir=/usr/share/doc/pkg-config-0.29.2 --with-internal-glib这会让 pkg-config 使用它自己内部版本的 Glib，因为在 LFS 中没有可用的外部版本。 --disable-host-tool这个选项取消创建到 pkg-config 程序的不必要的硬链接。 Ncurses-6.1Ncurses 软件包包含用于不依赖于特定终端的字符屏幕处理的库。Don’t install a static library that is not handled by configure:sed -i &#39;/LIBTOOL_INSTALL/d&#39; c++/Makefile.in 编译:1234567./configure --prefix=/usr \ --mandir=/usr/share/man \ --with-shared \ --without-debug \ --without-normal \ --enable-pc-files \ --enable-widec --enable-widec这个选项会编译宽字符库（例如 libncursesw.so.5.9）而不是常规的）例如 libncurses.so.5.9）。宽字符库可用于多字节和传统的 8 位本地字符， 而常规的库只能用于 8 位本地字符。宽字符库和常规的库是源文件兼容的，而不是二进制文件兼容的。 --enable-pc-files该选项为 pkg-config 生成和安装 .pc 文件。 --without-normal该选项取消生成与安装静态库 Move the shared libraries to the /lib directory, where they are expected to reside(转移库文件):mv -v /usr/lib/libncursesw.so.6* /lib Because the libraries have been moved, one symlink points to a non-existent file. Recreate it(重新链接库文件):ln -sfv ../../lib/$(readlink /usr/lib/libncursesw.so) /usr/lib/libncursesw.so 很多应用程序仍然希望编辑器能找到非宽字符的 Ncurses 库。通过符号链接和链接器脚本欺骗这样的应用链接到宽字符库：12345for lib in ncurses form panel menu ; do rm -vf /usr/lib/lib$&#123;lib&#125;.so echo &quot;INPUT(-l$&#123;lib&#125;w)&quot; &gt; /usr/lib/lib$&#123;lib&#125;.so ln -sfv $&#123;lib&#125;w.pc /usr/lib/pkgconfig/$&#123;lib&#125;.pcdone 最后，确保在编译时会查找 -lcurses 的旧应用程序仍然可以编译： 123rm -vf /usr/lib/libcursesw.soecho &quot;INPUT(-lncursesw)&quot; &gt; /usr/lib/libcursesw.soln -sfv libncurses.so /usr/lib/libcurses.so 注意上面的指令并不会创建非宽字符 Ncurses 库，因为没有从源文件中编译安装的软件包会在运行时链接它们。如果你由于一些仅有二进制的应用程序或要和 LSB 兼容而必须要有这样的库，用下面的命令重新编译软件包：123456789make distclean./configure --prefix=/usr \ --with-shared \ --without-normal \ --without-debug \ --without-cxx-binding \ --with-abi-version=5make sources libscp -av lib/lib*.so.5* /usr/lib Attr-2.4.47attr 软件包包含管理文件系统对象的扩展属性的工具。Modify the documentation directory so that it is a versioned directory:sed -i -e &#39;s|/@pkg_name@|&amp;-@pkg_version@|&#39; include/builddefs.in Prevent installation of manual pages that were already installed by the man pages package:sed -i -e &quot;/SUBDIRS/s|man[25]||g&quot; man/Makefile Fix a problem in the test procedures caused by changes in perl-5.26:(修复问题)sed -i &#39;s:{(:\\{(:&#39; test/run 123./configure --prefix=/usr \ --bindir=/bin \ --disable-static The shared library needs to be moved to /lib, and as a result the .so file in /usr/lib will need to be recreated:mv -v /usr/lib/libattr.so.* /libln -sfv ../../lib/$(readlink /usr/lib/libattr.so) /usr/lib/libattr.so Acl-2.2.52Modify the documentation directory so that it is a versioned directory: sed -i -e &#39;s|/@pkg_name@|&amp;-@pkg_version@|&#39; include/builddefs.inFix some broken tests: sed -i &quot;s:| sed.*::g&quot; test/{sbits-restore,cp,misc}.testFix a problem in the test procedures caused by changes in perl-5.26: sed -i &#39;s/{(/\\{(/&#39; test/runAdditionally, fix a bug that causes getfacl -e to segfault on overly long group name:12sed -i -e &quot;/TABS-1;/a if (x &gt; (TABS-1)) x = (TABS-1);&quot; \ libacl/__acl_to_any_text.c Prepare Acl for compilation:1234./configure --prefix=/usr \ --bindir=/bin \ --disable-static \ --libexecdir=/usr/lib Libcap-2.25Libcap 软件包实现了可用在 Linux 内核上的对 POSIX 1003.1e 功能的用户空间接口。 这些功能将所有强大 root 权限划分为不同的权限组合。Install the package:12make RAISE_SETFCAP=no lib=lib prefix=/usr installchmod -v 755 /usr/lib/libcap.so The meaning of the make option:RAISE_SETFCAP=noThis parameter skips trying to use setcap on itself. This avoids an installation error if the kernel or file system does not support extended capabilities. lib=libThis parameter installs the library in $prefix/lib rather than $prefix/lib64 on x86_64. It has no effect onx86. Sed-4.4The Sed package contains a stream editor.First fix an issue in the LFS environment and remove a failing test:sed -i &#39;s/usr/tools/&#39; build-aux/help2mansed -i &#39;s/testsuite.panic-tests.sh//&#39; Makefile.in Prepare Sed for compilation:./configure --prefix=/usr --bindir=/bin Shadow-4.5Shadow 软件包包含以安全方式处理密码的程序。注意如果你喜欢强制使用更强的密码，在编译 Shadow 之前可以根据 http://www.linuxfromscratch.org/blfs/view/systemd/postlfs/cracklib.html 安装 CrackLib。然后在下面的 configure 命令中增加 --with-libcrack。 取消安装 groups 程序以及它的 man 文档，因为 Coreutils 提供了一个更好的版本：1234sed -i &apos;s/groups$(EXEEXT) //&apos; src/Makefile.infind man -name Makefile.in -exec sed -i &apos;s/groups\.1 / /&apos; &#123;&#125; \;find man -name Makefile.in -exec sed -i &apos;s/getspnam\.3 / /&apos; &#123;&#125; \;find man -name Makefile.in -exec sed -i &apos;s/passwd\.5 / /&apos; &#123;&#125; \; 比起默认的 crypt 方法，用更安全的 SHA-512 方法加密密码，它允许密码长度超过 8 个字符。也需要把 Shadow 默认使用的用户邮箱由陈旧的 /var/spool/mail 位置改为正在使用的 /var/mail 位置12sed -i -e &apos;s@#ENCRYPT_METHOD DES@ENCRYPT_METHOD SHA512@&apos; \ -e &apos;s@/var/spool/mail@/var/mail@&apos; etc/login.defs Note如果你选择编译支持 Cracklib 的 Shadow，运行下面的命令：sed -i &#39;s@DICTPATH.*@DICTPATH\t/lib/cracklib/pw_dict@&#39; etc/login.defs 做个小的改动使 useradd 的默认设置和 LFS 的组文件一致：sed -i &#39;s/1000/999/&#39; etc/useradd Prepare Shadow for compilation:./configure --sysconfdir=/etc --with-group-name-max-length=32 The meaning of the configure option:--with-group-name-max-length=32最长用户名为 32 个字符，使组名称也是如此 Configuring Shadow该软件包包含增加、更改、以及删除用户和组的工具；设置和修改密码；执行其它特权级任务。软件包解压后的 doc/HOWTO 文件有关于 password shadowing 的完整解释。如果使用 Shadow 支持，记住需要验证密码（显示管理器、FTP 程序、pop3 守护进程等）的程序必须和 Shadow 兼容。 也就是说，它们要能使用 Shadow 加密的密码。 运行下面的命令启用 shadow 密码；pwconv 运行下面的命令启用 shadow 组密码：grpconv 用于 useradd 工具的 Shadow 配置有一些需要解释的注意事项。首先，useradd 工具的默认操作是创建用户以及和用户名相同的组。默认情况下，用户 ID(UID) 和组 ID(GID) 的数字从 1000 开始。这意味着如果你不传递参数给 useradd，系统中的每个用户都会属于一个不同的组。如果不需要这样的结果，你需要传递参数 -g 到 useradd。默认参数保存在 /etc/default/useradd 文件中。你需要修改该文件中的两个参数来实现你的特定需求。 /etc/default/useradd 参数解释 GROUP=1000该参数设定 /etc/group 文件中使用的起始组序号。你可以把它更改为任何你需要的数字。注意 useradd 永远不会重用 UID 或 GID。如果该参数指定的数字已经被使用了，将会使用它之后的下一个可用数字。另外注意如果你系统中没有序号为 1000 的组，第一次使用useradd 而没有参数 -g 的话，你会在终端中看到一个提示信息： useradd: unknown GID 1000。你可以忽视这个信息，它会使用组号 1000。 CREATE_MAIL_SPOOL=yes这个参数会为 useradd 新添加的用户创建邮箱文件。useradd 会使组 mail 拥有该文件的所有权，并赋予组 0660 的权限。如果你希望 useradd 不创建这些邮箱文件，你可以运行下面的命令：sed -i &#39;s/yes/no/&#39; /etc/default/useradd 设置 root 密码运行下面的命令为用户 root 设置密码：passwd root (由于构建文档太长,此篇到此结束,还有50多个工具的编译过程没有记录,主要还是要看构建文档)]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[迷时]]></title>
    <url>%2F2018%2F07%2F17%2Fxiaoshi1%2F</url>
    <content type="text"><![CDATA[时光从不给人告白的机会慢慢的便失去了感觉直到掌心温度渐渐流逝眼眶才湿润起来到底我该以怎样的眼神看着你抉择中我总自以为抓住了时光却不想陷入了它的把戏可惜一生只够走一次只愿能给我留个念想]]></content>
      <categories>
        <category>诗歌集</category>
      </categories>
      <tags>
        <tag>小诗</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-07-15 lfs编译日志]]></title>
    <url>%2F2018%2F07%2F15%2F2018-07-15%2F</url>
    <content type="text"><![CDATA[接上一次错误:发现是自己设备umount没有全部umount掉,然后就关机了结果出现了错误,经过一系列的操作后,正确这里简单的写一下:最开始是找到网上的fuser,结果发现没什么用最后使用mount,列出所有挂载点,将我u盘的挂载点逐一umount掉从最小的开始,最后成功 ps:gcc编译时长11小时,苦了我的电脑了cpu 70多度已经习以为常,编译工具链有点麻烦啊我现在进度还只有一半,后面的总结估计要好几天才能完成]]></content>
      <categories>
        <category>lfs编译日志</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[keyboard_shortcuts]]></title>
    <url>%2F2018%2F07%2F14%2Fkeyboard-shortcuts%2F</url>
    <content type="text"><![CDATA[内核以下是系统底层的快捷键，通常被用于调试。遇到系统问题，请尽可能尝试这些快捷键，而不是按住电源开关强制关机。 这些快捷键需要首先使用如下命令激活echo &quot;1&quot; &gt; /proc/sys/kernel/sysrq如果你希望在系统启动时就开启，请编辑 /etc/sysctl.d/99-sysctl.conf 并添加配置 kernel.sysrq = 1. 如果你希望在挂载分区和启动引导前就开启的话, 请在内核启动参数上添加 sysrq_always_enabled=1. 记住这个激活命令的通用口诀是 “Reboot Even If System Utterly Broken” (或者”REISUB“)。 键盘快捷键 描述Alt+SysRq+R+ Unraw 从X收回对键盘的控制 Alt+SysRq+E+ Terminate 向所有进程发送SIGTERM信号，让它们正常终止 Alt+SysRq+I+ Kill 向所有进程发送SIGKILL信号，强制立即终止 Alt+SysRq+S+ Sync 将待写数据写入磁盘 Alt+SysRq+U+ Unmount 卸载所有硬盘然后重新按只读模式挂载 Alt+SysRq+B+ Reboot 重启]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux From Scratch Pass 3]]></title>
    <url>%2F2018%2F07%2F14%2Flfs3%2F</url>
    <content type="text"><![CDATA[安装基本的系统软件注意使用root用户 准备虚拟内核文件系统内核会挂载几个文件系统用于自己和用户空间程序交换信息。这些文件系统是虚拟的，并不占用实际磁盘空间，它们的内容会放在内存里。 开始先创建将用来挂载文件系统的目录： mkdir -pv $LFS/{dev,proc,sys,run} 创建初始设备节点在内核引导系统的时候，它依赖于几个设备节点，特别是 console 和 null 两个设备。这些设备节点需要创建在硬盘上，这样在 udevd 启动之前它们也仍然有效，特别是在 Linux 使用init=/bin/bash 参数启动的时候。运行下面的命令创建这几个设备节点： mknod -m 600 $LFS/dev/console c 5 1mknod -m 666 $LFS/dev/null c 1 3注: mknod创建设备文件 -m设置权限 挂载和激活 /dev通常激活 /dev 目录下设备的方式是在 /dev目录挂载一个虚拟文件系统（比如 tmpfs），然后允许在检测到设备或打开设备时在这个虚拟文件系统里动态创建设备节点。这个通常是在启动过程中由 Udev 完成。由于我们的新系统还没有 Udev 而且也没有被引导启动，有必要手动挂载和激活 /dev 目录。这可以通过绑定挂载宿主机系统的 /dev 目录实现。绑定挂载是一种特殊的挂载模式，它允许在另外的位置创建某个目录或挂载点的镜像。运行下面的命令来实现： mount -v --bind /dev $LFS/dev 挂载虚拟文件系统现在挂载剩下的虚拟内核文件系统：1234mount -vt devpts devpts $LFS/dev/pts -o gid=5,mode=620mount -vt proc proc $LFS/procmount -vt sysfs sysfs $LFS/sysmount -vt tmpfs tmpfs $LFS/run 注:mount [-fnrsvw] [-t vfstype] [-o options] device dir gid=5这个选项会让 devpts 创建的所有设备节点属主的组 ID 都是 5。这是我们待会将要指定给 tty 组的 ID。现在我们先用 ID代替组名，因为宿主机系统可能会为它的 tty 组分配了不同的 ID。 mode=0620这个选项会让 devpts 创建的所有设备节点的属性是 0620（属主用户可读写，组成员可写）。和上一个选项同时使用，可以保证 devpts 所创建的设备节点能满足 grantpt() 函数的要求，这意味着不需要 Glibc 的 pt_chown 帮助程序（默认没有安装）了。 在某些宿主机系统里，/dev/shm 是一个指向 /run/shm 的软链接。这个 /run 下的 tmpfs 文件系统已经在之前挂载了，所以在这里只需要创建一个目录。123if [ -h $LFS/dev/shm ]; then mkdir -pv $LFS/$(readlink $LFS/dev/shm)fi 升级问题软件包管理器可以在软件新版本发布后轻松升级。一般来说 LFS 和 BLFS 手册里的指令可以用来升级到新版本。下面是一些在你准备升级软件包时需要注意的事情，特别是在一个运行中的系统。 1.如果需要升级 Glibc 到新版本（比如，从 glibc-2.19 升级到 glibc-2.20），重新构建整个 LFS 会比较安全。虽然你也许能够按依赖关系重新编译所有的软件包，不过我们不建议这样做。 2.如果某个包含的动态库的软件包升级了，而且库名字有改变，那么所有动态链接到这个库的软件包都需要重新链接新的库。（请注意软件包版本和库名字并不存在相关性。）举个例子，某个软件包 foo-1.2.3 安装了一个名叫 libfoo.so.1 的动态库。然后假设你把这个软件包升级到了新版本 foo-1.2.4，而新版本会安装名叫 libfoo.so.2的动态库。在这种情况下，所有动态链接到 libfoo.so.1 的软件包都需要重新编译链接到 libfoo.so.2。注意在所有依赖软件包重新编译完成之前，请不要删除旧版的库文件。 创建软件包存档在这种方式里，像之前的软链接软件包管理方式里所描述的那样，软件包被伪装安装到一个独立的目录树里。在安装完成后，会将已安装文件打包成一个软件包存档。然后这个存档会用来在本地机器或其他机器上安装软件包。 这种方式为商业发行版中的大多数包管理器所采用。一些例子是 RPM（它顺便也是 Linux 标准规范 里所要求的）、pkg-utils、Debian 的 apt、以及 Gentoo 的 Portage 系统。该页面描述了如何在 LFS 系统里采用这种包管理方式： http://www.linuxfromscratch.org/hints/downloads/files/fakeroot.txt。 创建带有依赖关系的软件包存档非常复杂，已经超出 LFS 手册范围了。 基于用户的软件包管理在这种方式，是 LFS 特有的，由 Matthias Benkmann 所设计，可以在 Hints Project 里能找到。在这种方式里，每个软件包都由一个单独的用户安装到标准的位置。属于某个软件包的文件可以通过检查用户 ID 轻松识别出来。关于这种方式的特性和短处非常复杂，在本节里说不清楚。详细的信息请参看 http://www.linuxfromscratch.org/hints/downloads/files/more_control_and_pkg_man.txt。 在多个系统上布置 LFSLFS 系统的一个优点是没有会依赖磁盘系统里文件位置的文件。克隆一份 LFS 到和宿主机器相似配置的机器上，简单到只要对包含根目录的 LFS 分区（对于一个基本的 LFS 构建不压缩的话大概有 250MB）使用 tar命令打包，然后通过网络传输或光盘拷贝到新机器上展开即可。在这之后，还需要调整一些配置文件，包括：/etc/hosts、/etc/fstab、/etc/passwd、/etc/group、/etc/shadow 和 /etc/ld.so.conf。 根据系统硬件和原始内核配置文件的差异，可能还需要重新编译一下内核。 最后，需要使用 8.4 “用 GRUB 设置引导过程”里所介绍的方法让新系统可引导 进入 Chroot 环境123456chroot &quot;$LFS&quot; /tools/bin/env -i \ HOME=/root \ TERM=&quot;$TERM&quot; \ PS1=&apos;\u:\w\$ &apos; \ PATH=/bin:/usr/bin:/sbin:/usr/sbin:/tools/bin \ /tools/bin/bash --login +h(选项来关闭其哈希功能) 给 env 命令传递 -i 选项会清除这个 chroot 切换进去的环境里所有变量。随后，只重新设定了 HOME、TERM、PS1 和 PATH 变量。TERM=$TERM 语句会设定 chroot 进入的环境里的 TERM 变量为进入前该变量同样的值。许多程序需要这个变量才能正常工作，比如 vim 和 less。如果还需要设定其他变量，比如 CFLAGS 或 CXXFLAGS，就在这里一起设定比较合适。 从这里以后，就不再需要 LFS 变量了，因为后面所有工作都将被限定在 LFS 文件系统里。这是因为我们已经告诉 Bash 终端 $LFS 就是当前的根目录（/）。 请注意 /tools/bin 放在了 PATH 变量的最后。意思是在每个软件的最后版本编译安装好后就不再使用临时工具了。这还需要让 shell 不要“记住”每个可执行文件的位置—这样的话，还要给 bash 加上 +h 选项来关闭其哈希功能。 注意一下 bash 的提示符是 I have no name!。这是正常的，因为这个时候 /etc/passwd文件还没有被创建。 注意:非常重要，本章从这以后的命令，以及后续章节里的命令都要在 chroot 环境下运行。如果因为某种原因（比如说重启）离开了这个环境，请保证要按照 “挂载和激活 /dev” 和 “挂载虚拟内核文件系统” 里所说的那样挂载虚拟内核文件系统，然后在继续构建之前重新运行 chroot 进入环境。 创建目录123456789101112131415161718mkdir -pv /&#123;bin,boot,etc/&#123;opt,sysconfig&#125;,home,lib/firmware,mnt,opt&#125;mkdir -pv /&#123;media/&#123;floppy,cdrom&#125;,sbin,srv,var&#125;install -dv -m 0750 /rootinstall -dv -m 1777 /tmp /var/tmpmkdir -pv /usr/&#123;,local/&#125;&#123;bin,include,lib,sbin,src&#125;mkdir -pv /usr/&#123;,local/&#125;share/&#123;color,dict,doc,info,locale,man&#125;mkdir -v /usr/&#123;,local/&#125;share/&#123;misc,terminfo,zoneinfo&#125;mkdir -v /usr/libexecmkdir -pv /usr/&#123;,local/&#125;share/man/man&#123;1..8&#125;case $(uname -m) in x86_64) mkdir -v /lib64 ;;esacmkdir -v /var/&#123;log,mail,spool&#125;ln -sv /run /var/runln -sv /run/lock /var/lockmkdir -pv /var/&#123;opt,cache,lib/&#123;color,misc,locate&#125;,local&#125; 一般目录默认会按 755 的权限创建，但是这并不适用于所有的目录。在上面的命令里，有两个改动—一个是 root 用户的主目录，另一个是存放临时文件的目录。 第一个模式改动能保证不是所有人都能进入 /root目录—同样的一般用户也需要为他/她的主目录设置这样的模式。第二个模式改动能保证所有用户都可以写目录 /tmp 和 /var/tmp。还增加了一个所谓的 “粘滞位”的限制，即位掩码 0x1777 中最高位的比特(1)。 关于 FHS 兼容性这个目录树是基于文件系统目录结构标准（FHS）（参看 https://wiki.linuxfoundation.org/en/FHS) ,FHS 标准还规定了要有 /usr/local/games 和 /usr/share/games 目录。另外 FHS 标准关于/usr/local/share 里子目录的结构要求并不清晰，所以我们只创建了我们需要的目录。不过，如果你更喜欢严格遵守 FHS 标准，创建这些目录也不会有问题。 创建必需的文件和符号链接有些程序里会使用写死的路径调用其它暂时还未安装的程序。为了满足这种类型程序的需要，我们将创建一些符号链接，在完成本章内容后这些软件会安装好，并替代之前的符号链接:12345ln -sv /tools/bin/&#123;bash,cat,dd,echo,ln,pwd,rm,stty&#125; /binln -sv /tools/bin/&#123;install,perl&#125; /usr/binln -sv /tools/lib/libgcc_s.so&#123;,.1&#125; /usr/libln -sv /tools/lib/libstdc++.&#123;a,so&#123;,.6&#125;&#125; /usr/libln -sv bash /bin/sh 由于历史原因，Linux 在文件/etc/mtab中维护一个已挂载文件系统的列表。而现代内核改为在内部维护这个列表，并通过 /proc 文件系统输出给用户。为了满足一些依赖 /etc/mtab 文件的应用程序，我们要创建下面的符号链接：ln -sv /proc/self/mounts /etc/mtab为了让 root 用户能正常登录，而且 root 的名字能被正常识别，必须在文件 /etc/passwd 和 /etc/group 中写入相应的内容。 运行下面的命令创建 /etc/passwd 文件 1234567cat &gt; /etc/passwd &lt;&lt; &quot;EOF&quot;root:x:0:0:root:/root:/bin/bashbin:x:1:1:bin:/dev/null:/bin/falsedaemon:x:6:6:Daemon User:/dev/null:/bin/falsemessagebus:x:18:18:D-Bus Message Daemon User:/var/run/dbus:/bin/falsenobody:x:99:99:Unprivileged User:/dev/null:/bin/falseEOF root 用户的实际密码（这里的 “x” 只是占位符）将在后面创建。 运行下面的命令创建 /etc/group 文件：12345678910111213141516171819202122232425cat &gt; /etc/group &lt;&lt; &quot;EOF&quot;root:x:0:bin:x:1:daemonsys:x:2:kmem:x:3:tape:x:4:tty:x:5:daemon:x:6:floppy:x:7:disk:x:8:lp:x:9:dialout:x:10:audio:x:11:video:x:12:utmp:x:13:usb:x:14:cdrom:x:15:adm:x:16:messagebus:x:18:systemd-journal:x:23:input:x:24:mail:x:34:nogroup:x:99:users:x:999:EOF 这里创建的用户组没有参照任何标准 — 它们一部分是为了满足本章中配置 Udev 的需要，还有一部分来自一些现存 Linux 发行版的通用设定。另外，某些测试套件也依赖特定用户或组。而 Linux 标准规范 （LSB，参见http://www.linuxbase.org ）只要求以组 ID（GID）为 0 创建用户组 root 以及以 GID 为 1 创建用户组 bin。系统管理员可以自由分配其它所有用户组名字和 GID，因为优秀的程序不会依赖 GID 数字，而是使用组名。 为了移除 “I have no name!” 的提示符，可以打开一个新 shell。由于完整的 Glibc 已经在 第五章 里装好了，而且已经创建好了 /etc/passwd 和 /etc/group 文件，用户名和组名就可以正常解析了： exec /tools/bin/bash --login +h 注意这里使用了 +h 参数。这样会告诉 bash 不要使用它内建的路径哈希功能。而不加这个参数的话， bash 将会记住曾经执行过程序的路径。为了在新编译安装好程序后就能马上使用，参数 +h 将在本章中一直使用。 程序 login，agetty 和init（还有一些其它的）会使用一些日志文件来记录信息，比如谁在什么时候登录了系统。不过，在日志文件不存在的时候这些程序一般不会写入。下面初始化一下日志文件并加上合适的权限：1234touch /var/log/&#123;btmp,lastlog,wtmp&#125;chgrp -v utmp /var/log/lastlogchmod -v 664 /var/log/lastlogchmod -v 600 /var/log/btmp The /var/log/wtmp file records all logins and logouts.The /var/log/lastlog file records when each user last logged in.The /var/log/faillog file records failed login attempts.The /var/log/btmp file records the bad login attempts.文件 /run/utmp 会记录当前已登录的用户。这个文件会在启动脚本中动态创建。 接下来还是漫长的构建的过程,尤其是glibc和gcc (吐血)]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-07-14 lfs编译日志]]></title>
    <url>%2F2018%2F07%2F14%2F2018-07-14%2F</url>
    <content type="text"><![CDATA[编译gcc出现错误:123456789In file included from ../../gcc-4.9.2/gcc/cp/except.c:1013:cfns.gperf:101:1: error: &apos;const char* libc_name_p(const char*, unsigned int)&apos; redeclared inline with &apos;gnu_inline&apos; attributecfns.gperf:26:14: note: &apos;const char* libc_name_p(const char*, unsigned int)&apos; previously declared herecfns.gperf:26:14: warning: inline function &apos;const char* libc_name_p(const char*, unsigned int)&apos; used but never definedmake[2]: *** [Makefile:1058: cp/except.o] Error 1make[2]: Leaving directory &apos;/mnt/lfs/sources/gcc-build/gcc&apos;make[1]: *** [Makefile:4027: install-gcc] Error 2make[1]: Leaving directory &apos;/mnt/lfs/sources/gcc-build&apos;make: *** [Makefile:2176: install] Error 2 原因 : 编译用的gcc版本太高 编译了一部分,电脑关机去睡觉,结果醒来接着编译的时候出错了…我估计着是chroot进去的时候出现了差异,然后出现错误真实让人头大,打算测试一下….有点心累同时比较幸运,还好不是在编完gcc的时候出现,不然时间浪费太严重了]]></content>
      <categories>
        <category>lfs编译日志</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tar基础]]></title>
    <url>%2F2018%2F07%2F14%2Ftar%2F</url>
    <content type="text"><![CDATA[记录: 来自网上 12345678910111213141516171、*.tar 用 tar -xvf 解压2、*.gz 用 gzip -d或者gunzip 解压3、*.tar.gz和*.tgz 用 tar -xzf 解压4、*.bz2 用 bzip2 -d或者用bunzip2 解压5、*.tar.bz2用tar -xjf 解压6、*.Z 用 uncompress 解压7、*.tar.Z 用tar -xZf 解压8、*.rar 用 unrar e解压9、*.zip 用 unzip 解压 我一般解压都是直接使用tar -xvf xxx这样比较省事吧 (=\ _ \ =) 然后压缩就是直接使用 tar -cvf xxx 了 加入有大小需求的话,我会考虑加上z,j之类的]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux From Scrach Pass 2]]></title>
    <url>%2F2018%2F07%2F14%2Flfs2%2F</url>
    <content type="text"><![CDATA[编译临时工具软件包过程说明:主要列出重要信息(来自官方),切忌不要按照本文编译我只是把它们列出来加深理解,还有一些配置我没有列出包括make与make install Binutils-2.25 - Pass 1Binutils 软件包包括了一个链接器、汇编器 和 其它处理目标文件 的工具。 1234567../binutils-2.25/configure \ --prefix=/tools \ --with-sysroot=$LFS \ --with-lib-path=/tools/lib \ --target=$LFS_TGT \ --disable-nls \ --disable-werror 配置选项的含义： --prefix=/tools 告诉配置脚本将 Binutils 程序安装到 /tools 文件夹。 --with-sysroot=$LFS 用于交叉编译，告诉编译系统在 $LFS 中查找所需的目标系统库。 --with-lib-path=/tools/lib 指定需要配置使用的链接器的库路径。 --target=$LFS_TGT (这个还不是很清楚) 因为 LFS_TGT 变量中的机器描述和 config.guess 脚本返回的值略有不同，这个选项会告诉 configure 脚本调整 Binutils 的编译系统来编译一个交叉链接器。 --disable-nls 这会禁止国际化（i18n），因为国际化对临时工具来说没有必要。 --disable-werror 这会防止来自宿主编译器的警告事件导致停止编译。 GCC-7.3.0 - Pass 1GCC 软件包是 GNU 编译器 集合的一部分，其中包括 C 和 C++ 的编译器。 下面的指令将会修改 GCC 默认的动态链接器为安装在 /tools 文件夹中的。它也会从 GCC 的 include 搜索路径中移除 /usr/include 12345678910111213for file in \ $(find gcc/config -name linux64.h -o -name linux.h -o -name sysv4.h)do cp -uv $file&#123;,.orig&#125; sed -e &apos;s@/lib\(64\)\?\(32\)\?/ld@/tools&amp;@g&apos; \ -e &apos;s@/usr@/tools@g&apos; $file.orig &gt; $file echo &apos;#undef STANDARD_STARTFILE_PREFIX_1#undef STANDARD_STARTFILE_PREFIX_2#define STANDARD_STARTFILE_PREFIX_1 &quot;/tools/lib/&quot;#define STANDARD_STARTFILE_PREFIX_2 &quot;&quot;&apos; &gt;&gt; $file touch $file.origdone 如果上面的看起来难以理解，让我们分开来看一下吧。首先我们找到 gcc/config 文件夹下的所有命名为 linux.h, linux64.h 或sysv4.h 的文件。对于找到的每个文件，我们把它复制到相同名称的文件，但增加了后缀 “.orig”。然后第一个 sed 表达式在每个 “/lib/ld”, “/lib64/ld” 或 “/lib32/ld” 实例前面增加“/tools”，第二个 sed 表达式替换 “/usr” 的硬编码实例。然后，我们添加这改变默认 startfile 前缀到文件末尾的定义语句。注意 “/tools/lib/” 后面的 “/” 是必须的。最后，我们用 touch 更新复制文件的时间戳。当与 cp -u 一起使用时，可以防止命令被无意中运行两次造成对原始文件意外的更改。(厉害厉害) 8.2版本中不用以下命令(可能修复了这个错误):12GCC 不能正确检测栈保护，这会导致编译 Glibc-2.21 时出现问题，用下面的命令修复这个问题：sed -i &apos;/k prot/agcc_cv_libc_provides_ssp=yes&apos; gcc/configure 准备编译 GCC:1234567891011121314151617181920212223../gcc-4.9.2/configure \ --target=$LFS_TGT \ --prefix=/tools \ --with-sysroot=$LFS \ --with-newlib \ --without-headers \ --with-local-prefix=/tools \ --with-native-system-header-dir=/tools/include \ --disable-nls \ --disable-shared \ --disable-multilib \ --disable-decimal-float \ --disable-threads \ --disable-libatomic \ --disable-libgomp \ --disable-libitm \ --disable-libquadmath \ --disable-libsanitizer \ --disable-libssp \ --disable-libvtv \ --disable-libcilkrts \ --disable-libstdc++-v3 \ --enable-languages=c,c++ --with-newlib由于还没有可用的 C 库，这确保编译 libgcc 时定义了常数 inhibit_libc。这可以防止编译任何需要 libc 支持的代码。 --without-headers创建一个完成的交叉编译器的时候，GCC 要求标准头文件和目标系统兼容。对于我们的目的来说，不需要这些头文件。这个选项可以防止 GCC 查找它们。 --with-local-prefix=/toolsGCC 会查找本地已安装的 include 文件的系统位置。默认是 /usr/local。把它设置为 /tools 能把主机位置中的 /usr/local 从 GCC 的搜索路径中排除。 --with-native-system-header-dir=/tools/includeGCC 默认会在/usr/include 中查找系统头文件。和 sysroot 选项一起使用，会转换为 $LFS/usr/include。在后面两个章节中头文件会被安装到 $LFS/tools/include。这个选项确保 gcc 能正确找到它们。第二次编译 GCC 时，同样的选项可以保证不会去寻找主机系统的头文件。 --disable-shared这个选项强制 GCC 静态链接到它的内部库。我们这样做是为了避免与主机系统可能出现的问题。 --disable-decimal-float, --disable-threads, --disable-libatomic, --disable-libgomp, --disable-libitm, --disable-libquadmath, --disable-libsanitizer, --disable-libssp, --disable-libvtv, --disable-libcilkrts, --disable-libstdc++-v3这些选项取消了对十进制浮点数扩展、线程化、libatomic、 libgomp、 libitm、 libquadmath、 libsanitizer、 libssp、 libvtv、 libcilkrts 和 C++ 标准库的支持。这些功能在编译交叉编译器的时候会导致编译失败，对于交叉编译 临时 libc 来说也没有必要。 –disable-multilib在 x86_64 机器上， LFS 还不支持 multilib 配置。这个选项对 x86 来说无害。 –enable-languages=c,c++这个选项确保只编译 C 和 C++ 编译器。这些是现在唯一需要的语言。 Linux-4.15.3 API Headers供系统 C 库（在 LFS 中是 Glibc）使用的应用程序编程接口（API） Glibc-2.27Glibc 软件包包括主要的 C 库。这个库提供了基本的内存分配、文件夹搜素、读写文件、字符串处理、模式匹配、算术 等等例程。 编译:12345678910../glibc-2.21/configure \ --prefix=/tools \ --host=$LFS_TGT \ --build=$(../glibc-2.21/scripts/config.guess) \ --disable-profile \ --enable-kernel=2.6.32 \ --with-headers=/tools/include \ libc_cv_forced_unwind=yes \ libc_cv_ctors_header=yes \ libc_cv_c_cleanup=yes --host=$LFS_TGT, --build=$(../glibc-2.21/scripts/config.guess)这些选项的组合效果是 Glibc 的构建系统配置它自己用 /tools 里面的交叉链接器和交叉编译器交叉编译自己。 --disable-profile编译库但不包含分析信息。如果临时工具需要分析信息则忽略此选项。 --enable-kernel=3.2这告诉 Glibc 编译能支持 Linux 3.2 以及之后的内核库。更早的内核版本不受支持。 --with-headers=/tools/include告诉 Glibc 利用刚刚安装在 tools 文件夹中的头文件编译自身，此能够根据内核的具体特性提供更好的优化。 libc_cv_forced_unwind=yes在 “Binutils-2.25 - Pass 1” 中安装的链接器是交叉编译的，在安装完 Glibc 之前不能使用。由于依赖于工作的链接器，这意味着 force-unwind 支持的配置测试会失败。将 libccvforced_unwind=yes 变量传递进去告诉configure 命令 force-unwind 支持是可用的，不需要进行测试。 libc_cv_c_cleanup=yes类似的，我们传递 libc_cv_c_cleanup=yes 到 configure 脚本跳过测试就完成了 C 清理支持的配置。 libc_cv_ctors_header=yes类似的，我们传递 libc_cv_ctors_header=yes 到 configure 脚本跳过测试就完成了 gcc 构建器支持的配置。 Libstdc++-7.3.0Libstdc++ 是标准的 C++ 库。g++ 编译器正确运行需要它。Libstdc++ 是标准的 C++ 库。g++ 编译器正确运行需要它。 编译:123456789../gcc-4.9.2/libstdc++-v3/configure \ --host=$LFS_TGT \ --prefix=/tools \ --disable-multilib \ --disable-shared \ --disable-nls \ --disable-libstdcxx-threads \ --disable-libstdcxx-pch \ --with-gxx-include-dir=/tools/$LFS_TGT/include/c++/4.9.2 –host=…指示使用我们刚才编译的交叉编译器，而不是 /usr/bin 中的。 –disable-libstdcxx-threads由于我们还没有编译 C 线程库，C++ 的也还不能编译。 –disable-libstdcxx-pch此选项防止安装预编译文件，此步骤并不需要。 –with-gxx-include-dir=/tools/$LFS_TGT/include/c++/7.3.0这是 C++ 编译器搜索标准 include 文件的位置。在一般的编译中，这个信息自动从顶层文件夹中传入 Libstdc++ configure 选项。在我们的例子中，必须明确给出这信息。 Binutils-2.30 - Pass 2编译:123456789CC=$LFS_TGT-gcc \AR=$LFS_TGT-ar \RANLIB=$LFS_TGT-ranlib \../binutils-2.25/configure \ --prefix=/tools \ --disable-nls \ --disable-werror \ --with-lib-path=/tools/lib \ --with-sysroot CC=$LFS_TGT-gcc AR=$LFS_TGT-ar RANLIB=$LFS_TGT-ranlib因为这是真正的原生编译 Binutils，设置这些变量能确保编译系统使用交叉编译器和相关的工具，而不是宿主系统中已有的。 --with-lib-path=/tools/lib这告诉配置脚本在编译 Binutils 的时候指定库搜索目录，此处将 /tools/lib 传递到链接器。这可以防止链接器搜索宿主系统的库目录。 --with-sysrootsysroot 功能使链接器可以找到包括在其命令行中的其它共享对象明确需要的共享对象。 否则的话，在某些主机上一些软件包可能会编译不成功。(….未理解) 为下一章的“再调整”阶段准备链接器：123make -C ld cleanmake -C ld LIB_PATH=/usr/lib:/libcp -v ld/ld-new /tools/bin -C ld clean告诉 make 程序移除所有 ld 子目录中编译过的文件。 -C ld LIB_PATH=/usr/lib:/lib这个选项重新编译 ld 子目录中的所有文件。在命令行中指定 Makefile 的 LIB_PATH 变量可以使我们能够重写临时工具的默认值并指向正确的最终路径。该变量的值指定链接器的默认库搜索路径。 下一章中会用到这个准备。 GCC-7.3.0 - Pass 2我们第一次编译 GCC 的时候安装了一些内部系统头文件。其中的一个 limits.h 会反过来包括对应的系统头文件 limits.h， 在我们的例子中，是 /tools/include/limits.h。但是，第一次编译 gcc 的时候 /tools/include/limits.h 并不存在，因此 GCC 安装的内部头文件只是部分的自包含文件， 并不包括系统头文件的扩展功能。这足以编译临时 libc，但是这次编译 GCC 要求完整的内部头文件。 使用和正常情况下 GCC 编译系统使用的相同的命令创建一个完整版本的内部头文件： 12cat gcc/limitx.h gcc/glimits.h gcc/limity.h &gt; \ `dirname $($LFS_TGT-gcc -print-libgcc-file-name)`/include-fixed/limits.h 准备编译 GCC:12345678910111213CC=$LFS_TGT-gcc \CXX=$LFS_TGT-g++ \AR=$LFS_TGT-ar \RANLIB=$LFS_TGT-ranlib \../gcc-4.9.2/configure \ --prefix=/tools \ --with-local-prefix=/tools \ --with-native-system-header-dir=/tools/include \ --enable-languages=c,c++ \ --disable-libstdcxx-pch \ --disable-multilib \ --disable-bootstrap \ --disable-libgomp --enable-languages=c,c++这个选项确保编译了 C 和 C++ 编译器。 --disable-libstdcxx-pch不为 libstdc++ 编译预编译的头文件(PCH)。这会花费很多时间，却对我们没有用处。 --disable-bootstrap对于原生编译的 GCC，默认是做一个“引导”构建。这不仅会编译 GCC，而且会多次编译。 它用第一次编译的程序去第二次编译自己，然后同样进行第三次。 比较第二次和第三次迭代确保它可以完美复制自身。这也意味着已经成功编译。 但是，LFS 的构建方法能够提供一个稳定的编译器，而不需要每次都重新引导。 Tcl-core-8.6.8Tcl软件包包含工具命令语言（Tool Command Language）相关程序。 此软件包和后面三个包（Expect、DejaGNU 和 Check）用来为 GCC 和 Binutils还有其他的一些软件包的测试套件提供运行支持。仅仅为了测试目的而安装 4 个软件包，看上去有点奢侈，虽然因为大部分重要的工具都能正常工作而并不需要去做测试。 尽管在本章中并没有执行测试套件（并不做要求），但是在第六章 中都要求执行这些软件包自带的测试套件。不强求为本章中所构建的临时工具运行测试套件。 Expect-5.45Expect 软件包包含一个实现用脚本和其他交互式程序进行对话的程序。 首先，强制 Expect 的 configure 配置脚本使用 /bin/stty 替代宿主机系统里可能存在的 /usr/local/bin/stty。这样可以保证我们的测试套件工具在工具链的最后一次构建能够正常。12cp -v configure&#123;,.orig&#125;sed &apos;s:/usr/local/bin:/bin:&apos; configure.orig &gt; configure 编译:123./configure --prefix=/tools \ --with-tcl=/tools/lib \ --with-tclinclude=/tools/include --with-tcl=/tools/lib这个选项可以保证 configure 配置脚本会从临时工具目录里找 Tcl 的安装位置， 而不是在宿主机系统中寻找。 --with-tclinclude=/tools/include这个选项会给 Expect 显式地指定 Tcl 内部头文件的位置。通过这个选项可以避免 configure 脚本不能自动发现 Tcl 头文件位置的情况。 DejaGNU-1.6.1./configure --prefix=/tools M4-1.4.18M4 软件包包含一个宏预处理器。./configure --prefix=/tools Ncurses-6.1123456./configure --prefix=/tools \ --with-shared \ --without-debug \ --without-ada \ --enable-widec \ --enable-overwrite --without-ada这个选项会保证 Ncurse 不会编译对宿主机系统里可能存在的 Ada 编译器的支持， 而这在我们 chroot 切换环境后就不再可用。 --enable-overwrite这个选项会告诉 Ncurses 安装它的头文件到 /tools/include 目录， 而不是 /tools/include/ncurses 目录， 保证其他软件包可以正常找到 Ncurses 的头文件。 --enable-widec这个选项会控制编译宽字符库（比如，libncursesw.so.5.9） 而不是默认的普通库（比如，libncurses.so.5.9）。 这些宽字符库在多字节和传统的 8 位环境下使用，而普通库只能用于 8 位环境。 宽字符库和普通库的源代码是兼容的，但并不是二进制兼容。 Bash-4.4.18Bash 软件包包含 Bourne-Again SHell 终端程序编译:1./configure --prefix=/tools --without-bash-malloc --without-bash-malloc这个选项会禁用 Bash 的内存分配功能（malloc）， 这个功能已知会导致段错误。而禁用这个功能后，Bash 将使用 Glibc 的 malloc 函数，这样会更稳定。 Bison-3.0.4The Bison package contains a parser generator.(解析器生成器) Bzip2-1.0.6Bzip2 软件包包含压缩和解压文件的工具。 用 bzip2 压缩文本文件比传统的 gzip 压缩比高得多。 Coreutils-8.29Coreutils 软件包包含一套用于显示和设定基本系统属性的工具。12./configure --prefix=/tools --enable-install-program=hostname--enable-install-program=hostname 这个选项会允许编译和安装 hostname 程序 – 默认是不安装的但是 Perl 测试套件需要它。 Diffutils-3.6Diffutils软件包包含用来比较文件或目录之间差异的工具。 File-5.32File 软件包包含用来判断文件类型的工具。 Findutils-4.6.0Findutils 软件包包含用来查找文件的工具。这些工具可以用来在目录树中递归查找，或者创建、维护和搜索数据库（一般会比递归查找快，但是如果不经常更新数据库的话结果不可靠）。 Gawk-4.2.0Gawk 软件包包含处理文本文件的工具。 Gettext-0.19.8.1Gettext 软件包包含了国际化和本地化的相关应用。它支持程序使用 NLS（本地语言支持）编译，允许程序用用户本地语言输出信息。12cd gettext-toolsEMACS=&quot;no&quot; ./configure --prefix=/tools --disable-shared EMACS=&quot;no&quot;这个选项会禁止配置脚本侦测安装 Emacs Lisp 文件的位置，已知在某些系统中会引起错误。 --disable-shared这次我们不需要安装任何的 Gettext 动态库，所以不需要编译。 Grep-3.1Grep 软件包包含了在文件中搜索的工具。 Gzip-1.9Gzip 软件包包含压缩和解压缩文件的工具。 Make-4.2.1Make 软件包包含了一个用来编译软件包的程序。12./configure --prefix=/tools --without-guile--without-guile 这个选项会保证 Make 不会链接宿主系统上可能存在的 Guile 库，而在下一章里通过 chroot 切换环境后就不再可用 Patch-2.7.6Patch 软件包包含一个可以通过应用“补丁”文件来修改或创建文件的程序，补丁文件通常由diff程序生成。 Perl-5.26.1Perl 软件包包含了处理实用报表提取语言（Practical Extraction and Report Language）的程序。 Sed-4.4Sed 软件包包含一个字符流编辑器。 Tar-1.30Tar 软件包包含了一个存档工具。 Texinfo-6.5Texinfo软件包包含了读写和转换info文档的工具。 Util-linux-2.31.1Util-linux 软件包包含了各种各样的小工具。12345./configure --prefix=/tools \ --without-python \ --disable-makeinstall-chown \ --without-systemdsystemunitdir \ PKG_CONFIG=&quot;&quot; --without-python这个选项会禁止使用宿主系统中可能安装了的 Python。这样可以避免构建一些不必要的捆绑应用。 --disable-makeinstall-chown这个选项会禁止在安装的时候使用 chown 命令。这对我们安装到 /tools 目录没有意义而且可以避免使用 root 用户安装。 --without-systemdsystemunitdir对于使用 systemd 的系统，这个软件包会尝试安装 systemd 特定文件到 /tools 下一个不存在的目录里。这个选项可以避免这个不必要的动作。 PKG_CONFIG=&quot;&quot;设定这个环境变量可以避免增加一些宿主机上存在却不必要的功能。请注意这里设定环境变量的方式和 LFS 其他部分放在命令前面的方式不同。在这里是为了展示一下使用 configure 脚本配置时设定环境变量的另一种方式。 Xz-5.2.3Xz 软件包包含了用于压缩和解压文件的程序。它提供了对 lzma 和更新的 xz 压缩格式的支持。使用 xz 压缩文本文件能比传统的 gzip 或 bzip2 命令有更高的压缩比。 提醒$LFS/tools 目录可以在 LFS 系统构建完成后删除，但仍然可以保留下来用于构建额外的相同版本 LFS 系统。备份 $LFS/tools 目录到底有多少好处取决于你个人如果你想保留临时工具用来构建新的 LFS 系统，现在就要备份好。本书随后第六章中的指令将对当前的工具做些调整，导致在构建新系统时会失效 到此,准备工作完成]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-07-13 lfs编译日志]]></title>
    <url>%2F2018%2F07%2F13%2F2018-07-13%2F</url>
    <content type="text"><![CDATA[lfs准备工作已经全部完成 已经把编译的工作做完了 现在准备在写md文档总结一下(时间估计两天时间) 说实话,编译的工作是真的费时间]]></content>
      <categories>
        <category>lfs编译日志</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux From Scrach Pass 1]]></title>
    <url>%2F2018%2F07%2F12%2Flfs1%2F</url>
    <content type="text"><![CDATA[准备准备空间大概8g 准备在u盘上构建系统我准备将u盘对半分成两个盘一个安装系统,就用ext4一个当作普通的u盘来用记得分区的时候格式化可以使用fdisk 或者 其他工具 另外: 我的arch上没有创建swap分区 挂载U盘/移动硬盘使用fdisk -l命令得到123456789Disk /dev/sdb：29 GiB，31104958464 字节，60751872 个扇区单元：扇区 / 1 * 512 = 512 字节扇区大小(逻辑/物理)：512 字节 / 512 字节I/O 大小(最小/最佳)：512 字节 / 512 字节磁盘标签类型：dos磁盘标识符：0xcad4ebea设备 启动 起点 末尾 扇区 大小 Id 类型/dev/sdb4 * 63 60751871 60751809 29G c W95 FAT32 (LBA) 创建文件夹 将sdb4挂载到文件夹上 一般u盘会自动挂载,像我的系统就是挂载到run/s/xxx 下面不过官方建议还是挂载到 /mnt/lfs 下面 mount -v -t ext4 /dev/&lt;xxx&gt; $LFS # 将 /dev/ 挂载到 $LFS 如果重启设备，可能进入后发现 /mnt/lfs 目录下没有内容，这是只需要再次挂载 /dev/ 到 /mnt/lfs。 此处 用实际的设备名称代替 这里我只使用一个 / 分区 如果 LFS 使用了多个分区，(比如：一个 /，一个 /usr)，用下面的命令挂载它们： 1234mkdir -pv $LFS # 建立 / 分区的挂载点mount -v -t ext4 /dev/&lt;xxx&gt; $LFS # 将 /dev/&lt;xxx&gt; 挂载到 $LFSmkdir -v $LFS/usr # 建立 $LFS/usr 挂载点，用于挂载 /usrmount -v -t ext4 /dev/&lt;yyy&gt; $LFS/usr # 将 /dev/&lt;yyy&gt; 挂载到 $LFS/usr 附:如果你正在使用交换分区，用 swapon 命令确保它已经启用。 /sbin/swapon -v /dev/&lt;zzz&gt;用 swap 分区的名字替换。 注意:确保你的 echo $LFS 在root与用户帐号上时/mnt/lfs 你可以使用全局变量来设置 软件包和补丁mkdir -v $LFS/sources在$lfs里创建sources文件夹 来存放 下载好的软件包和补丁 设置目录的写权限和粘滞模式。“粘滞模式”意思是就算有多个用户对某个目录有写权限，仍然只有该文件的主人能删除一个粘滞目录里的文件。下面的命令可以打开写权限和粘滞模式：chmod -v a+wt $LFS/sources 下载软件包可以去中科院镜像里面去下载里面已经有了md5sums 解压放到source里面就行下载完成后使用md5sums检验====&gt;将下载的md5sums 放到sources目录下 123pushd $LFS/sourcesmd5sum -c md5sumspopd 最后的准备工作 我们还需要为构建临时系统做一些额外的准备工作。我们会在 $LFS 中新建一个文件夹用于临时工具的安装，增加一个非特权用户用于降低风险，并为该用户创建合适的构建环境。我们也会解释用于测量构建 LFS 软件包花费时间的单位，或者称为“标准编译单位（SBU）”，并给出一些关于软件包测试套件的信息 以 root 用户运行以下的命令来创建需要的文件夹：mkdir -v $LFS/tools 下一步是在宿主系统中创建 /tools 的符号链接，将其指向 LFS 分区中新建的文件夹。同样以 root 用户运行下面的命令： ln -sv $LFS/tools / 添加 LFS 用户当以 root 用户登录时，犯一个小错误可能会破坏或摧毁整个系统。因此，我们建议在本章中以非特权用户编译软件包。你可以使用你自己的用户名，但要容易的话，就建立一个干净的工作环境，创建一个名为 lfs 的新用户作为新组（名字也是 lfs ）的成员，并在安装过程中使用这个用户。以 root 用户运行以下命令来添加新用户：12groupadd lfsuseradd -s /bin/bash -g lfs -m -k /dev/null lfs 12345678910111213命令行选项的意思：-s /bin/bash把 bash 设置为 lfs 用户的默认 shell。-g lfs这个选项将用户 lfs 添加到组 lfs 中。-m为 lfs 用户创建主目录。-k /dev/null这个参数通过改变输入位置为特殊的空（null）设备，以防止可能从一个模版目录中（默认是 /etc/skel）复制文件。 lfs这是创建的组和用户的实际名称。 要以 lfs 用户身份登录（以 root 身份登录切换到 lfs 用户时不要求 lfs 用户设置了密码），需要给 lfs 用户一个密码：passwd lfs 通过更改文件夹所有者为 lfs，为用户 lfs 赋予了访问 $LFS/tools 文件夹的所有权限： chown -v lfs $LFS/tools 如果正如建议的一样创建了一个单独的工作目录，给 lfs 用户赋予这个文件夹的所有权：chown -v lfs $LFS/sources 下一步，以 lfs 用户身份登录。可以能通过一个虚拟控制台、显示控制器，或者下面的切换用户命令完成：su - lfs 设置环境123cat &gt; ~/.bash_profile &lt;&lt; &quot;EOF&quot;exec env -i HOME=$HOME TERM=$TERM PS1=&apos;\u:\w\$ &apos; /bin/bashEOF 当以 lfs 用户身份登录时，初始 shell 通常是一个可登录的 shell，它先读取宿主机的 /etc/profile文件（很可能包括一些设置和环境变量），然后是 .bash_profile 文件。.bash_profile 文件中的exec env -i.../bin/bash 命令用一个除了 HOME、TERM和 PS1 变量，完全空环境的 shell 代替运行中的 shell。这可以确保没有不必要的或者有潜在风险的环境变量从宿主机系统中泄露到构建环境。这里使用的技巧是为了有一个干净环境。 新的 shell 实例是一个非登录 shell，不会读取 /etc/profile 或者 .bash_profile文件，而是读取 .bashrc文件。 现在创建 .bashrc 文件：123456789cat &gt; ~/.bashrc &lt;&lt; &quot;EOF&quot;set +humask 022LFS=/mnt/lfsLC_ALL=POSIXLFS_TGT=$(uname -m)-lfs-linux-gnuPATH=/tools/bin:/bin:/usr/binexport LFS LC_ALL LFS_TGT PATHEOF set +h命令关闭了 bash的哈希功能。 哈希通常是一个有用的功能，bash 用一个哈希表来记录可执行文件的完整路径，以避免搜索PATH 的时间和又找到一个相同的可执行文件。然而，新工具要一安装后就使用。通过关闭哈希功能，一个程序准备运行时 shell 总是会搜索PATH变量。如此，shell 能在新编译的工具可用时马上在文件夹 $LFS/tools 中找到，而不是记录相同程序在其它地方的之前版本。 设置用户文件新建掩码（umask）为 022，确保新建的文件和目录只有它们自己的所有者可写，任何人都可读和可执行(假定open(2) 系统调用使用的默认模式是新文件使用 644模式，文件夹使用755模式)。 LFS 变量应该设置为选定的挂载点。 LC_ALL 变量控制特定程序的本地化，使得它们的消息能遵循特定国家的惯例。设置 LC_ALL 为 “POSIX” 或 “C”（两者是等价的），确保 chroot 环境中一切如期望的那样进行。 当编译我们的交叉编译器和链接器以及交叉编译我们的临时工具链时，LFS_TGT变量设置了一个非默认，但兼容的机器说明。5.2,“工具链技术说明”包含更多的信息。 把 /tools/bin 放在标准的 PATH 变量前面， 第五章中安装的软件一安装完成 shell 就可使用。这和关闭哈希功能一起，降低了在第五章环境中新程序可用时宿主机器使用旧程序的风险。 生效配置文件:source ~/.bash_profile 关于 SBUSBU 衡量方式如下。我们以第五章编译的第一个软件包 Binutils 所用时间作为一个标准编译单位（SBU），其它软件的生成时间都以其为标准进行比较。 例如，假如编译一个软件耗时 4.5 SBU，而编译安装初代 Binutils 用时 10 分钟的话，那么编译这个软件包大约耗时 45 分钟。当然啦，对于大多数人来说，编译 Binutils 可用不了 10 分钟那么久。 在一些情况下，使用多处理器同时编译可能失败，分析错误日志变得异常困难：因为不同处理器之间的执行路线是交错的。如果你在编译的时候遇到问题，那么请回过来使用单处理器编译，以正确地查看错误消息。 构建临时系统目标是生成一个临时的系统，它包含一个已知的较好工具集，该工具集可与宿主系统分离。通过使用 chroot，其余各章中的命令将被包含在此环境中，以保证目标 LFS 系统能够洁净且无故障地生成。该构建过程的设计就是为了使得新读者有最少的风险，同时还能有最好的指导价值。 最后确认一次是否正确设置了 LFS 环境变量：echo $LFS确认输出显示的是 LFS 分区挂载点的路径，在我们的例子中也就是 /mnt/lfs。 最后，必须强调两个重要的点： [重要] 重要 编译指南假定你已经正确地设置了宿主系统需求和符号链接： shell使用的是 bash。 sh 是到 bash的符号链接。 /usr/bin/awk 是到 gawk的符号链接。 /usr/bin/yacc 是到bison的符号链接或者一个执行 bison 的小脚本。 [重要] 重要 再次强调构建过程： 把所有源文件和补丁放到 chroot 环境可访问的目录，例如 /mnt/lfs/sources/。但是千万不能把源文件放在 /mnt/lfs/tools/ 中。 进入到源文件目录。 对于每个软件包: 用 tar 程序解压要编译的软件包。在第五章中，确保解压软件包时你使用的是 lfs 用户。2. 进入到解压后创建的目录中。4. 根据指南说明编译软件包。6. 回退到源文件目录。8. 除非特别说明，删除解压出来的目录和所有编译过程中生成的 &lt;package>-build 目录。 这里我在使用lfs帐号进入lfs文件夹的时候出现了权限不足,我使用了chmod加了权限 之后就是漫长的编译过程具体按照官方资料上进行编译注意在编译过程中必须清楚知道当前步骤 make 与 make install 防止错误 ====&gt;之后的构建过程 之后补充上 资料(建议配合使用)中文资料(已过期7.7,但翻译还是可以用)https://linux.cn/lfs/LFS-BOOK-7.7-systemd/index.html 英文资料(目前是8.2)http://www.linuxfromscratch.org/lfs/view/stable/index.html]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git443错误]]></title>
    <url>%2F2018%2F07%2F11%2Fgit443%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[在git push的时候出现OpenSSL SSL_connect: SSL_ERROR_SYSCALL in connection to github.com:443 网上的解决方法都不管用 解决方法: 换网络,连上手机热点,成功 =.=]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git上传脚本]]></title>
    <url>%2F2018%2F07%2F11%2Fgit%E4%B8%8A%E4%BC%A0%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[由于上传命令太多了,所以写了脚本命名为comgit12#!/bin/bashgit add --all;git commit -m &quot;$1&quot;;git pull;git push; 使用方法 comgit &quot;comment&quot;]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python脚本基础]]></title>
    <url>%2F2018%2F07%2F10%2Fpython%E8%84%9A%E6%9C%AC%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[数组类型tuple字典类型dict使用*tuple解析数组使用**dict解析字典传参数的时候 单个字符,数字由*args接受,形成tuple键值对有**kwargs接受,形成dict主函数if __name__ == &#39;__main__&#39;当然也可以使用sys.argv来读取参数]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell基础]]></title>
    <url>%2F2018%2F07%2F10%2Fshell%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[shell开头12345#!/bin/bash#!/bin/sh#!/usr/bin/awk#!/usr/bin/env python#!/usr/bin/perl 这几行在第一行,不再第一行就是注释不用的话就用相关解释器带上文件名执行在写脚本的时候如果不加前面的也会交给bash解释,但是还是指定解释器比较好 注意点一定义变量名的时候等号旁边没有空格 要在登录后显示初始化内容可以将脚本文件放到 /etc/profile.d/ 的下面或者是更改/etc/motd 文件 单引号中`` 命令无效 当参数大于9个要使用${}方式 touch 主要是用来更新时间的…. 使用cat连接两个文件 ln -s 源文件 快捷方式 s表示链接 有多个命令就使用;号,若将输出全都重定向用{}扩起来,{}是父shell,()是子shell 注意点二12345678read读到换行符为止,所以就使用循环可以将文件读完,不然好像只能读一行黑洞/dev/null 了解一下-.-cp mv rm 加上-i 进行确认`command` == $(command)`read file` 是要求用户输入并且保存到file变量中...`read -p &quot;&quot; ver `就像scanf一样..(我下所的)readonly xxx=1 或者readonly xxx(已定义) 表示只读无法改变包括unset用重定向代替输入 shell中0给了stdin 1给了stdout 2给了stderr 其余数字可以来关联输出文件 例如文件中&gt;&amp;3 命令用3&gt;文件 输出到文件中 exec 将文件关联paste两个文件连接tr替换if空格[] while空格: expand unexpand 将tab 转换成空格 一般只早开头 使用-a 替换全部 unset 消除定义的变量 ${array[*]} == ${array[@]} “”${array[*]} != “${array[@]}”前面是整个数组的字符串,后面时整个数组 123456789条件语句if or elif 后面要跟上 then 如:if [ xx ]then xxxxxelif [ xxx ]then xxxxxfi 12345678[ ]是shell的内置命令 下列给出操作符:-eq equal to-ne not equal to-lt less than-gt great thange (没有-哦) great or equal如果条件判断与then写在统一行then前面加;[]两侧要有空格 注意点三 在case中 中止case语句用;;如果用;&amp; or ;;&amp;来终止还是执行下去,他中止的是子句,会使用word在去匹配pattern,直到遇到;;结束case递增可以使用let x=$x+1 当然也可以使用expr语句-.-我也是随机选择的….do 和while [ ]写在一起的时候do前面有分号while 后面可以加上命令list 使用分号;来隔开,决定是否推出循环的是最后一个命令返回值,注意是最后一个!!!其中命令都是用[]扩起来的列入[];[];[][] 是条件-n String 判断字符串长度是否非0-z Stirng 判断字符串长度为0时就断(zero)String=StringString!=String 1234567for 后面不加in就选择传入的参数同时也可以写的像c的for一样但是写成要这样for((p=1,k=321;p&lt;10;p++,k++))do xxxxxdone 1234select var in xxx xxx xxx xxxdo xxxx(可以是case或者是if条件判断)done 在select中要写退出循环的条件,不然会一直循环下去,还有就是要写其他选项的条件,不然不科学-.- option 参数的代码123456while getopts x:x:x: OPTION 这个option可以随便定义do case $OPTION in x) xxx=$OPTAGE;; 这个变量时规定的 与$REPLY一样done 写的函数可以直接调用 条件中-r 可读 -w 可写 -a and12345$&#123;xx:-dsa&#125;等:(也可以通过其他方式来实现) :-先默认后赋值,不改变 :+先赋值,不改变 :=改变 :?若是空的则输出错误退出脚本 $PAGER 似乎是系统变量 显示文档的less也可以通过其他方式来实现 echo -n 不换行输出 -e 激活转移字符 sort -u 忽略相同行 当不能使用管道时可以使用命令替代参数 数字或者字符串expression不能出现在$()中,要用()扩起来后在放到$()中间…… 循环的话要注意变量的赋值 find 命令 从给定的文件位置开始找 -iname 忽略大小写-mtime 表示修改时间 -mtime n修改时间为n天-mtime +n 大于n天-mtime -n 小于n天 如果有多个命令使用-a来连接 其中如果有()要使用\ 来转义 -o 或者 xargs 可以处理很多参数 locate 查询本地的数据库 find是直接查找文件系统 dd if=file of=file count=blocks bs=bytes 信号 通过trap xx(函数名) 数字 可以将让函数才该信号出现的时候调用该函数使用trap也可以脚本结束后调用函数,数字是0使用’’ 或者: 是屏蔽中断INT信号使用trap INT 来恢复exit xx 中xx是信号kill 进程的时候要先kill掉子进程,不然会交给init进程接管使用&amp;&amp;连接多个命令的时候是一个一个执行的,会创建子bash执行进程 command|sed &#39;command&#39;sed &#39;command&#39; file 使用;分号来连接多个command(不用另起’’) 或者使用-e(要’’分离每个命令)也可以将多个command写在一个文件中,-f指定文件!可以对取值范围取反sed stream editor 可以从管道或者文件中读取参数 sed -n &#39;1~2p&#39; 1 start 2 step sed还能加上正则/^$/使用正则还能代替行号使用&amp;表示正则表达式获取的结果 awk ‘script’ files12345678910&apos;script&apos; 中一般为 /pattern/&#123;action&#125; exprssion &#123;action&#125;其中特殊的有: value ~ /pattern/ 与正则匹配 加个!就是不匹配 可以使用()将判断分离,这样()间可以使用||或者&amp;&amp;链接分割符通过-F指定可以在awk文件中写好命令之后使用-f来指定文件使用BEGIN&#123;&#125;来执行处理数据之前执行的初始化操作使用next来表示 将两个命令的结果连接起来 #!/bin/bash/awk -f awk脚本 .awk awk中有内建变量 NR为行数 FS分割符 等等]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim基础]]></title>
    <url>%2F2018%2F07%2F10%2Fvim%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[ctrl + s 卡死后使用 ctrl + q z回车 将光标所在行移动到屏幕顶端 z. 将光标所在行移动到屏幕中间 z- 将光标所在行移动到屏幕低端]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql基本知识]]></title>
    <url>%2F2018%2F07%2F10%2Fsql%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[当我在将大小写忽略的时候之前使用大写的数据库的名字就进不去了,因为系统自动将我打的大写换成了小写….. 每一个检查点都会记录他自己的操作对象的信息状态,当共用的是同一个硬盘上的数据,硬盘上的数据是会被最近的检查点所覆盖掉的,当要回复在之前的某一个特定的检查点的时候,是先通过最近的检查点来逐级向上恢复的 当colume 与condition 条件相等时结果为result123456case colume when condition then result when condition then result when condition then resultelse resultend 当满足某一条件时，执行某一result123456case when condition then result when condition then result when condition then resultelse resultend 当满足某一条件时，执行某一result,把该结果赋值到new_column_name 字段中123456case when condition then result when condition then result when condition then resultelse resultend new_column_name case when 用在select 语句中，新的字段new_column_name可以用来排序，但是不能用在where中]]></content>
      <categories>
        <category>sql</category>
      </categories>
      <tags>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git基本操作]]></title>
    <url>%2F2018%2F07%2F10%2Fgit%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[一般提交操作为:1234git add -all ==&gt; gaagit commit -m &quot;xxxx&quot; ==&gt; gcgit pull ==&gt; glgit push ==&gt; gp 自动保存密码:git config --global credential.helper store]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[css渐变动画]]></title>
    <url>%2F2018%2F07%2F10%2Fcss%E6%B8%90%E5%8F%98%E5%8A%A8%E7%94%BB%2F</url>
    <content type="text"><![CDATA[鼠标放上去渐变12345678div&#123;width:100px;transition: width 2s;-moz-transition: width 2s; /* Firefox 4 */-webkit-transition: width 2s; /* Safari 和 Chrome */-o-transition: width 2s; /* Opera */&#125; 使div居中:要设置div的宽度 再使用margin 0 auto设置]]></content>
      <categories>
        <category>css</category>
      </categories>
      <tags>
        <tag>css</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于pacman]]></title>
    <url>%2F2018%2F07%2F10%2Fpacman%2F</url>
    <content type="text"><![CDATA[pacman -D --asexplicite xxx 设置为单独安装的包 pacman -Qtd 出来的包可以删 但有些删除可能会有影响,大多没用 pacman -Rscn 可以删除干净包 pacman -Rdd 是在出现了依赖问题的时候用用,一般就不用它,他是强行破坏 依赖关系 有些依赖关系不一定会长久存在,所以pacman -Qtd不是删除干净包就一定没有东西的 可能会在你装包的时候出现依赖的更新 pacman不会自动帮你删除孤包 Rdd所操作的对象往往是这个包同时被其他的包依赖， 如果你正常情况下用R去卸它是会报错提示破坏依赖的。 比如一条依赖树a-b-c，即c依赖b，b又依赖a， 那么如果你用R或者Rs去卸载b就会报错提示你c的依赖将被破坏所以不能卸。 如果是用Rdd卸载b就是不管谁依赖它就是强行把b一个东西删掉， 本质上是临时破坏一下依赖。这个做法往往是出了什么问题才会用的。 卸载同时删配置文件的参数是-n，-c的意思是同时把依赖它的包也卸载掉 总结一下 对于a-b-c这样一条依赖树且a和b都是作为依赖安装的， 那么如果使用-R、-Rs去卸载b就会报错提示c的依赖被破坏并中断操作， 使用-Rdd卸载b就会强行删b，同时a和c被保留。 使用-Rc卸b就会把b和c卸载掉，用-Rsc卸b就会把abc全卸掉 ####pacman基本用法 1234567891011121314151617181920212223pacman -Sy abc #和源同步后安装名为abc的包pacman -S abc #从本地数据库中得到abc的信息，下载安装abc包pacman -Sf abc #强制安装包abcpacman -Ss abc #搜索有关abc信息的包pacman -Si abc #从数据库中搜索包abc的信息pacman -Q #列出已经安装的软件包pacman -Q abc #检查 abc 软件包是否已经安装pacman -Qi abc #列出已安装的包abc的详细信息pacman -Ql abc #列出abc软件包的所有文件pacman -Qo /path/to/abc #列出abc文件所属的软件包pacman -Syu #同步源，并更新系统pacman -Sy #仅同步源pacman -Su #更新系统pacman -R abc #删除abc包pacman -Rd abc #强制删除被依赖的包pacman -Rc abc #删除abc包和依赖abc的包pacman -Rsc abc #删除abc包和abc依赖的包pacman -Sc #清理/var/cache/pacman/pkg目录下的旧包pacman -Scc #清除所有下载的包和数据库pacman -U abc #安装下载的abs包，或新编译的abc包pacman -Sd abc #忽略依赖性问题，安装包abcpacman -Su --ignore foo #升级时不升级包foopacman -Sg abc #查询abc这个包组包含的软件包]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
</search>
