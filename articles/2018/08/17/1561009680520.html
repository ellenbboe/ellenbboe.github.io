<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0"/><meta name="theme-color" content="#3b3e43"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=no"/><title>Scrapy Pass 3 - Kosmos</title><meta name="description" content="Learn Scrapy pass 3 接上文Extract data Extracting quotes and authors Now that you know a bit about selection and extraction, let’s complete our spider by writing the code to extract the quotes from the web page. Each quote in http://quotes.toscrape.comis represented by HTML elements that look like this: &lt;div class=&quot;quote&quot;&gt; &lt;span class=&quot;text&quot;&gt;“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”&lt;/span&gt; &lt;span&gt; by &amp;l...."/><meta property="og:description" content="Learn Scrapy pass 3 接上文Extract data Extracting quotes and authors Now that you know a bit about selection and extraction, let’s complete our spider by writing the code to extract the quotes from the web page. Each quote in http://quotes.toscrape.comis represented by HTML elements that look like this: &lt;div class=&quot;quote&quot;&gt; &lt;span class=&quot;text&quot;&gt;“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”&lt;/span&gt; &lt;span&gt; by &amp;l...."/>    <meta name="keywords" content="开源,开发者,笔记,kosmos"/><link rel="dns-prefetch" href="https://ellenbboe.github.io"/><link rel="dns-prefetch" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="https://ellenbboe.github.io"><link rel="icon" type="image/png" href="https://s2.ax1x.com/2020/01/12/loApPe.png"/><link rel="apple-touch-icon" href="https://s2.ax1x.com/2020/01/12/loApPe.png"><link rel="shortcut icon" type="image/x-icon" href="https://s2.ax1x.com/2020/01/12/loApPe.png"><meta name="copyright" content="B3log"/><meta http-equiv="Window-target" content="_top"/><meta property="og:locale" content="zh_CN"/><meta property="og:title" content="Scrapy Pass 3 - Kosmos"/><meta property="og:site_name" content="Kosmos"/><meta property="og:url"      content="https://ellenbboe.github.io/articles/2018/08/17/1561009680520.html?"/><meta property="og:image" content="https://s2.ax1x.com/2020/01/12/loApPe.png"/><link rel="search" type="application/opensearchdescription+xml" title="Scrapy Pass 3 - Kosmos" href="/opensearch.xml"><link href="https://ellenbboe.github.io/rss.xml" title="RSS" type="application/rss+xml" rel="alternate"/><link rel="manifest" href="https://ellenbboe.github.io/manifest.json">        <link rel="canonical" href="https://ellenbboe.github.io/articles/2018/08/17/1561009680520.html">        <link rel="stylesheet"
              href="https://ellenbboe.github.io/skins/Casper/css/base.css?1578931371974"/>
            <link rel="prev" title="Scrapy Pass 2" href="https://ellenbboe.github.io/articles/2018/08/16/1561009675671.html">
            <link rel="next" title="2018-08-17" href="https://ellenbboe.github.io/articles/2018/08/17/1561009684132.html">
    </head>
<body class="fn__flex-column">
<div id="pjax" class="fn__flex-1">
    
    <header class="header header--article">
        <div class="wrapper header__title">
            <h1 class="header__h1 fn__flex-inline">
                <img src="https://s2.ax1x.com/2020/01/12/loApPe.png">
                <a href="https://ellenbboe.github.io" rel="start" class="header__title">Kosmos</a>
            </h1>
            <h2 class="header__h2">---就算是痛苦难受，也要去体会那被称为生存的分量---</h2>
        </div>
        <nav class="wrapper header__nav fn__clear">
            <a href="https://ellenbboe.github.io" rel="start">
                    Kosmos
            </a>

                <a class="fn__flex-inline" href="/my-github-repos" target="_self" rel="section">
                    <img src="/images/github-icon.png"> 我的开源
                </a>
                <a class="fn__flex-inline" href="/readlist" target="_self" rel="section">
                     阅读列表
                </a>


            <div class="fn__right">
    <a href="https://hacpai.com/member/ellenbboe"
       title="https://hacpai.com/member/ellenbboe"
       class="user__site"
       target="_blank" rel="noopener nofollow">
        <svg viewBox="0 0 32 32" width="100%" height="100%">
            <path fill="#d23f31" style="fill: var(--color1, #d23f31)" d="M5.787 17.226h17.033l5.954 9.528c0.47 0.752 0.003 1.361-1.042 1.361h-15.141z"></path>
            <path d="M10.74 3.927h17.033c1.045 0 1.512 0.609 1.042 1.361l-5.954 9.528h-19.872l6.379-10.209c0.235-0.376 0.849-0.681 1.372-0.681z"></path>
            <path d="M2.953 17.226h2.839l6.804 10.889h-1.892c-0.523 0-1.137-0.305-1.372-0.681z"></path>
        </svg>
    </a>

        <a href="https://github.com/ellenbboe"
           title="https://github.com/ellenbboe"
           class="user__site"
           target="_blank" rel="noopener nofollow">
            <svg viewBox="0 0 32 32" width="100%" height="100%">
                <path d="M16 0.331c-8.836 0-16 7.163-16 16 0 7.069 4.585 13.067 10.942 15.182 0.8 0.148 1.094-0.347 1.094-0.77 0-0.381-0.015-1.642-0.022-2.979-4.452 0.968-5.391-1.888-5.391-1.888-0.728-1.849-1.776-2.341-1.776-2.341-1.452-0.993 0.11-0.973 0.11-0.973 1.606 0.113 2.452 1.649 2.452 1.649 1.427 2.446 3.743 1.739 4.656 1.33 0.143-1.034 0.558-1.74 1.016-2.14-3.554-0.404-7.29-1.777-7.29-7.907 0-1.747 0.625-3.174 1.649-4.295-0.166-0.403-0.714-2.030 0.155-4.234 0 0 1.344-0.43 4.401 1.64 1.276-0.355 2.645-0.532 4.005-0.539 1.359 0.006 2.729 0.184 4.008 0.539 3.054-2.070 4.395-1.64 4.395-1.64 0.871 2.204 0.323 3.831 0.157 4.234 1.026 1.12 1.647 2.548 1.647 4.295 0 6.145-3.743 7.498-7.306 7.895 0.574 0.497 1.085 1.47 1.085 2.963 0 2.141-0.019 3.864-0.019 4.391 0 0.426 0.288 0.925 1.099 0.768 6.354-2.118 10.933-8.113 10.933-15.18 0-8.837-7.164-16-16-16z"></path>
            </svg>
        </a>

                <a rel="alternate" href="https://ellenbboe.github.io/rss.xml">
                    RSS
                </a>

            </div>
        </nav>
    </header>
    <div class="article__top">
        <div class="fn__clear">
            <div class="toc fn__none" onclick="$('.post__toc').slideToggle()">目录</div>
            <div class="title fn__pointer" onclick="Util.goTop()">Scrapy Pass 3</div>
<div class="article__share"
     data-title="Scrapy Pass 3"
     data-blogtitle="Kosmos"
     data-url="https://ellenbboe.github.io/articles/2018/08/17/1561009680520.html"
     data-avatar="https://img.hacpai.com/avatar/1560335521987_1576387741689.jpeg?imageView2/1/w/128/h/128/interlace/0/q/100">
    <span class="item" data-type="qqz">
        <svg viewBox="0 0 32 32" width="100%" height="100%">
            <path d="M22.824 13.989l-8.348 6.287s3.351 0.522 8.404 0.461l-0.23-1.040 7.2-6.549c0.132-0.12 0.183-0.312 0.129-0.487s-0.203-0.299-0.377-0.314l-9.492-0.856-3.708-9.213c-0.068-0.169-0.226-0.279-0.401-0.279s-0.333 0.11-0.401 0.279l-3.708 9.213-9.492 0.856c-0.174 0.015-0.323 0.139-0.377 0.314s-0.004 0.366 0.129 0.487l7.2 6.549-2.158 9.742c-0.040 0.178 0.026 0.365 0.168 0.474 0.142 0.107 0.331 0.115 0.481 0.021l8.158-5.165 8.158 5.165c0.070 0.045 0.147 0.066 0.225 0.066 0.090 0 0.18-0.029 0.256-0.086 0.142-0.109 0.208-0.295 0.168-0.474l-1.707-7.704c0.732-0.386 1.538-1.040 1.538-1.040s-3.195 1.638-14.664 0.838l8.312-6.325s-0.327-0.534-10.744-0.914c-0.697-0.026 8.493-1.83 15.281-0.305z"></path>
        </svg>
    </span>
    <span class="item" data-type="wechat">
        <svg viewBox="0 0 32 32" width="100%" height="100%">
            <path d="M9.062 9.203c0-0.859-0.562-1.422-1.422-1.422-0.844 0-1.703 0.562-1.703 1.422 0 0.844 0.859 1.406 1.703 1.406 0.859 0 1.422-0.562 1.422-1.406zM20.672 17.125c0-0.562-0.562-1.125-1.422-1.125-0.562 0-1.125 0.562-1.125 1.125 0 0.578 0.562 1.141 1.125 1.141 0.859 0 1.422-0.562 1.422-1.141zM16.984 9.203c0-0.859-0.562-1.422-1.406-1.422-0.859 0-1.703 0.562-1.703 1.422 0 0.844 0.844 1.406 1.703 1.406 0.844 0 1.406-0.562 1.406-1.406zM26.906 17.125c0-0.562-0.578-1.125-1.422-1.125-0.562 0-1.125 0.562-1.125 1.125 0 0.578 0.562 1.141 1.125 1.141 0.844 0 1.422-0.562 1.422-1.141zM22.75 10.922c-0.359-0.047-0.719-0.063-1.094-0.063-5.375 0-9.625 4.016-9.625 8.953 0 0.828 0.125 1.625 0.359 2.375-0.359 0.031-0.703 0.047-1.063 0.047-1.422 0-2.547-0.281-3.969-0.562l-3.953 1.984 1.125-3.406c-2.828-1.984-4.531-4.547-4.531-7.656 0-5.391 5.094-9.625 11.328-9.625 5.563 0 10.453 3.391 11.422 7.953zM32 19.687c0 2.547-1.688 4.813-3.969 6.516l0.859 2.828-3.109-1.703c-1.141 0.281-2.281 0.578-3.406 0.578-5.391 0-9.625-3.688-9.625-8.219s4.234-8.219 9.625-8.219c5.094 0 9.625 3.688 9.625 8.219z"></path>
        </svg>
    </span>
    <span class="item" data-type="weibo">
        <svg viewBox="0 0 32 32" width="100%" height="100%">
            <path d="M13.444 27.064c-5.3 0.525-9.875-1.875-10.219-5.35-0.344-3.481 3.675-6.719 8.969-7.244 5.3-0.525 9.875 1.875 10.212 5.35 0.35 3.481-3.669 6.725-8.963 7.244zM24.038 15.521c-0.45-0.137-0.762-0.225-0.525-0.819 0.512-1.287 0.563-2.394 0.006-3.188-1.038-1.481-3.881-1.406-7.137-0.037 0 0-1.025 0.444-0.762-0.363 0.5-1.613 0.425-2.956-0.356-3.737-1.769-1.769-6.469 0.069-10.5 4.1-3.013 3.006-4.763 6.212-4.763 8.981 0 5.287 6.787 8.506 13.425 8.506 8.7 0 14.494-5.056 14.494-9.069 0-2.431-2.044-3.806-3.881-4.375z"></path>
            <path d="M29.819 5.833c-2.1-2.331-5.2-3.219-8.063-2.612v0c-0.663 0.144-1.081 0.794-0.938 1.45 0.144 0.662 0.788 1.081 1.45 0.938 2.038-0.431 4.238 0.2 5.731 1.856s1.9 3.913 1.256 5.888v0c-0.206 0.644 0.144 1.331 0.788 1.544 0.644 0.206 1.331-0.144 1.544-0.787v-0.006c0.9-2.762 0.331-5.938-1.769-8.269z"></path>
            <path d="M26.588 8.752c-1.025-1.138-2.538-1.569-3.925-1.269-0.569 0.119-0.931 0.688-0.813 1.256 0.125 0.569 0.688 0.931 1.25 0.806v0c0.681-0.144 1.419 0.069 1.919 0.619 0.5 0.556 0.637 1.313 0.419 1.975v0c-0.175 0.55 0.125 1.15 0.681 1.331 0.556 0.175 1.15-0.125 1.331-0.681 0.438-1.356 0.163-2.906-0.863-4.037z"></path>
            <path d="M13.738 20.771c-0.188 0.319-0.594 0.469-0.912 0.337-0.319-0.125-0.412-0.488-0.231-0.794 0.188-0.306 0.581-0.456 0.894-0.337 0.313 0.113 0.425 0.469 0.25 0.794zM12.044 22.933c-0.512 0.819-1.613 1.175-2.438 0.8-0.813-0.369-1.056-1.319-0.544-2.119 0.506-0.794 1.569-1.15 2.388-0.806 0.831 0.356 1.1 1.3 0.594 2.125zM13.969 17.146c-2.519-0.656-5.369 0.6-6.463 2.819-1.119 2.262-0.037 4.781 2.506 5.606 2.638 0.85 5.75-0.456 6.831-2.894 1.069-2.394-0.262-4.85-2.875-5.531z"></path>
        </svg>
    </span>
    <span class="item" data-type="twitter">
        <svg viewBox="0 0 32 32" width="100%" height="100%">
            <path d="M32.003 6.075c-1.175 0.525-2.444 0.875-3.769 1.031 1.356-0.813 2.394-2.1 2.887-3.631-1.269 0.75-2.675 1.3-4.169 1.594-1.2-1.275-2.906-2.069-4.794-2.069-3.625 0-6.563 2.938-6.563 6.563 0 0.512 0.056 1.012 0.169 1.494-5.456-0.275-10.294-2.888-13.531-6.862-0.563 0.969-0.887 2.1-0.887 3.3 0 2.275 1.156 4.287 2.919 5.463-1.075-0.031-2.087-0.331-2.975-0.819 0 0.025 0 0.056 0 0.081 0 3.181 2.263 5.838 5.269 6.437-0.55 0.15-1.131 0.231-1.731 0.231-0.425 0-0.831-0.044-1.237-0.119 0.838 2.606 3.263 4.506 6.131 4.563-2.25 1.762-5.075 2.813-8.156 2.813-0.531 0-1.050-0.031-1.569-0.094 2.913 1.869 6.362 2.95 10.069 2.95 12.075 0 18.681-10.006 18.681-18.681 0-0.287-0.006-0.569-0.019-0.85 1.281-0.919 2.394-2.075 3.275-3.394z"></path>
        </svg>
    </span>
    <span class="item__qr"></span>
</div>        </div>
        <progress class="article__progress"></progress>
    </div>
    <div class="article">
        <div class="ft__center">
            <div class="item__meta">
                <time>
                    2018-08-17
                </time>
                /
                    <a class="tag" rel="tag"
                       href="https://ellenbboe.github.io/tags/Scrapy">Scrapy</a> &nbsp;
            </div>
            <h2 class="item__title">
                Scrapy Pass 3
            </h2>
        </div>
        <div class="item__cover" style="background-image: url(https://img.hacpai.com/bing/20180131.jpg?imageView2/1/w/1280/h/720/interlace/1/q/100)"></div>
        <div class="wrapper">
            <section class="item__content item__content--article vditor-reset">
                <p></p><h2 id="b3_solo_h2_0">Learn Scrapy pass 3</h2>
<h3 id="b3_solo_h3_1">接上文Extract data</h3>
<h4 id="b3_solo_h4_2">Extracting quotes and authors</h4>
<p>Now that you know a bit about <code>selection</code> and <code>extraction</code>, let’s complete our spider by writing the code to extract the quotes from the web page.</p>
<p>Each quote in <code>http://quotes.toscrape.com</code>is represented by HTML elements that look like this:</p>
<pre><code>&lt;div class="quote"&gt;
    &lt;span class="text"&gt;“The world as we have created it is a process of our
    thinking. It cannot be changed without changing our thinking.”&lt;/span&gt;
    &lt;span&gt;
        by &lt;small class="author"&gt;Albert Einstein&lt;/small&gt;
        &lt;a href="/author/Albert-Einstein"&gt;(about)&lt;/a&gt;
    &lt;/span&gt;
    &lt;div class="tags"&gt;
        Tags:
        &lt;a class="tag" href="/tag/change/page/1/"&gt;change&lt;/a&gt;
        &lt;a class="tag" href="/tag/deep-thoughts/page/1/"&gt;deep-thoughts&lt;/a&gt;
        &lt;a class="tag" href="/tag/thinking/page/1/"&gt;thinking&lt;/a&gt;
        &lt;a class="tag" href="/tag/world/page/1/"&gt;world&lt;/a&gt;
    &lt;/div&gt;
&lt;/div&gt;
</code></pre>
<p>Let’s open up scrapy shell and play a bit to find out how to extract the data we want:<br>
<code>$ scrapy shell 'http://quotes.toscrape.com'</code><br>
We get a <strong>list</strong> of selectors for the quote HTML elements with:</p>
<pre><code>&gt;&gt;&gt; response.css("div.quote")
</code></pre>
<p>Each of the selectors <strong>returned by the query</strong> above allows us to run further queries over their sub-elements. Let’s assign the first selector to a variable, so that we can run our CSS selectors directly on a particular quote:</p>
<pre><code>&gt;&gt;&gt; quote = response.css("div.quote")[0]
</code></pre>
<p>Now, let’s extract title, author and the tags from that quote using the quote object we just created:</p>
<pre><code>&gt;&gt;&gt; title = quote.css("span.text::text").extract_first()
&gt;&gt;&gt; title
'“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”'
&gt;&gt;&gt; author = quote.css("small.author::text").extract_first()
&gt;&gt;&gt; author
'Albert Einstein'
</code></pre>
<p>Given that the tags are a <strong>list</strong> of strings, we can use the <code>.extract()</code> method to get all of them:</p>
<pre><code>&gt;&gt;&gt; tags = quote.css("div.tags a.tag::text").extract()
&gt;&gt;&gt; tags
['change', 'deep-thoughts', 'thinking', 'world']
</code></pre>
<p>Having figured out how to extract each bit, we can now iterate over all the quotes elements and put them together into a Python dictionary:</p>
<pre><code>&gt;&gt;&gt; for quote in response.css("div.quote"):
...     text = quote.css("span.text::text").extract_first()
...     author = quote.css("small.author::text").extract_first()
...     tags = quote.css("div.tags a.tag::text").extract()
...     print(dict(text=text, author=author, tags=tags))
{'tags': ['change', 'deep-thoughts', 'thinking', 'world'], 'author': 'Albert Einstein', 'text': '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”'}
{'tags': ['abilities', 'choices'], 'author': 'J.K. Rowling', 'text': '“It is our choices, Harry, that show what we truly are, far more than our abilities.”'}
    ... a few more of these, omitted for brevity
&gt;&gt;&gt;
</code></pre>
<h4 id="b3_solo_h4_3">Extracting data in our spider</h4>
<p>Let’s get back to our spider. Until now, it doesn’t extract any data in particular, just saves the whole HTML page to a local file <em>(额)</em> . Let’s integrate the extraction logic(逻辑) above into our spider.</p>
<p>A Scrapy spider typically <strong>generates many dictionaries</strong> containing the data extracted from the page. To do that, we use the <strong>yield</strong> Python keyword in the callback, as you can see below:</p>
<pre><code>import scrapy
class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
        'http://quotes.toscrape.com/page/2/',
    ]

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').extract_first(),
                'author': quote.css('small.author::text').extract_first(),
                'tags': quote.css('div.tags a.tag::text').extract(),
            }
            (yield 不断调用,似乎不用储存)
</code></pre>
<p>If you run this spider, it will output the extracted data with the log:</p>
<pre><code>2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://quotes.toscrape.com/page/1/&gt;
{'tags': ['life', 'love'], 'author': 'André Gide', 'text': '“It is better to be hated for what you are than to be loved for what you are not.”'}
2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://quotes.toscrape.com/page/1/&gt;
{'tags': ['edison', 'failure', 'inspirational', 'paraphrased'], 'author': 'Thomas A. Edison', 'text': "“I have not failed. I've just found 10,000 ways that won't work.”"}
</code></pre>
<h4 id="b3_solo_h4_4">Storing the scraped data</h4>
<p>The simplest way to store the scraped data is by using <strong>Feed exports</strong>, with the following command:<br>
<code>scrapy crawl quotes -o quotes.json</code></p>
<p>That will generate an quotes.json file containing all scraped items, serialized in <code>JSON</code>.</p>
<p>For historic reasons, Scrapy <strong>appends to</strong> a given file <strong>instead of overwriting</strong> its contents. If you run this command twice without removing the file before the second time, you’ll end up with a broken JSON file.<br>
<strong>(两次使用会损坏json文件)</strong><br>
You can also use other formats, like JSON Lines:<br>
<code>scrapy crawl quotes -o quotes.jl</code></p>
<blockquote>
<p>The JSON Lines format is useful because it’s <strong>stream-like</strong>, <strong>you can easily append new records to it.</strong> It doesn’t have the same problem of JSON when you run twice. Also, <strong>as each record is a separate line</strong>, you can process big files without having to fit everything in memory, there are tools like JQ to help doing that at the command-line.</p>
</blockquote>
<p>In small projects (like the one in this tutorial), that should be enough. However, if you want to <strong>perform more complex things with the scraped items, you can write an Item Pipeline</strong>. A placeholder file for Item Pipelines has been set up for you when the project is created, in tutorial/pipelines.py.<strong>(预先创建好了pipelines文件)</strong> Though you <strong>don’t need to implement any item pipelines</strong> if you just want to store the scraped items.</p>
<h4 id="b3_solo_h4_5">Following links</h4>
<p>Let’s say, instead of just scraping the stuff from the first two pages from <code>http://quotes.toscrape.com</code>, you want quotes from all the pages in the website.</p>
<p>Now that you know how to extract data from pages, let’s see how to follow links from them.</p>
<p>First thing is to extract the link to the page we want to follow. Examining our page, we can see there is a link to the next page with the following markup:</p>
<pre><code>&lt;ul class="pager"&gt;
    &lt;li class="next"&gt;
        &lt;a href="/page/2/"&gt;Next &lt;span aria-hidden="true"&gt;&amp;rarr;&lt;/span&gt;&lt;/a&gt;
    &lt;/li&gt;
&lt;/ul&gt;
</code></pre>
<p>We can try extracting it in the shell:</p>
<pre><code>&gt;&gt;&gt; response.css('li.next a').extract_first()
'&lt;a href="/page/2/"&gt;Next &lt;span aria-hidden="true"&gt;→&lt;/span&gt;&lt;/a&gt;'
</code></pre>
<p>This gets the anchor element, but we want the attribute href. For that, Scrapy supports a CSS extension that let’s you select the attribute contents, like this:</p>
<pre><code>&gt;&gt;&gt; response.css('li.next a::attr(href)').extract_first()
'/page/2/'
</code></pre>
<p>Let’s see now our spider modified to recursively follow the link to the next page, extracting data from it:</p>
<pre><code>import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
    ]

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').extract_first(),
                'author': quote.css('small.author::text').extract_first(),
                'tags': quote.css('div.tags a.tag::text').extract(),
            }

        next_page = response.css('li.next a::attr(href)').extract_first()
        if next_page is not None:
            next_page = response.urljoin(next_page)
            yield scrapy.Request(next_page, callback=self.parse)
</code></pre>
<p>Now, after extracting the data, the parse() method looks for the link to the next page, builds a full absolute URL <strong>using the urljoin() method</strong> (since the links can be relative) and yields a new request to the next page, registering itself <strong>as callback</strong> to <strong>handle the data</strong> extraction for the next page and to keep the crawling going through all the pages.</p>
<p>What you see here is Scrapy’s mechanism(机制) of following links: <strong>when you yield a Request in a callback method, Scrapy will schedule that request to be sent and register a callback method to be executed when that request finishes.</strong></p>
<p>Using this, you can build complex crawlers that follow links according to rules you define, and extract different kinds of data depending on the page it’s visiting.</p>
<p>In our example, it creates a sort of loop, following all the links to the next page until it doesn’t find one – handy for crawling blogs, forums and other sites with pagination.</p>
<h4 id="b3_solo_h4_6">A shortcut for creating Requests</h4>
<p>As a shortcut for creating Request objects you can use response.follow:</p>
<pre><code>import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
    ]

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').extract_first(),
                'author': quote.css('span small::text').extract_first(),
                'tags': quote.css('div.tags a.tag::text').extract(),
            }

        next_page = response.css('li.next a::attr(href)').extract_first()
        if next_page is not None:
            yield response.follow(next_page, callback=self.parse)
</code></pre>
<p>Unlike scrapy.Request, <strong>response.follow supports relative URLs directly - no need to call urljoin</strong>.(震惊,似乎挺厉害的) Note that response.follow just returns a Request instance; you still have to yield this Request.</p>
<p>You can also pass a selector to response.follow instead of a string; this selector should extract necessary attributes:</p>
<pre><code>for href in response.css('li.next a::attr(href)'):
    yield response.follow(href, callback=self.parse)
</code></pre>
<p>For <a target="_blank"> elements there is a shortcut: response.follow uses their href attribute automatically. So the code can be shortened further:</a></p><a target="_blank">
<pre><code>for a in response.css('li.next a'):
    yield response.follow(a, callback=self.parse)
</code></pre>
<p><strong>Note</strong><br>
<code>response.follow(response.css('li.next a'))</code> is not valid because <code>response.css</code> returns a list-like object with selectors for all results, not a single selector. A for loop like in the example above, or <code>response.follow(response.css('li.next a')[0])</code> is fine.</p>
<h4 id="b3_solo_h4_7">More examples and patterns</h4>
<p>Here is another spider that illustrates callbacks and following links, this time for scraping author information:</p>
<pre><code>import scrapy
class AuthorSpider(scrapy.Spider):
    name = 'author'
    start_urls = ['http://quotes.toscrape.com/']
    def parse(self, response):
        # follow links to author pages
        for href in response.css('.author + a::attr(href)'):
            yield response.follow(href, self.parse_author)

        # follow pagination links
        for href in response.css('li.next a::attr(href)'):
            yield response.follow(href, self.parse)

    def parse_author(self, response):
        def extract_with_css(query):
            return response.css(query).extract_first().strip()

        yield {
            'name': extract_with_css('h3.author-title::text'),
            'birthdate': extract_with_css('.author-born-date::text'),
            'bio': extract_with_css('.author-description::text'),
        }
</code></pre>
<p>This spider will start from the main page, it will follow all the links to the authors pages calling the parse_author callback for each of them, and also the pagination links with the parse callback as we saw before.</p>
<p>Here we’re passing callbacks to response.follow as positional arguments to make the code shorter; it also works for <code>scrapy.Request</code>.</p>
<p>The parse_author callback <strong>defines a helper function</strong> to extract and cleanup the data from a CSS query and yields the Python dict with the author data.</p>
<p>Another interesting thing this spider demonstrates(显示) is that, even if there are many quotes from the same author, we don’t need to worry about visiting the same author page multiple times. By default, Scrapy filters out duplicated requests to URLs already visited, avoiding the problem of hitting servers too much because of a programming mistake. This can be configured by the setting DUPEFILTER_CLASS.(不用担心死循环)</p>
<p>Hopefully by now you have a good understanding of how to use the mechanism of following links and callbacks with Scrapy.</p>
</a><p><a target="_blank">As yet another example spider that leverages the mechanism of following links, check out the CrawlSpider class for a generic spider that implements a small rules engine that you can use to write your crawlers on top of it.(可以写自己的规则?是这个意思吧)<br>
Also, a common pattern is to build an item with data from more than one page, using </a><a href="https://doc.scrapy.org/en/latest/topics/request-response.html#topics-request-response-ref-request-callback-arguments" target="_blank">a trick to pass additional data to the callbacks</a>.</p>
<h4 id="b3_solo_h4_8">Using spider arguments</h4>
<p>You can provide command line arguments to your spiders by using the -a option when running them:<br>
<code>scrapy crawl quotes -o quotes-humor.json -a tag=humor</code><br>
(使用-a选项 添加attr)<br>
These arguments are passed to the Spider’s _<em>init</em>_ method and become spider attributes by default.</p>
<p>In this example, the value provided for the tag argument will be available via self.tag. You can use this to make your spider fetch only quotes with a specific tag, building the URL based on the argument:</p>
<pre><code>import scrapy
class QuotesSpider(scrapy.Spider):
    name = "quotes"
    def start_requests(self):
        url = 'http://quotes.toscrape.com/'
        tag = getattr(self, 'tag', None)
        if tag is not None:
            url = url + 'tag/' + tag
        yield scrapy.Request(url, self.parse)

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').extract_first(),
                'author': quote.css('small.author::text').extract_first(),
            }(先处理得到的文本)
        next_page = response.css('li.next a::attr(href)').extract_first()
        (再去获取下一个page)
        if next_page is not None:
            yield response.follow(next_page, self.parse)
</code></pre>
<p>If you pass the <code>tag=humor</code> argument to this spider, you’ll notice that it will only visit URLs from the humor tag, such as <code>http://quotes.toscrape.com/tag/humor</code>.</p>
<p>You can learn more about <a href="https://doc.scrapy.org/en/latest/topics/spiders.html#spiderargs" target="_blank">handling spider arguments here</a>.</p>
<p></p>
                    <div>
                        <hr>
<img alt="今日诗词" src="https://v2.jinrishici.com/one.svg" style="max-width:100%; display: block; margin: 0 auto;">

标题：Scrapy Pass 3<br>
作者：<a href="https://ellenbboe.github.io" target="_blank">ellenbboe</a><br>
地址：<a href="https://ellenbboe.github.io/articles/2018/08/17/1561009680520.html" target="_blank">https://ellenbboe.github.io/articles/2018/08/17/1561009680520.html</a><br>
<!-- 签名档内可使用 HTML、JavaScript -->
<br>
                    </div>
            </section>
        </div>
    </div>
        <div class="post__toc">
<ul class="article__toc">
        <li class="toc__h2">
            <a href="#b3_solo_h2_0">Learn Scrapy pass 3</a>
        </li>
        <li class="toc__h3">
            <a href="#b3_solo_h3_1">接上文Extract data</a>
        </li>
        <li class="toc__h4">
            <a href="#b3_solo_h4_2">Extracting quotes and authors</a>
        </li>
        <li class="toc__h4">
            <a href="#b3_solo_h4_3">Extracting data in our spider</a>
        </li>
        <li class="toc__h4">
            <a href="#b3_solo_h4_4">Storing the scraped data</a>
        </li>
        <li class="toc__h4">
            <a href="#b3_solo_h4_5">Following links</a>
        </li>
        <li class="toc__h4">
            <a href="#b3_solo_h4_6">A shortcut for creating Requests</a>
        </li>
        <li class="toc__h4">
            <a href="#b3_solo_h4_7">More examples and patterns</a>
        </li>
        <li class="toc__h4">
            <a href="#b3_solo_h4_8">Using spider arguments</a>
        </li>
</ul>        </div>
<div class="comment">
    <div class="comment__wrapper wrapper">
        <div class="comment__title">
            评论
        </div>
        <textarea rows="3" placeholder="评论内容只能为 2 到 500 个字符！" id="comment"></textarea>

        <ul id="comments">
        </ul>
    </div>
</div>
    <div class="article__bottom">
        <div class="wrapper">
            <div class="fn__flex">
                <div class="item" id="randomArticles"></div>
                <div class="item" id="relevantArticles"></div>
            </div>
        </div>
    </div>
    
</div>
<footer class="footer">
    <div class="wrapper fn__clear">
        <div class="fn__left">
            &copy; 2020
            <a href="https://ellenbboe.github.io">Kosmos</a>
            <a href="http://www.beian.miit.gov.cn/">浙ICP备18045840号-1</a>--------Let's move on
本站已安全运行:
<span id="momk"></span>
<script language=javascript>
function show_date_time(){
window.setTimeout("show_date_time()", 1000);
BirthDay=new Date("09-10-2018 19:47:57");//建站日期
today=new Date();
timeold=(today.getTime()-BirthDay.getTime());
sectimeold=timeold/1000
secondsold=Math.floor(sectimeold);
msPerDay=24*60*60*1000
e_daysold=timeold/msPerDay
daysold=Math.floor(e_daysold);
e_hrsold=(daysold-e_daysold)*-24;
hrsold=Math.floor(e_hrsold);
e_minsold=(hrsold-e_hrsold)*-60;
minsold=Math.floor((hrsold-e_hrsold)*-60);
seconds=Math.floor((minsold-e_minsold)*-60);
momk.innerHTML=daysold+"天"+hrsold+"小时"+minsold+"分"+seconds+"秒" ;
}
show_date_time();
</script>
<style>
#momk{animation:change 10s infinite;font-weight:800; }
@keyframes change{0%{color:#5cb85c;}25%{color:#556bd8;}50%{color:#e40707;}75%{color:#66e616;}100% {color:#67bd31;}}
</style> &nbsp;•&nbsp;
            Powered by <a href="https://solo.b3log.org" target="_blank">Solo</a>
            <br>
            Theme Casper
            <sup>[<a href="https://github.com/TryGhost/Casper" target="_blank">ref</a>]</sup>
            by <a href="http://vanessa.b3log.org" target="_blank">Vanessa</a>
        </div>
        <div class="fn__right">
            <a href="https://ellenbboe.github.io/tags.html" rel="section">
            标签墙
            </a>
            &nbsp;•&nbsp;
            <a href="https://ellenbboe.github.io/archives.html">
            存档
            </a>
            &nbsp;•&nbsp;
            <a rel="archive" href="https://ellenbboe.github.io/links.html">
            友情链接
            </a>
            <br>
            115 文章 &nbsp;
            9 评论 &nbsp;
            <span data-uvstaturl="https://ellenbboe.github.io">11608</span> 浏览 &nbsp;
            4 当前访客
        </div>
    </div>
</footer>

<script type="text/javascript" src="https://ellenbboe.github.io/js/lib/compress/pjax.min.js" charset="utf-8"></script>
<script type="text/javascript" src="https://ellenbboe.github.io/js/common.min.js?1578931371974"
        charset="utf-8"></script>
<script type="text/javascript"
        src="https://ellenbboe.github.io/skins/Casper/js/common.min.js?1578931371974"
        charset="utf-8"></script>
<script src="https://cdn.jsdelivr.net/npm/uvstat@v1.0.4/dist/index.min.js"></script>
<script>
    var Label = {
        servePath: "https://ellenbboe.github.io",
        staticServePath: "https://ellenbboe.github.io",
        luteAvailable: false,
        hljsStyle: 'colorful',
        langLabel: "zh_CN",
        version: "3.7.0",
    }
    Util.init()
</script>


<script type="text/javascript">
    Util.addScript('https://ellenbboe.github.io/js/page.min.js?1578931371974', 'soloPageScript')
    var page = new Page({
        "commentContentCannotEmptyLabel": "评论内容只能为 2 到 500 个字符！",
        "oId": "1561009680520",
        "blogHost": "https://ellenbboe.github.io",
        "randomArticles1Label": "随机阅读：",
        "externalRelevantArticles1Label": "站外相关阅读："
    });
    $(document).ready(function () {
        page.load();
    page.tips.externalRelevantArticlesDisplayCount = "0";
    Skin.initArticle()
    });
</script>

</body>
</html>

<!-- Generated by Latke (https://github.com/88250/latke) in 34ms, 2020/01/14 12:34:58 -->